\documentclass[%
  a4paper,oneside,%
  %arial,
  11pt,% <10pt, 9pt>
  smallchapters,
  %style=screen,
  %sender=bottom,
  green,% <orange, green, violet>
  rgb, <cmyk>
  %mono
  ]{tubsbook}

\usepackage[backend=biber,style=ieee,backref=true]{biblatex}

\addbibresource{Library.bib}

\usepackage{siunitx}
\sisetup{load-configurations = abbreviations}  
  
\usepackage{listings}  % for code stuff
\usepackage{minted}

\usepackage{physics}

\usepackage{bbm} %for probability notation


\usepackage{tikz}
\usepackage{tikzit}
\input{MA.tikzstyles}
\newcommand{\rn}[2]{%% "rn": "remember node"
	\tikz[remember picture,baseline=(#1.base)]\node [inner sep=0] (#1)  {$#2$};%
}

\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    morekeywords={self},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}


\newcommand\pythonstyle{\lstset{
	language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    morekeywords={self},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}




%lstset{style=mystyle}

\usepackage{bm} 
\usepackage{amsmath}
 
\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\RequirePackage{scrlfile}
\ReplacePackage{scrpage2}{scrlayer-scrpage}

\usepackage[ngerman,english]{babel}

\usepackage{lipsum} % Blindtext-Paket
\definecolor{InAGreen}{RGB}{172,193,58}

% Titelseiten-Elemente
\title{A Statistical Approach for the Fusion of Data and
Finite Element Analysis in Vibroacoustics}
\subtitle{Untertitel}
\author{Lucas Hermann}
%\logo{Institut fuer Lorem Ipsum}
\logo{\includegraphics{InA-Logo-rgb.pdf}}
\titleabstract{\lipsum[2]}
\titlepicture{infozentrum.jpg}
% Rückseiten-Elemente
\address{%
  Herr Mustermann\\
  Schlossallee 1\\
  33333 Darmstadt}
\backpageinfo{%
  \lipsum[5]
}
\usepackage{pdfpages}								% Einbinden von PDF-Pages
\usepackage[colorlinks,pdfpagelabels,pdfstartview = FitH,bookmarksopen = true,bookmarksnumbered = true,linkcolor = black,plainpages = false,hypertexnames = false,citecolor = black] {hyperref}
\usepackage[figure]{hypcap} 

\begin{document}

%\maketitle[image,logo=left]%[<plain/image/imagetext>,<logo=left/right>]
%\makebackpage[trisec]%[<plain/info/addressinfo>]
\includepdf[pages=-]{./Deckblatt/2099_StA_Name_Deckblatt.pdf}

\chapter*{Declaration}
Hiermit versichere ich, Lucas Hermann, durch meine Unterschrift, dass ich die
vorliegende Masterarbeit mit dem Titel ``<<Titel>>'' selbständig und ohne Benutzung
anderer als der angegebenen Hilfsmittel angefertigt habe. Alle Stellen, die wörtlich oder sinn-
gemäß aus veröffentlichten oder unveröffentlichten Schriften entnommen sind, habe ich als
solche kenntlich gemacht. Insbesondere sind auch solche Inhalte gekennzeichnet, die von
betreuenden wissenschaftlichen Mitarbeiterinnen und Mitarbeitern des Instituts für Akustik eingebracht wurden.

Die Arbeit oder Auszüge daraus haben noch nicht in gleicher oder ähnlicher Form dieser
oder einer anderen Prüfungsbehörde vorgelegen.

Mir ist bewusst, dass Verstöße gegen die Grundsätze der Selbstständigkeit als Täuschung
betrachtet und entsprechend der Prüfungsordnung geahndet werden.

\begin{flushright}
Braunschweig, \today
\end{flushright}

\vspace{2cm}
\hspace{2cm}\rule{5cm}{1pt}

\hspace{2cm}\small{Lucas Hermann} 

\chapter*{Abstract}
\lipsum[1]

\tableofcontents


\chapter*{Introduction}

\textcolor{tubsSecondary}{Dies ist ein Text in \texttt{tubsSecondary}.}
\textcolor{tubsViolet}{Dies ist ein Text in \texttt{tubsViolet}.}
\textcolor{tubsGreenDark}{Dies ist ein Text in \texttt{tubsGreenDark}.}\bigskip

\lipsum[1]

\begin{itemize}
  \item Aufzählungspunkt Eins
  \item Aufzählungspunkt Zwei
    \begin{itemize}
      \item Unter-Aufzählungspunkt Eins
      \item Unter-Aufzählungspunkt Zwei
    \end{itemize}
  \item Aufzählungspunkt Drei
\end{itemize}



\chapter{FEM Solution of the Helmholtz Equation}
An acoustic field is represented in time domain by the wave equation. It returns the acoustical pressure at a point in space $x$ for a time variable $t$.
The Helmholtz equation describes the wave equation in a time-independent manner, i.e. in frequency domain. It has a dependency on the frequency, so the goal is to solve the Helmholtz equation for a range of frequencies in order to obtain a frequency response plot which gives insight on the modal behaviour of the system.

\section{Wave Equation and Helmholtz Equation}
In a typical vibroacoustic problem, for example an interior structural acoustic coupling problem \cite{atalla2015}, there are usually two coupled domains: At first, there is some kind of solid medium which emits sound. The movement of that solid can be described within the field of elastodynamics using for instance (a) Bernoulli or Timoshenko beam theory for 1D beam-like structures, (b) Kirchhoff or Reissner-Mindlin plate theory for 2D plate-like structures. The focus of this thesis is another: As soon as the sound is emitted from the solid, the wave equation is used to describe the motions of sound waves through an acoustic medium such as air. 
The following closely follows \cite{larson2013} and \cite{moser2005}.
The behaviour of a gas in a domain $\Omega$ can be described using the density $\rho$, the pressure $p$ and the velocity $v$. By changing the velocity or pressure locally by e.g. a speaker or a vibrating plate, this change is propageted through the medium. One can think of the gas as a system of infinitesimally small spring-mass systems. Deflecting one spring yields a deflection of the adjacent spring, or gas volume, but time-delayed due to the gas's inertia. That leads to a wave propagating through the medium. 
Newton's second law states that mass times accelaration equals force. If a gas volume is accelerated in one direction, that leads to a force on the neighbouring particle \cite{moser2005}.
%
\begin{equation}
m \dot{v} = S  \left[  p(x) -p(x + \Delta x)  \right] 
\end{equation}
with $m$ the mass of the gas volume, $S$ the surface of the contact area of the two volumes  and $\Delta x$ the length of a gas volume in the direction of motion. Considering $m = \Delta x S \rho$ there holds
\begin{equation}
\rho \dot{v} = - \frac{p(x) -p(x + \Delta x)}{\Delta x} \; .
\label{eqn:waveEqDeriv1}
\end{equation}
Assuming interaction between infinitessimaly small gas particles and therefore taking the limit of the difference quotient in (\ref{eqn:waveEqDeriv1}) yields the differential quotient and hence
\begin{equation}
\rho \dot{v} = - \nabla p \;.
\label{eqn:Tragheitsges}
\end{equation}
$\nabla$ is the nabla operator $\left( \pdv{}{x_1},\pdv{}{x_2},...,\pdv{}{x_n} \right)$ which consists of the partial derivatives of the used coordinates. For cartesian coordinates, applied as a product to a quantity, it describes the gradient of the quantity, i.e.
\begin{equation}
\nabla p = \mathrm{grad}p = \left( \pdv{p}{x},\pdv{p}{y},\pdv{p}{z} \right) \;.
\end{equation}


If a volume is expanded, the pressure of the contained gas drops and vice versa. Therefore a change of volume is proportional to a negative change of pressure. An expanding volume is also represented by the divergence of the velocity \cite{larson2013}. Imagine quickly pulling a piston out of an airtight cylinder: The air particles will move in the direction of the piston movement while the volume expands and the pressure drops. Hence, there holds
\begin{equation}
\dot{p} = -k \nabla u
\label{eqn:Constit}
\end{equation}
with $k$ a proportionality constant dependent on the medium.
Differentiating (\ref{eqn:Constit}) with respect to $t$ yields 
\begin{equation}
\ddot{p} = -k \nabla \dot{u} \;.
\end{equation}
Inserting \ref{eqn:Tragheitsges} and considering $c^2 = k/\rho$ there holds for the acoustic wave equation
\begin{equation}
\ddot{p} = c^2 \nabla^2 p 
\label{eqn:WaveEq}
\end{equation}
which involves derivatives in both time and space. To obtain a general representation of the behaviour of a pressure field independent of time, the Helmholtz equation can be derived as follows.
Considering a separation ansatz \cite{westermann2015}
\begin{equation}
p(x,t) = X(x) \cdot T(t) \; ,
\end{equation}
there holds for \ref{eqn:WaveEq}

\begin{align}
X(x) \cdot \pdv[2]{T(t)}{t} &= c^2 \nabla^2 X(x) \cdot T(t)\\
\frac{1}{T(t)} \pdv[2]{T(t)}{t} &= c^2 \frac{1}{X(x)} \nabla^2 X(x) = const. = - \omega^2 \; .
\end{align}
With the wave number $k = \omega / c$ 
\begin{equation}
\nabla^2 X(x) + k^2 X(x) = 0
\end{equation}
is the Helmholtz equation which is a representation of the wave equation independent of time \cite[p. 1083 ff.]{arens2015}.
%
With $p [\SI{}{\pascal}]$ the pressure field and $k$ the wave number it is in the following going to be referred to by
\begin{equation}
\nabla^2 p + k^2 p = 0 \;.
\label{eqn:Helmholtz}
\end{equation}
With $\omega$[\SI{}{\radian}] the angular frequency, $f$[\SI{}{\per\s}] the frequency and $c_0[\SI{}{\metre\per\s}]$ the speed of sound in the considered medium $k$ is defined as
\begin{equation}
k = \frac{\omega}{c_0} = \frac{2 \pi f}{c_0} \;.
\end{equation}
(\ref{eqn:Helmholtz}) is solved as a boundary value problem which means that the behavior of the system has to be described beforehand for the boundaries of the calculation domain. That is done using boundary conditions.



\section{Boundary Conditions}
The following closely follows \cite{atalla2015}. There are three different types of boundary conditions: Dirichlet, Neumann and Robin type. Neumann boundary conditions are also called natural boundary conditions because these automatically arise in the equation while deriving the weak form of the problem. Dirichlet boundary conditions, on the other hand, don't. They are called essential boundary conditions and have to be applied by hand after deriving the weak form and building the linear system of equations.
\paragraph{Dirichlet}
The Dirichlet boundary condition for the Helmholtz equation is simply a prescribed pressure $\bar{p}(x)$ at the $\Gamma_D$ part of the boundary:
\begin{equation}
p(x) = \bar{p}(x) \quad \forall x \in \Gamma_D \;.
\end{equation}

	
\paragraph{Neumann}
The Neumann boundary condition at $\Gamma_N$ reads
\begin{equation}
\dv{p(x)}{n} = \rho \omega^2 \vec{U(x)} \cdot \vec{n} \quad \forall x \in \Gamma_N
\end{equation}
with $\vec{U}$ the displacement and $\vec{n}$ the outer normal direction of the surface. It can be thought of as a piston moving with circular frequency $\omega$ at the boundary which creates a velocity and pressure change. Choosing a homogeneous Neumann BC $\dv{p(x)}{n} = 0$ resembles a reflecting boundary. The part of the boundary, where a homogeneouos Neumann BC is applied, is going to be called $\Gamma_{N0}$.

\paragraph{Robin}
The Robin or impedance boundary condition at $\Gamma_R$ reads
\begin{equation}
\dv{p(x)}{n} + ik \beta p(x) = 0 \quad \forall x \in \Gamma_R
\end{equation}
with $\beta$ the specific normalized acoustic admittance. The Robin boundary condition is also called impedance boundary conditions because an acoustic impedance can be simulated what leads to e.g. partially reflecting walls.

Figure \ref{fig:BCEx} shows an example on how boundary conditions could be applied on a 2D domain. $\Gamma_{N0}$ boundary conditions are the mentioned natural boundary conditions which don't have to be applied explicitly. Still, a Neummann boundary condition $\Gamma_{N}$ can be imposed onto the system which behaves differently. 
Figure \ref{fig:BCTh} shows the setting chosen in this thesis: All walls are reflecting and on the left side of the domain a Neumann boundary conditon is imposed which is used to model the previously mentioned sound source. 


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\textwidth]{pics/BCsExample}
\caption{Two dimensional representation of various boundary conditions. In this example, the left side is a Neumann boundary, the upper and lower boundaries are totally reflecting and the right side consists of an impedance BC and a Dirichlet BC.}
\label{fig:BCEx}
\end{center}
\end{figure}


\begin{figure}[h]
\begin{center}
\includegraphics[width=0.4\textwidth]{pics/BCsThesis1}
\caption{Boundary Conditions of the problem for this thesis. A Neumann boundary is applied on the side of the domain. The rest of the boundary is considered as reflecting, i.e. $\Gamma_N = 0$.}
\label{fig:BCTh}
\end{center}
\end{figure}

FEniCS \cite{langtangen2016} is a Python/C++ library for solving PDEs with the Finite Element Method. It makes it easy to go directly from the variational formulation of a problem to solving it.
Using FEniCS, the boundary conditions can be applied to the system of equations by 
%
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
class BoundaryX_L(SubDomain):
	self.tol = 1E-14
	def inside(self, x, on_boundary):
		return on_boundary and near(x[0], 0, tol)# and (x[1] < 0.3)
boundary_markers.set_all(9999) # all markers default to zero.
bxL = BoundaryX_L()
bxL.mark(boundary_markers,0) # left side is marked as "0"
ds = Measure('ds', domain=self.mesh, subdomain_data=boundary_markers) #define a new operator for the boundary
\end{minted}
%
%
This construction can then be used when defining the variational problem:
%
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
U = 0.0001 #Piston displacement
g = self.rho * self.omega**2 * U
a = inner(nabla_grad(self.u), nabla_grad(self.v))*dx - self.k**2 * inner(self.u,self.v)*dx #variational Problem
L = (self.v*g)*ds(0) # 0 is the chosen boundary marker
\end{minted}

Having the problem defined mathematically, the equations can now be prepared for being solved by FEM.

\section{Weak Formulation and Discretization}
Explain what the weak form is and why we need it. 
Derive the weak form, completely with integration by parts etc. explain all parts of it and what they represent.

explain what a bilinear and a linear form is!


Eq. (\ref{eqn:Helmholtz}) is considered a \emph{strong}	formulation. In order to be able to set up the FEM, the PDE needs to be in the \emph{weak} form. Among other possibilities, the weak form can be derived using the Weighted Residuals Method.

In order to lower the degree of derivation, integration by parts is executed:




Dirichlet boundary conditions can be applied after assembling the system by 
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def boundaryDiri(x):
	return x[0] > (1.0 - DOLFIN_EPS) and x[1] < 0.0 + DOLFIN_EPS
self.bcDir = DirichletBC(self.V, Constant(0.0), boundaryDiri)
A = assemble(a)
b = assemble(L)
self.bcDir.apply(A, b)
\end{minted}






\section{The Classical Finite Element Method}


The Finite Element Method (FEM) is a procedure which makes it simple to approximately solve Partial Differential Equations (PDEs) which would be very hard or even impossible to solve analytically. As the name suggests, the calculation domain is split into $n_e$ individual elements on which the PDE is approximated using so-called ansatz functions. The process of dividing the domain in $n_e$ elements is called discretization. The polynomial degree of the ansatz functions and the number of elements determine the accuracy of the approximation. In order to set up the FEM, the PDE needs to be in the so-called variational, or "weak", formulation. 
After discretization of the domain and deriving the weak formulation, the PDE can be approximated on the individual elements. Assembling the element matrices yields a global system of equations which can, after boundary conditions are applied, be solved for the unknown vector.


\subsection{Integral and Weak Formulation}
- Explain Vector spaces
- Test function: needs to obey the dirichlet conditions


The Helmholtz equation is the so-called governing equation for the problem considered in this thesis. The governing equation is given in the strong form and is usually of higher order. This property makes it hard to solve the equation, both analytically and numerically. For being able to apply the FEM, the order of the PDE has to be lowered. That is archieved by first bringing the equation in the weak, or also called variational, formulation.
One way to derive the weak formulation of the governing equation is the method of weighted residuals. 
\paragraph{The Weigthed Residuals Method}
The residual $R$ is the difference between approximation and exact solution. The exact solution is usually not known. Therefore the residual is formed by inserting the approximation of the solution into the PDE. The result is non-zero and a measure for the quality of the approximation \cite{langtangen2019}. To increase the quality, the residual has to be minimized. 
An FEM approximation, see \ref{chap:ApproxFem} for details, uses FEM basis functions which are weighted with some factors $c_j$. The goal is finding the factors which minimize $R$. The seeked approximated solution vector to the Helmholtz equation $\bm{p}$ is part of a space $V$ which is spanned by the FEM basis functions $\varphi_i$. Demanding the scalar product of the residual and a test function $v$ to be zero
\begin{equation}
(R,v) = 0, \quad \forall v \in V \;,
\end{equation}
i.e. demanding that they are orthogonal, can be used to minimize $R$. $V$ can be spanned by the FEM basis functions as well. Nevertheless, it doesn't necessarily need to be defined in the same space as $\bm{p}$. Using the same space would be called the Galerkin method. Applying the Weighted Residuals Method to the Helmholtz equation is executed as follows.
At first the strong formulation (PDE) is multiplied with the test function $v$ and integrated over the domain $\Omega$ to get the so-called integral formulation:
\begin{equation}
\int_{\Omega} v \nabla^2 p \,\mathrm{d}\Omega + \int_{\Omega} k^2 pv \,\mathrm{d}\Omega = 0
\label{eqn:IntForm}
\end{equation}
%

The approximated function within an element, here it is $p$, is called the trial function. It is part of a constrained space of trial functions or termed as the trial space. The test function $v$ is also part of a test space. Usually, trial and test spaces are chosen to be the same. Both trial and test spaces have to fulfill the Dirichlet boundary conditions imposed on the PDE.
The Helmholtz equation and therefore also the now formed integral formulation contain derivatives of second order. However, the FEM works by adjoining individual element functions to a global function. Therefore, the global solution is a piecewise polynomial \cite{langtangen2019} what causes problems when calculating higher-order derivatives because those become discontinuous. In consequence, integration by parts is performed using Green's first identity \cite[pp. 53,54]{atalla2015} on the higher-order terms to get lower order derivatives. It makes use of the divergence theorem, also called Gauss's theorem:

\begin{equation}
\int_{\Omega}  \nabla \cdot \vec{F} \,\mathrm{d}\Omega = \oint_{\Gamma} \vec{F} \cdot \vec{n} \,\mathrm{d}\Gamma \; .
\label{eqn:DivTheo}
\end{equation}
It states that all of the sources and sinks inside a domain (which generate divergence) must be equal to the amount of flux over the boundary of that domain. 
By defining  $F = vu$ with $u = \nabla p$ and making use of the product rule of vector calculus $\nabla \cdot (v u) = v \nabla \cdot u + u \cdot \nabla v$ there holds for Green's first identity
\begin{equation}
\oint_{\Gamma} v \nabla p \,\mathrm{d}\Gamma = \int_{\Omega} \nabla p \cdot \nabla v \,\mathrm{d}\Omega + \int_{\Omega}  v\nabla^2 p \,\mathrm{d}\Omega \; .
\end{equation}
%
Applying it to (\ref{eqn:IntForm}) yields
\begin{equation}
-\int_{\Omega} \nabla p \cdot \nabla v \,\mathrm{d}\Omega + \oint_{\Gamma} v \nabla p  \,\mathrm{d}\Gamma+ \int_{\Omega} k^2 pv \,\mathrm{d}\Omega = 0
\label{eqn:WeakHelmholtz}
\end{equation}
with the boundary of the domain $\Gamma$.
The resulting equation is now called the weak formulation as opposed to the strong formulation since the trial function now only needs to be differentiable once less often.
The boundary term can be decomposed into three parts
\begin{equation}
\oint_{\Gamma} v \nabla p  \,\mathrm{d}\Gamma = \oint_{\Gamma_D} v \nabla \bar{p}  \,\mathrm{d}\Gamma + \oint_{\Gamma_N} v (\rho \omega^2 \vec{U}\vec{n})  \,\mathrm{d}\Gamma + \oint_{\Gamma_R} v(-ik\beta \bar{p})  \,\mathrm{d}\Gamma
\end{equation}
with $\oint_{\Gamma_D} v \nabla \bar{p}  \,\mathrm{d}\Gamma = 0$ for the Dirichlet part because if the value at the boundary is known, then $\bar{f}$ is a scalar quantitity of which the derivative in outward normal direction of the boundary is zero.

\subsection{Discretization}
In this thesis, two different orders of domains are used: In a simple example, a 1D domain is going to serve as an illustration. For the actual vibroacoustics problem, a 2D domain will be examined. In this chapter, the FEM discretization for both orders is discussed.
%
Discretization means that the domain $\Omega$ is split into $n_e$ individual elements $\Omega^{(e)}$
\begin{equation}
\Omega = \bigcup_{e=1}^{n_e} \Omega^{(e)} \quad e = 1,...,n_e \;.
\end{equation}
This holds not only for 1D meshes but for 2D and 3D meshes as well.
For 1D meshes, such as in Figure \ref{fig:1DDom}, each element has two so-called nodes. This is because between the element boundaries a linear function is spanned which needs two support points to be defined.  In a 2D domain, an element would have 3 nodes, see Figure \ref{fig:2DDom}. Also different shapes of elements, e.g. quaratic elements with 4 nodes, are possible. The collection of nodes is called a mesh. The total number of nodes in a 1D mesh is $n = n_e + 1$, in a 2D mesh it is $n = n_x \times n_y$ with $n_x$ and $n_y$ the number of nodes in each coordinate. The minimum degree of the used function is determined by the weak form: It always needs to be differentiable, therefore a constant approximation function would not work \cite{atalla2015}. 

Figures \ref{fig:1DDom} and \ref{fig:2DDom} illustrate the discretization of a 1D and 2D domain, respectively.  For the 1D domain, the first ansatz functions are sketched: It is visible that for each node there is an ansatz function that yields the value $1$ only at that node and $0$ at all other nodes in the domain.
%
\begin{figure}[h]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/1dDomain}
\caption{A discretized 1D domain with nodes in green and linear ansatz functions shown for the first four nodes}
\label{fig:1DDom}

\end{center}
\end{figure}
%
\begin{figure}[h]
\begin{center}

\includegraphics[width=0.4\textwidth]{pics/2dDomain}
\caption{A discretized 2D domain with nodes in green}
\label{fig:2DDom}

\end{center}
\end{figure}
%
The finer the mesh, i.e. the more elements are used in a domain, the closer the FEM approximation comes to the exact solution. Since computing very fine meshes is computationally expensive, a convergence study is conducted in order to find the element size from which on smaller element sizes don't result in more accurate results.

Using FEniCS, a 1D mesh can be created by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
self.dom_a = 0.0 #domain boundaries
self.dom_b = 1.0
self.ne	= 60 #number of elements
self.mesh = IntervalMesh(self.ne,self.dom_a,self.dom_b) #define mesh
\end{minted}
The interval of the mesh and the desired number of elements have to be provided.

A 2D mesh can, for example, be obtained by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
self.a,self.b = 2,2
self.mesh = UnitSquareMesh(self.a,self.b) #define mesh
\end{minted}
In this case a unit square is chosen what means that the domain size will be quadratic and of length $1.0$ in every direction. $2$ elements were chosen for every dimension what leads to a mesh looking like \ref{fig:2DDom}. Of course every desired 2D shape can be used as a mesh.

\subsection{Approximation and Building Element Matrices}
\label{chap:ApproxFem}
In FEM, the values between nodes within an element are interpolated with basis functions $\varphi_i$. 
For each degree of freedom in the domain an FEM basis function is defined. By weighing those basis functions, the solution can be approximated with 
\begin{equation}
p(x_i) = \sum_{j = 1}^n c_j  \varphi_j(x_i) = f(x_i) \; .
\label{eqn:WeighingFunction}
\end{equation}
(\ref{eqn:WeighingFunction}) means, that the value of one particular point $x_i$ is the sum of all products of all weight coefficients $c_j$ and test functions $\varphi_j$ evaluated at that point. If now orthogonal basis functions are used, as it is commonly the case in FEM, (\ref{eqn:WeighingFunction}) simplifies as described below. One popular choice of FEM basis functions is the Lagrange family of polynomials. The right hand side $f(x_i)$ does, in practice, describe the source term of the PDE which does not depend on $p(x)$.

\paragraph{Lagrange Polynomials}
Following \cite{langtangen2019}.
Lagrange polynomials are one possible choice for FEM basis functions and are used in this thesis. They can be calculated for any desired degree $N$ and their main property is that they are $1$ at only a single node on the domain, $0$ at all other nodes and that they are nonzero on only a very limited fraction of the domain, as opposed to e.g. basis functions constructed from $\sin$ and $\cos$ terms. They are used as an interpolation between given points which means that the polynomial always exactly goes through the given points. The behaviour at all other points is determined by the definition of the Lagrange polynomial
\begin{equation}
\varphi_i(x) = \prod_{j=0,j\neq i}^N \frac{x-x_j}{x_i-x_j} \;.
\end{equation}
They are frequently used in FEM because there holds, if at $x_s$ there is a node of the mesh 
\begin{equation}
\varphi_i(x_s) = \delta_{is}, \quad \delta_{is}  =     \left\{ \begin{array}{rcl} 1 & \mbox{for}& i = s  \\ 0 & \mbox{for} & i \neq s\end{array}\right .
\end{equation}
which means that the polynomial is only $1$ at one node in the domain. At all other nodes it is $0$. Between nodes the Lagrange polynomial is spanned. This behavior simplifies (\ref{eqn:WeighingFunction}), if $x_i$ is the location of a node of the mesh:
\begin{equation}
p(x_i) = \hat{p}_i \varphi_i(x_i) = \hat{p}_i \;.
\end{equation}
The weight factor is therefore the approximated value at the node.
Another asset of using polynomials with $\varphi_i(x) = 0$ in most parts of the domain is that they lead to sparse matrices which can be solved much quicker than densely packed ones.
%
Now, to apply it to the weak form of the Helmholtz equation (\ref{eqn:WeakHelmholtz}), an approximation ansatz has to be made for both the trial and the test function. There holds for each element, if the same basis functions are used for both trial and test spaces,
\begin{align}
\begin{split}
\tilde{p} &= \bm{\varphi^T} \hat{\bm{p}} \\
\tilde{v} &= (\bm{\varphi^T} \delta\hat{\bm{p}})^T
\end{split}
\label{eqn:FEMAnsatz}
\end{align}
with $\delta\hat{\bm{p}}$ an arbitrary variation [Langer CompAc] of the pressure $\hat{\bm{p}}$, which gets cancelled out later on because the test function is present in every term.
Inserting (\ref{eqn:FEMAnsatz}) in the weak form, considering no Robin boundary conditions, results in
\begin{equation}
\sum_e \int_{\Omega_e} (\nabla \bm{\varphi} \nabla \bm{\varphi}^T)\,\mathrm{d}\Omega_e \hat{\bm{p}} -k^2 \sum_e \int_{\Omega_e} (\bm{\varphi} \bm{\varphi}^T)\,\mathrm{d}\Omega_e \hat{\bm{p}} = \rho \omega^2 \sum_e \int_{\Gamma_{e,N}} \bm{\varphi} U_n \,\mathrm{d}\Gamma_{e,N} \;.
\label{eqn:DiscretizedFEM}
\end{equation}
The sum $\sum_e$ means assembling the global system of matrices out of individual element matrices. In short, (\ref{eqn:DiscretizedFEM}) can be written as
\begin{equation}
\left( \sum_e \bm{K}_1^e - k^2 \sum_e \bm{K}_2^e \right)\hat{\bm{p}} = \rho \omega^2 \sum_e \bm{f}^e
\end{equation}
With $\bm{K}_1^e$ and $\bm{K}_2^e$ the elemental system matrices and $\bm{f}^e$ the elemental source vector. After assembly there holds for the global linear system
\begin{equation}
(\bm{K}_1 - k^2 \bm{K}_2) \hat{\bm{p}} = \rho \omega^2 \bm{f}
\end{equation}
which is now to be solved for $\hat{\bm{p}}$.
To illustrate the assembly process, the so-called dynamic stiffness matrix $\bm{A}$ is introduced as
\begin{equation}
\bm{A} = (\bm{K}_1 - k^2 \bm{K}_2) \;.
\end{equation}
It is assembled using the individual matrices $\bm{A}^e$ for each element as visible in (\ref{eqn:Assembly}).
The goal is to solve the linear system of equations for the weight factors of the basis functions $p$, also called the degrees of freedom.
% It can be notated as
%\begin{equation}
%\sum_j A_{i,j} c_j = b_i
%\end{equation}
%with
%\begin{align}
%A_{i,j} &= \varphi_j(x_i), \\
%b_i &= f(x_i) \;.
%\end{align}
The calculation is usually done for one element at a time. Thereby arises an element matrix $A^{(e)}$ whose individual entries can be computed as
\begin{equation}
A_{r,s}^{(e)} = \int_{\Omega^{(e)}} \nabla \varphi_r \nabla \varphi_s \, \mathrm{d}\Omega^{(e)} - k^2 \int_{\Omega^{(e)}} \varphi_r \varphi_s \, \mathrm{d}\Omega^{(e)}
\end{equation}
with $r,s$ the indices of the individual nodes in an element. For 1D linear Lagrange elements, i.e. elements with two nodes each having one degree of freedom per node, a $2\times 2$ matrix results and for 1D quadratic Lagrange elements with three nodes each a $3\times3$ matrix results.
%
Also the right hand side vector, now called $\bm{b}$, is computed element-wise using
\begin{equation}
b_r^{(e)} = \rho \omega^2 \int_{\Omega^{(e)}} \int_{\Gamma_{e,N}} \varphi_r U_n \,\mathrm{d}\Gamma_{e,N} \;.
\end{equation}


The element matrices are then assembled to a global system matrix. Within an element, the coordinates of the nodes are $[0,...,d]$ with $d$ the dimension of the element. For linear elements, the coordinates are $[0,1]$. For assembly they need to be mapped to the global coordinate system. This is accomplished by using a mapping function, noted as $q$ which maps $[r,s]$ of the elements to $[i,j]$ in the global system: $i=q(e,r)$ and $j = q(e,s)$ with $e$ the number of the element. For the global system matrix there now holds
\begin{equation}
A_{q(e,r), q(e,s)} =  A_{r,s}^{(e)}
\end{equation}
for adding the entries of an element matrix to to global matrix.
The resulting matrix after assembly is sparse, here for three elements with linear ansatz functions:
\begin{equation}
A_{q(e,r), q(e,s)}  =
\begin{bmatrix}
\textcolor[rgb]{0.49,0.83,0.13}{A_{0,0}^{( 0)}} & \textcolor[rgb]{0.49,0.83,0.13}{A}\textcolor[rgb]{0.49,0.83,0.13}{_{0,1}^{( 0)}} & 0 & 0\\
\textcolor[rgb]{0.49,0.83,0.13}{A}\textcolor[rgb]{0.49,0.83,0.13}{_{1,0}^{( 0)}} & \textcolor[rgb]{0.49,0.83,0.13}{A}\textcolor[rgb]{0.49,0.83,0.13}{_{1,1}^{( 0)}}\textcolor[rgb]{0,0,0}{+}\textcolor[rgb]{0.49,0.83,0.13}{\ }\textcolor[rgb]{0.82,0.01,0.11}{A_{0,0}^{( 1)}} & \textcolor[rgb]{0.82,0.01,0.11}{A}\textcolor[rgb]{0.82,0.01,0.11}{_{0,1}^{( 1)}} & 0\\
0 & \textcolor[rgb]{0.82,0.01,0.11}{A}\textcolor[rgb]{0.82,0.01,0.11}{_{1,0}^{( 1)}} & \textcolor[rgb]{0.82,0.01,0.11}{A}\textcolor[rgb]{0.82,0.01,0.11}{_{1,1}^{( 1)} \ }\textcolor[rgb]{0,0,0}{+}\textcolor[rgb]{0.29,0.56,0.89}{A_{0,0}^{( 2)}} & \textcolor[rgb]{0.29,0.56,0.89}{A}\textcolor[rgb]{0.29,0.56,0.89}{_{0,1}^{( 2)}}\\
0 & 0 & \textcolor[rgb]{0.29,0.56,0.89}{A}\textcolor[rgb]{0.29,0.56,0.89}{_{1,0}^{( 2)}} & \textcolor[rgb]{0.29,0.56,0.89}{A}\textcolor[rgb]{0.29,0.56,0.89}{_{1,1}^{( 2)}}
\end{bmatrix} \ \ 
\label{eqn:Assembly}
\end{equation}
%
For the global right-hand-side vector there holds
\begin{equation}
b_{q(e,r)} = b_r^{(e)} \; 
\end{equation}
what is, again, done for each elemental source vector.
With FEniCS, the system is assembled by 
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
A = assemble(a)
b = assemble(L)
\end{minted}
The left-hand side matrix is called the bilinear form since for both trial and test functions the basis functions are used. The right-hand side vector is called the linear form because only basis functions for the test functions are included.
The system of equations can now be solved for the vector of unknowns by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
self.u = Function(self.V) 
U = self.u.vector()
solve(A, U, b)
\end{minted}

\subsection{Setting Constraints}


\subsection{Solving and Convergence}
explain how the linear system is solved (LU decomp or Cholesky)







\chapter{Gaussian Processes and the Statistical Finite Element Method}

\section{Basic Probability Theory}
The statistical Finite Element Method (statFEM) is based on Gaussian Processes (GPs) and Bayesian inference. To be able to work with both, some basic understanding of probability theory is necessary.

\subsection{Gaussian Distribution}
speak about linearity somewhere! see Römer lectures

explain PDF/CDF and moments

explain the multivariate gaussian

A random variable $X$ is considered normally, or Gaussian, distributed if $X \sim \mathcal{N}(\mu, \sigma)$ with $\mu$ the mean and $\sigma$ the standard deviation. The shape of the PDF is a Bell curve.

From any distribution, the moments of the random variable can be deduced. Using the PDF there holds for the mean
\begin{equation}
\mu = \mathbb{E}[X] = \int_{\mathbb{R}} x f_X(x) \, \mathrm{d}x
\end{equation}
%
and for the variance
\begin{equation}
\mathbb{V}[X] = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 \;.
\end{equation}
%
For the covariance between two random variables $X$ and $Y$ there holds (see \cite[p. 1434]{arens2015})
\begin{equation}
\mathrm{cov}[X,Y] = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y]\;.
\label{eqn:Covariance}
\end{equation}
It is clearly visible that $\mathrm{cov}[X,X] = \mathbb{V}[X]$.
%
For the PDF of the Gaussian distribution there holds
\begin{equation}
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2) \;.
\end{equation}
%
If multiple random variables are observed, which are all normally distributed, they can be aggregated in a so-called multivariate Gaussian disribution. Instead of a random variable it is a random vector which is defined by a mean vector and a covariance matrix. The matrix is named covariance and not variance matrix because not only the variance of the individual random variables but also the covariance among the different variables makes up entries of it. It is similar to auto- and cross correlation, both described in a single matrix.
For the PDF of a multivariate Gaussian distribution there holds
\begin{equation}
f_X(\bm{x}) = \frac{\exp(-\frac{1}{2}(\bm{x}-\bm{\mu})^T \Sigma^{-1}(\bm{x}-\bm{\mu}))}{\sqrt{(2\pi)^k \lvert \Sigma \rvert}}
\label{eqn:MultivariateGaussian}
\end{equation}
with $\Sigma$ the covariance matrix, $\bm{\mu}$ the mean vector and $k$ the dimension, i.e. the number of random variables, of the distribution.
An example of a multivariate Gaussian is visible in Figure \ref{fig:MultiGauss}. The underlying mean vector and covariance matrix read
\begin{equation}
\bm{\mu} = \begin{bmatrix}
           0 \\
           1
         \end{bmatrix}
, \quad     
\Sigma = \begin{bmatrix}
1 & -0.7 \\
-0.7 & 1.5 
\end{bmatrix} \;.
\end{equation}
The smaller the absolute of the non-diagonal entries of the covariance matrix is, the less correlated the individual variables are.

\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{pics/Gaussians}
\caption{A multivariate Gaussian distribution with two dimensions. A strong correlation between the two random variables is visible. [scipy tutorial] }
\label{fig:MultiGauss}
\end{center}
\end{figure}

\subsection{Bayesian Inference}
$posterior = \frac{likelihood x prior}{marginal likelihood}$ (Rasmussen p.9)
explain basics of bayesian inference with bayes rule etc. what is posterior/prior 
explain what the marginal likelihood is

Bayesian Inference uses Bayes' law to infer a posterior density from a prior distribution and data. The data "updates" the prior. The law states that
\begin{equation}
p(\bm{u}|\bm{y}) = \frac{p(\bm{y}|\bm{u})p(\bm{u})}{p(\bm{y})}
\end{equation}
where $p(\bm{u}|\bm{y})$ is the posterior, $p(\bm{y})$ is the likelihood, $p(\bm{u})$ is the prior and $p(\bm{y})$ is the marginal likelihood, also called evidence. The posterior to be infered is the new probability density describing the sought quantity after observing data $y$. The data, i.e. measurements, aid understanding the problem what leads to a smaller variance in the posterior as comapared to the prior. The prior density describes the prior belief of the underlying process. It can be deduced from expert knowledge or from previous measurement. 
The likelihood describes the probability of the data to be measuered at all given that, in the case of this thesis, the FEM solution holds: "How likely is the measured data to be true, according to the model?"
The marginal likelihood (see section \ref{sec:Marg} for more detail) serves as a normalization constant and describes how likely it is to measure the observed data.


\section{Gaussian Process Regression}

\subsection{Regression}
"Regression is used to find a function that rerpesents a set of data points as closely as possible" \cite{gortler2019}
\cite{gortler2019} "A Gaussian process is a probabilistic method that gives confidence for the predicted function"

"GPs allow us to make predictions about our data incorporating prior knowledge"



The following closely follows \cite{rasmussen2006}.


Gaussian Processes are a class of Bayesian non-parametric models. Non-parametric doesn't mean that there are no parameters involved but rather that there is an infinite number of them. Every realization of a Gaussian process doesn't yield a scalar or vector but a function. One can think of a Gaussian Process as a collection of infinitely many normally distributed random variables, i.e. a generalization of a Gaussian distribution: A vector with infinitely many entries is basically a function. By picking out a finite set of those random variables when discretizing e.g. on an FEM mesh, one obtains a multivariate distribution which is determined by a mean and a covariance matrix. \cite[p. 2]{rasmussen2006} Hence, the GP assigns a confidence band to a function where a usual polynomial regression wouldn't.

A GP is described by its mean function and the covariance function, also called kernel.






\subsection{Gaussian Processes}
\paragraph{Marginalization and Conditioning} 
\label{sec:Marg}
Marginalisation means that out of a multivariate Gaussian the single variables can directly be taken out of the mean vector and the covariance matrix. E.g. for a point on the axis describved by the GP, a single value can be picked out. A univariate Gaussian comes out by simply using the mean vectors value at that point and the i,i th index of the covariance matrix. This means that the underlying distributions for single variables do not change if a GP is used to describe an arbitrary set of variables [p13].
[Visual Exploration]
From a multivariate, in this case bivariate, Gaussian
\begin{equation}
P(X,Y) = \begin{bmatrix}
           X \\
           Y
         \end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}
           \mu_X \\
           \mu_Y
         \end{bmatrix}, \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX} & \Sigma_{YY} 
\end{bmatrix}  \right)
\end{equation}
one single random variable, say $X$, can be marginalized out by simply only looking at the entries corresponding solely to $X$:
\begin{equation}
X \sim \mathcal{N}(\mu_X, \Sigma_{XX}) \;.
\end{equation}
If the covariance matrix and mean vector are not known, a random variable can be marginaluzed out of an arbitrary distribution by
\begin{equation}
p_X(x) = \int_y p_{X,Y}(x,y)\,\mathrm{d}y = \int_y p_{X|Y}(x|y)p_Y(y)\,\mathrm{d}y
\label{eqn:margi}
\end{equation}
which intuitively means that summing the probability for $X$ at every possible $Y$ gives the total probability for $X$ independent of $Y$. 

(\ref{eqn:margi}) already uses the principle of conditioning. The condition operator $|$ yields the probability of one variable under the condition that another variable is true. As an illustration in words one could ask: "What is the probability of event $X$ to happen under the condition, that the event $Y=y$ happens simultaneously?"
For the mutlivariate Gaussian there holds for the conditioning
\begin{equation}
X|Y \sim \mathcal{N}(\mu_X +\Sigma_{XY} \Sigma_{YY}^{-1}(Y-\mu_Y), \Sigma_{XX} - \Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX} ) \;.
\end{equation}
%
Conditioning yields a so-called modified Gaussian distribution which can be imagined as a "cut" through a multivariate Gaussian $P(X,Y)$ at a certain point $y$. A new Gaussian with the dimension reduced by $1$ arises as illustrated in Figure \ref{fig:GaussCut3d}.

%
%
\begin{figure}[h]
\begin{center}
\includegraphics[width=0.6\textwidth]{pics/Gaussians3dCut}
\caption{Cutting, i.e. conditioning, a bivariate Gaussian at the dashed line leads to a new Gaussian with only one dimension.}
\label{fig:GaussCut3d}
\end{center}
\end{figure}



-Regression in general
-differences of a GP regression to e.g. a polynomial regression: every point is assgined an uncertainty

-a distribution over functions

-Herleitung der Kovarianzmatrix, evtl. auch für das standard linear regression modell

-parameter variations

\subsection{Kernel Functions}

If a kernel function is evaluated at a finite number of points, one gets a covariance matrix. This can be used as the covariance of a Gaussian vector from which samples can be drawn. The values of each entry are a function over the input points of the kernel function. \cite[p.14]{rasmussen2006}

In a covariance function all the assumptions of the function to be modeled are contained. The kernel defines or rather assesses how near or similar a value of one point is to the one of another. A training point which is close to a test point therefore gives information on how the value at the test point should be. \cite[p.79]{rasmussen2006}

\paragraph{Stationary Kernels} are a function of $x-x'$. They are stationary because it doesn't matter what value $x$ and $x_prime$ have as a starting value, the difference is always the same. Example: $1100mm -1000mm = 100 mm, 100100mm-100000mm = 100mm$


p80: A radial basis function is a function which is !only! a function of $r=x-x'$ and whioch is therefore isotropic, i.e. there are no rigid motions. r is like a radius so these functions are called radial basis functions.

Covariance matrices always have to be symmetric, i.e. $k(x-x') = k(x'-x)$, since everything else wouldnt make sense

The so-called Gram matrix is the function containing $x-x'$ evaluated at a set of points $x_i$. if the function is a covariance funtion, the gram matrix is called covariance matrix.
\paragraph{The Squared Exponential Kernel Function} is the most commonly used kernel function for GPs. It is stationary and infinitely differentiable, hence very smooth. 
It is defined as
\begin{equation}
k_{SE} = \sigma \exp(-\frac{r^2}{2l^2})
\end{equation}
%
and can be defined as a Python function with
[Code von Murphy] 
\begin{python}
def squared_exponential(xa, xb, l, sig):
    sq_norm = -0.5 * scipy.spatial.distance.cdist(xa, xb, 'sqeuclidean') \
    	* (1/l**2)
    return sig**2 * np.exp(sq_norm) .
\end{python}
\label{lst:sqEx}


\begin{figure}[h]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/sqexp_f_sampled}
\caption{The source term GP of $f(x)$ sampled for a Squared Exponential kernel with $l = 0.25$ and $\sigma = 0.3$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green.}
\label{fig:Matern1_2}

\end{center}
\end{figure}

\paragraph{Positive Definiteness}A covariance function has to be positive semidefinite, i.e. all eigenvalues are greater or equal zero. A covariance matrix K is positive semidefinite when $Q(v) = v^T K v \geq 0$. Q is the quadratic form. A kernel function is positive semidefinite if it only outputs matrices which are positive semidefinite.


\paragraph{The Characteristic Length Scale} can be thought of as the number of "upcrossings" in the interval, i.e. the number of crossings of the x axis. It can be calculated with eq 4.3. rasmussen. The more upcrossings, the shorter the length scale.

\paragraph{Matern Kernel Functions}
\cite{rasmussen2006} The squared exponential kernel is infinitely differentiable, which means that it behaves very smoothly. This is considered as unphysical by \cite{stein1999}. Therefore, the Matern class of covariance functions \cite{matern2013} is recommended by \cite{stein1999} which is finitely differentiable and therefore less smooth. It is called a "class" of kernels because by varying the parameters, very different behaviours can be obtained. The squared exponential kernel is actually a special case of the Matern class.

The general Matern kernel is defined as
\begin{equation}
k_M(r) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}  \left( \frac{\sqrt{2\nu}r}{l}  \right)^{\nu} K_{\nu}  \left(  \frac{\sqrt{2\nu}r}{l}   \right)
\label{eqn:MaternKernel}
\end{equation}
with $\Gamma(\nu) = (\nu - 1)!$ the gamma function and $K_{\nu}$ a modified Bessel function \cite[p.84 ff.]{abramowitz2013}, \cite{rasmussen2006}. $\nu$ and $l$ are free positive parameters.
For $\nu \to \infty$  (\ref{eqn:MaternKernel}) becomes the squared exponential kernel.
The covariance function simplifies for half integer values of $\nu$ and can be expressed as a product of an exponential and a polynomial. According to \cite{rasmussen2006} $\nu = 3/2$ and $\nu = 5/2$ are commonly used for GPs; the simplest Matern kernel is obtained with $\nu = 1/2$:

\begin{align}
k_{M;\nu = 1/2}(r) &=   \sigma^2 \exp(- \frac{r}{l})  \; , \\
\label{eqn:Materns1}
k_{M;\nu = 3/2}(r) &=  \sigma^2 \left(      1+ \frac{\sqrt{3}r}{l}  	\right)  \exp(- \frac{\sqrt{3} r}{l})  \; , \\
k_{M;\nu = 5/2}(r) &=  \sigma^2\left(      1+ \frac{\sqrt{5}r}{l}  + \frac{5r^2}{3l^2}	\right)  \exp(- \frac{\sqrt{5} r}{l}) \; .
\label{eqn:Materns3}
\end{align}
%
(\ref{eqn:Materns1}) - (\ref{eqn:Materns3}) are used in this thesis. Figures \ref{fig:Matern1_2} - \ref{fig:Matern5_2} show the differences between the different choices for $\nu$: The higher it is chosen, the smoother the GP becomes and the closer it gets to the squared exponential kernel.
\begin{figure}[h]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/matern1_2_f_sampled}
\caption{The source term GP of $f(x)$ sampled for a Matern kernel with $\nu=1/2$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green.}
\label{fig:Matern1_2}

\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/matern3_2_f_sampled}
\caption{The source term GP of $f(x)$ sampled for a Matern kernel with $\nu=3/2$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green.}
\label{fig:Matern3_2}

\end{center}
\end{figure}

\begin{figure}[h]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/matern5_2_f_sampled}
\caption{The source term GP of $f(x)$ sampled for a Matern kernel with $\nu=5/2$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green.}
\label{fig:Matern5_2}

\end{center}
\end{figure}


\paragraph{Periodic Kernel Functions}

\paragraph{Combination of Different Kernel Functions}


hyperparameters

hyperparameter learning

marginal likelihood: integral of the likelihood times the prior
marginal means marginalization over the function values so these dont appear in the equation anymore. in this case it's marginalizing over the test points X so those dont appear anymore.


\subsection{Prior and Posterior}
\paragraph{Prior}
explain, how a sample from the multivariate gaussian canbe drawn (A2 Rasmussen).
To obtain a sample from the mutlivariate Gaussian which represents the GP, at first the Cholesky decomposition of the covariance matrix $\Sigma = LL^T$ has to be computed. Next, a sample from a standard multivariate normal distribution $\bm{u} \sim \mathcal{N} (\bm{0},\bm{I})$ is drawn. The final sample $\bm{x}$ can now be obtained using
\begin{equation}
\bm{x} = \bm{m} + L \bm{u} 
\end{equation}
with the prescribed mean vector $\bm{m}$.
With Python and numpy, the sample is obtained with
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
x = np.random.multivariate_normal(
			mean = mean, cov=Sig,
			size=1)
\end{minted}
The numpy function does the Cholesky decomposition internally.
	
	
			
\paragraph{Occam's Razor}

\paragraph{Posterior after observation}
p16:
The prior is conditioned on the observations. It would also be possible, abut computationally demanding, to sample many many functions out of the GP and only take those which go through the training points.

explain conditioning



\section{The Statistical Finite Element Method}
The FEM is based on a model in form of a differential equation which is used to describe some kind of phenomenon, in the case of this thesis vibroacoustics, as accurately as possible. However, every model is only an approximation of reality which gives rise to a model inadequacy error: There is some unknown physics hidden between the model and reality. By collecting data, basically measuring reality with some kind of measurement error, that discrepancy $d$ between model and reality can be infered. That is what statFEM aims to do: It finds a new model, based on data, which solves the model inadequacy of the FEM.

\paragraph{The Statistical Generating Model}
Measuring data always involves a measurement error: One can never measure any physical quantity without some measurement noise. Therefore, the so-called statistical generating model \cite{girolami2021}, \cite{kennedy2001} for a vector of $n_y$ measurements $\bm{y} \in \mathbb{R}^{n_y}$, 
%
\begin{equation}
\bm{y} = \bm{z} + \bm{e} = \rho \bm{P} \bm{u} + \bm{d} + \bm{e} \; ,
\label{eqn:statGen}
\end{equation}
%
consists of response of the true underlying process $\bm{z} \in \mathbb{R}^{n_y}$ at the measurement points and the measurement noise $\bm{e} \in \mathbb{R}^{n_y}$. The true process $\bm{z}$ can further be expressed as a combination of the used model $\bm{u} \in \mathbb{R}^{n_u}$, which tries to describe the true process, and a model discrepancy error $d$ which accounts for the model not being a completely accurate representation of the true physics. The model is scaled using the scaling parameter $\rho \in \mathbb{R}$. It can be evaluated at $n_u$ points which are determined by the FEM mesh but only its evaluations at the measurement points are taken into account by using the projection matrix $\bm{P} \in \mathbb{R}^{n_y \times n_u}$. $n_u$ is the number of the so-called degrees of freedom of the domain, i.e. the number of nodes on which a solution is approximated.
%
$e$ is modeled as a multivariate Gaussian 
%
\begin{equation}
\bm{e} \sim p(\bm{e}) = \mathcal{N}(\bm{0}, \bm{C_e})
\end{equation}
with zero mean and the covariance matrix $\bm{C_e} = \sigma_e^2 \bm{I}$ which adds the measurement noise to each entry of $\bm{z}$.
%
The model discrepancy error is, in order to account for a possibly complex behavior, modeled as a GP
\begin{equation}
\bm{d} \sim p(\bm{d} | \sigma_d, l_d) = \mathcal{N}(\bm{0},\bm{C_d})
\end{equation}
which $\sigma_d, l_d$ the unknown parameters of a squared exponential kernel function. Evaluating it at the measurement positions, the GP is represented by a multivariate Gaussian with a covariance matrix $\bm{C_d} \in \mathbb{R}^{n_y \times n_y}$.




\paragraph{Basic statFEM procedure}
statFEM can basically be broken down into three major steps \cite{girolami2021}. The first one is applying Bayesian inversion to the FEM part of the solution. The prior GP is constructed and evaluated before data is introduced. Equations are derived which incorporate the data in order to compute a posterior density for the FEM solution GP. The equations for that solution are dependent on certain hyperparameters. These are estimated in the second step: For each of the hyperparameters a posterior density is calculated using a prior and the marginal likelihood. The third and last step is in principle finding a proper mesh size which is able to describe data the best. This last step may appear rather unimportant since usually in FEM the consensus is that a finer mesh leads to a more accurate result. However, it will be demonstrated that the optimum element size strongly depends on the value of certain hyperparameters estimated in step 2 and that there is a correlation between relatively inaccurate statFEM results and a too small element size.

\subsection{Step 1: Posterior of the FEM Forward Model}

\paragraph{Assembling the FEM System Matrix}
The FEM assembly and solving is handled by Fenics. To compute the system matrix and to later solve for the vector of unknowns, the PDE has to be defined in a variational formulation as a boundary value problem.
At first, the FEM solver is initiated by defining the domain, the wished number of elements and the kind of elements to be used. In this example, a 1 dimensional mesh with Lagrange basis functions is used.
\begin{python}
def __init__(self, nMC = 200):
	self.dom_a = 0.0 #domain boundaries
	self.dom_b = 1.0
	self.ne	= 100 #number of elements
	self.mesh = IntervalMesh(self.ne,self.dom_a,self.dom_b) #define mesh
	self.bbt = self.mesh.bounding_box_tree()
	self.coordinates = self.mesh.coordinates() # vertices vector
	self.V = FunctionSpace(self.mesh, "Lagrange", 1) # Function space
\end{python}
The variational formulation for the chosen PDE consists of a linear form $L$ and a bilinear form $a$. These need to be specified individually for Fenics. The boundary condition is set to $u_0 = 0.0$ for both boundaries in the 1D mesh. The system matrix $A$ can be returned by calling \mintinline{python}{A = assemble(a)}. The boundary conditions are applied to $A$ after assambly.

\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def doFEM(self):
	# Define Dirichlet boundary (x = 0 or x = 1)
	def boundary(x):
		return x[0] < (self.dom_a + DOLFIN_EPS) or x[0] > self.dom_b - DOLFIN_EPS
	# Define boundary condition
	u0 = Constant(0.0)
	self.bc = DirichletBC(self.V, u0, boundary)
	# Define variational problem
	self.u = TrialFunction(self.V)
	self.v = TestFunction(self.V)
	# find index of cell which contains point.
	class DoGP(UserExpression):
		def eval(self, value, x):
			collisions1st = bbt.compute_first_entity_collision(Point(x[0])) 
			value[0] = f_vals[collisions1st] # f is constant in a cell.
		def value_shape(self):
			return ()
	f_vals = self.draw_FEM_samples(coordinates)
	f_vals = f_vals[0]
	f = DoGP()
	
	a = dot(grad(self.u), grad(self.v))*dx #variational Problem
	L = f*self.v*dx

	A = assemble(a)
	b = assemble(L)
	self.bc.apply(A, b)
	
	U = self.u.vector()
	solve(A, U, b)
	return U, A

\end{minted}


\paragraph{Generate and Sample From Source Term GP}
The differential equation is treated from a Bayesian viewpoint: all parameters are random variables. For dependent parameters, such as a source term $f(x)$ dependent on the spatial coordinate, not a single distribution but a Gaussian Process is applied. Therefore prior to solving the FEM linear system, the parameter $ \mathcal{GP} ( \bar{f},C_f)$ is sampled. That sample is evaluated for each cell in the FEM mesh which makes it necessary to assemble the system matrix with that in mind. 
$\bar{f}$ is set to a prescribed value. $C_f$ is calculated using the prescribed hyperparameters of the chosen kernel function. As an example, a sample from $ \mathcal{GP} ( \bar{f},C_f)$ can be drawn for a Matern kernel by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def sample_f(self):
	c_f = self.matern_log(self.coordinates,
		self.coordinates, l=np.log(lf), sig=np.log(sigf))
	f_mean = (np.pi)**2*(1/5)*np.ones(self.ne+1)
	fGP = np.random.multivariate_normal(
		mean = f_mean, cov=c_f,
		size=10)
\end{minted}
Here, the Matern kernel accepts only the $\log$ of the parameters $lf$ and $sigf$.

\paragraph{Generate and sample from the diffusion coefficient GP}
If the diffusion coefficient $\kappa$ is assumed to be a random variable and dependent on $x$, it can be modeled as a GP as well and there holds
\begin{equation}
\kappa \sim \mathcal{GP}(\bar{\kappa},C_{\kappa})
\end{equation}

\paragraph{Computing the FEM Prior}
Having the GPs for the input parameters defined, the FEM system of equations can be solved. With the input parameters defined as a GP, also the FEM solution is going to be a GP. Therefore, 
\begin{equation}
u \sim \mathcal{GP}(\bar{u}, C_u)
\end{equation}
is to be calculated. Multiplying the FEM system matrix by $\bm{A}^{-1}$ from the left there holds
\begin{equation}
\bm{u} = \bm{A}^{-1}\bm{f} \;.
\end{equation}
\cite[p. 1428]{arens2015} states that the expectation operator $\mathbb{E}$ is linear for independent random variables $X_1,X_2,...,X_n$ what means that
\begin{equation}
\mathbb{E}\left[  \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \mathbb{E}[X_i] \;.
\end{equation}
A GP can be described by a set of independent normally distributed random variables. Because of the linearity there holds
\begin{equation}
\bar{u} = \mathbb{E}[\bm{A}^{-1} \bm{f}] = \bm{A}^{-1} \mathbb{E}[\bm{f}] = \bm{A}^{-1} \bar{f}
\end{equation}
and, acc. to \cite{schon2011, bardsley2018}, making use of the definition of covariance (\ref{eqn:Covariance}),
\begin{align}
\begin{split}
C_u &= \mathbb{E}[(\bm{u}-\bar{u})(\bm{u}-\bar{u})^T] \\
	&= \mathbb{E}\left[\left((\bm{A}^{-1} \bm{f}) - (\bm{A}^{-1} \bar{f}) \right) \left( (\bm{A}^{-1} \bm{f}) - (\bm{A}^{-1} \bar{f}) \right)^T   \right]\\
	&= \mathbb{E}\left[  \left( \bm{A}^{-1}(\bm{f}-\bar{f})   \right)  \left( \bm{A}^{-1}(\bm{f}-\bar{f})   \right)^T  \right]\\
	&= \mathbb{E} \left[    \bm{A}^{-1} ( \bm{f} - \bar{f} ) ( \bm{f} - \bar{f} )^T \bm{A}^{-T}   \right]\\
	&= \bm{A}^{-1}   \mathbb{E}\left[ (\bm{f}-\bar{f}) (\bm{f}-\bar{f})^T   \right] \bm{A}^{-T} \\
	&= \bm{A}^{-1} C_f \bm{A}^{-T}  \;.
\end{split}
\end{align}
%	 \\
%	 \\
%	
There fore there holds for the multivariate Gaussian, which describes the new $\bm{u} \sim \mathcal{GP}(\bar{u}, C_u)$
\begin{equation}
\bm{u} \sim p(u) = \mathcal{N} \left( A^{-1} \bar{f}, A^{-1} C_f A^{-T}  \right) \;.
\label{eqn:u_GP_disc}
\end{equation}
As visible in (\ref{eqn:u_GP_disc}), there holds for $\bar{u}$
\begin{equation}
\bar{u} = A^{-1} \bar{f} \; ,
\end{equation}
which can be computed using Python by calling
\begin{python}
def get_U_mean(self):
	u_mean = Function(self.V)
	U_mean = u_mean.vector()
	b_mean = assemble(f_mean*self.v*dx) 
	self.bc.apply(b_mean)
	solstd,A = self.doFEM()  # solstd is unused here.
	solve(A,U_mean,b_mean)
	return U_mean .
\end{python}
\label{lst:get_u_mean}
$C_u$ is obtained by calculating
\begin{equation}
A^{-1} C_f A^{-T} \; .
\end{equation}
In Python code there holds
\begin{python}
def get_C_u(self):
	C_f = self.get_C_f()
	solstd,A = self.doFEM()
	ident = np.identity(len(self.coordinates))
	self.bc.apply(A)
	A = A.array()
	A_inv = np.linalg.solve(A,ident)
	thresh = 1e-16
	C_u = np.dot( np.dot(A_inv,C_f), np.transpose(A_inv))
	C_u = C_u + 1e-16*(self.sigf**2)*ident
	c_u = np.transpose(self.integratedTestF) * C_u * self.integratedTestF
	return C_u .
\end{python}
\label{lst:get_C_u}
With $\bar{u}$ and $C_u$ the GP for the prior is completely defined. 
\paragraph{Find Prior Mean and Variance}
Once the prior is computed, it can be sampled. The mean is already given but the parameters of the underlying GP are unknown. It is possible to evaluate $C_u$ for the standard deviation with
\begin{equation}
\bm{\sigma_u} =  \mathrm{diag}(C_u).
\label{eqn:varCu}
\end{equation}
This yields the diagonal entries of the covariance matrix in a vector which now represents the variance at all test points. 
In Python
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
C_u_sig = np.sqrt(np.diagonal(C_u))
\end{minted}
solves (\ref{eqn:varCu}).

\paragraph{The Projection Matrix $P$}
The projection matrix $P \in \mathbb{R}^{n_y \times n_u}$ is constructed by evaluating all $n_u$ FEM ansatz functions of the mesh at the $n_y$ measurement positions $y_i$. For each node there is a corresponding ansatz function which can be evaluated all over the domain. The result is a sparse matrix with most entries zero and only those entries non-zero which correspond to the ansatz functions $\Phi_i(\bm{\bm{x}})$ associated with the finite element in which the measurement positions lie. The matrix is used below to project the matrices resulting from the FEM prior onto the coordinates of the observations. Figure \ref{fig:PIllust} shows how the individual entries of $P$ are created for a 1D problem. Observation points, i.e. the training points, are displayed in red whereas all other points, i.e. test points, are displayed in green. The hat functions are $1$ only at one single node.
%
\begin{figure}[h]
\begin{center}

\begin{tikzpicture}[remember picture,scale=\textwidth/22.0cm,samples=200]
	\begin{pgfonlayer}{nodelayer}
		\node [style=none] (1) at (7, 0) {};
		\node [style=none] (2) at (-7, 0) {};
		\node [style=none] (3) at (-5, 0) {};
		\node [style=none] (4) at (-3, 0) {};
		\node [style=none] (5) at (-1, 0) {};
		\node [style=none] (6) at (1, 0) {};
		\node [style=none] (8) at (5, 0) {};
		\node [style=none] (9) at (3, 0) {};
		\node [style=none] (10) at (-7, 3) {};
		\node [style=none] (11) at (8, 0) {};
		\node [style=none] (12) at (7, 0) {};
		\node [style=none] (13) at (8, 0) {};
		\node [style=none] (14) at (8, 0) {};
		\node [style=green] (15) at (-7, 0) {};
		\node [style=green] (16) at (-5, 0) {};
		\node [style=green] (17) at (-3, 0) {};
		\node [style=green] (18) at (-1, 0) {};
		\node [style=green] (19) at (1, 0) {};
		\node [style=green] (20) at (3, 0) {};
		\node [style=green] (21) at (5, 0) {};
		\node [style=green] (22) at (7, 0) {};
		\node [style=none] (23) at (8.25, -0.25) {};
		\node [style=none] (24) at (8.25, 0) {$x$};
		\node [style=none] (32) at (-7.25, 2) {};
		\node [style=none] (33) at (-6.75, 2) {};
		\node [style=none] (34) at (-7.25, 2) {};
		\node [style=none] (35) at (-7.5, 2) {$1$};
		\node [style=none] (36) at (-7, 2) {};
		\node [style=none] (37) at (-5, 2) {};
		\node [style=none] (38) at (-3, 2) {};
		\node [style=none] (39) at (-1, 2) {};
		\node [style=none] (40) at (1, 2) {};
		\node [style=none] (41) at (3, 2) {};
		\node [style=none] (42) at (5, 2) {};
		\node [style=none] (43) at (7, 2) {};
		\node [style=none] (44) at (-7, -0.5) {$0$};
		\node [style=none] (45) at (-5, -0.5) {$1$};
		\node [style=none] (46) at (-3, -0.5) {$2$};
		\node [style=none] (47) at (-1, -0.5) {$3$};
		\node [style=none] (48) at (1, -0.5) {$4$};
		\node [style=none] (49) at (3, -0.5) {$5$};
		\node [style=none] (50) at (5, -0.5) {$6$};
		\node [style=none] (51) at (7, -0.5) {$7$};
		\node [style=none] (52) at (-5, 2.5) {$\varphi_1$};
		\node [style=none] (53) at (-3, 2.5) {$\varphi_2$};
		\node [style=none] (54) at (-1, 2.5) {$\varphi_3$};
		\node [style=none] (55) at (1, 2.5) {$\varphi_4$};
		\node [style=none] (56) at (3, 2.5) {$\varphi_5$};
		\node [style=none] (57) at (5, 2.5) {$\varphi_6$};
		\node [style=none] (58) at (7, 2.5) {$\varphi_7$};
		\node [style=none] (59) at (-6.5, 2.5) {$\varphi_0$};
		\node [style=new style 0] (60) at (-5, 0) {};
		\node [style=new style 0] (61) at (1, 0) {};
		\node [style=new style 0] (62) at (3, 0) {};
		\node [style=none] (63) at (-5, -1.25) {$y_0$};
		\node [style=none] (64) at (1, -1.25) {$y_1$};
		\node [style=none] (65) at (3, -1.25) {$y_2$};
	\end{pgfonlayer}
	\begin{pgfonlayer}{edgelayer}
		\draw [in=180, out=0] (2.center) to (1.center);
		\draw [style=arrow] (14.center) to (12.center);
		\draw [style=arrow, in=90, out=-90] (10.center) to (2.center);
		\draw [style=line] (15) to (22);
		\draw [style=line] (32.center) to (33.center);
		\draw [style=blue line] (36.center) to (16);
		\draw [style=red line] (15) to (37.center);
		\draw [style=red line] (37.center) to (17);
		\draw [style=red line] (17) to (18);
		\draw [style=red line] (18) to (19);
		\draw [style=red line] (19) to (20);
		\draw [style=red line] (20) to (21);
		\draw [style=red line] (21) to (22);
		\draw [style=green line] (16) to (38.center);
		\draw [style=green line] (38.center) to (18);
		\draw [style=green line] (18) to (19);
		\draw [style=blue line] (16) to (17);
		\draw [style=blue line] (17) to (39.center);
		\draw [style=blue line] (39.center) to (19);
		\draw [style=red line] (18) to (40.center);
		\draw [style=red line] (40.center) to (20);
		\draw [style=green line] (19) to (41.center);
		\draw [style=green line] (41.center) to (21);
		\draw [style=blue line] (20) to (42.center);
		\draw [style=blue line] (42.center) to (22);
		\draw [style=red line] (21) to (43.center);
	\end{pgfonlayer}
\end{tikzpicture}

\begin{equation}
P =
\begin{bmatrix}
0 & \rn{101}{1} & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \rn{102}{1} & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \rn{103}{1} & 0 & 0
\end{bmatrix}
\end{equation}

\caption{The matrix $P$ contains the values of the FEM ansatz functions belonging to the nodes on which observations are available. Since linear Lagrange elements yield $1$ at their respective node number and $0$ on all other nodes, $P$ consists of only $1$s and $0$s.}
\label{fig:PIllust}


\begin{tikzpicture}[overlay,remember picture]
	\draw [->] (3) to [out=-40,in=30] (101);
	\draw [->] (6) to [out=-50,in=60] (102);
	\draw [->] (9) to [out=-120,in=60](103);
\end{tikzpicture}
\end{center}
\end{figure}
%
This computation for linear ansatz functions can easily be implemented in Python using Fenics:
\begin{python}
def getP(self,y_points):
	ny = len(y_points)
	P = np.zeros((ny, self.ne+1), dtype = float) #with ne the number of elements
	for j,point in enumerate(y_points):
		x = np.array([point])
		x_point = Point(x)
		bbt = self.mesh.bounding_box_tree()
		cell_id = bbt.compute_first_entity_collision(x_point)
		cell = Cell(self.mesh, cell_id)
		coordinate_dofs = cell.get_vertex_coordinates()
		values = np.ones(1, dtype=float)
		for i in range(self.el.space_dimension()): 
			phi_x = self.el.evaluate_basis(np.array(i),x 
				,np.array(coordinate_dofs),cell.orientation())
			P[j,cell_id+i] = phi_x
	self.Pu = np.dot(P,self.U_mean)
	return P
\end{python}
\label{lst:getP}
\paragraph{Calculate $C_e$}
$C_e$ is the measurement error term. It is defined in Python by
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def get_C_e(self,size):
	sige_square = 2.5e-5
	C_e = sige_square * np.identity(size)
	return C_e
\end{minted}

\paragraph{Calculate $C_d$}
The model discrepancy term $C_d$ is modeled as a GP with mean $\bar{d} = 0$. Therefore it can be obtained by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def get_C_d(self,y_points,ld, sigd):
	C_d = self.matern_log(y_points, y_points, l=ld, sig=sigd)
	return C_d
\end{minted}
The points in the domain and the hyperparameters have to be provided in order to calculate the covariance function. The latter ones are not yet known and need to be estimated, see below for a thorough explanation on that.

\subsection{Inference of the Posterior Density}
To infer the posterior density, observed data is necessary. These can be obtained by either actually measuring a physical process or by generating synthetic data which allows developing and improving the statFEM model already before having to set up an experiment and take real measurements beforehand.
%
Sampling from a synthetic source or from the real physical response yields the observation vector
%
\begin{equation}
\bm{y} = \bm{z} + \bm{e}
\end{equation}
%
with $\bm{z}$ the real response and $\bm{e}$ the measurement error which is also a part of $\bm{y}$ for the synthetic observations. It can be seen that the model mismatch error $\bm{d}$ is, as opposed to (\ref{eqn:statGen}), not part of the equation because it is only part of the FEM modeling error. 
%
The data is observed on multiple measurement points $y_i$ throughout the domain. It can be chosen if the data is assumed to be deterministic or to be random. For the deterministic case only one value is measured for each measurement point. In the random case many measurements are taken per point what leads to a probability distribution for each point.
%
Now, Bayes' rule can be applied to update the prior with the observations \cite{girolami2021}: 

\begin{equation}
p(\bm{u}|\bm{y}) = \frac{p(\bm{y}|\bm{u})p(\bm{u})}{p(\bm{y})} \propto p(\bm{y}|\bm{u})p(\bm{u}) \;.
\end{equation}
The marginal likelihood is, in form of a normalization term, going to be reintroduced in the end. Both densities $p(\bm{y}|\bm{u})$ and $p(\bm{u})$ are considered multivariate Gaussian, see (\ref{eqn:statGen}) and (\ref{eqn:MultivariateGaussian}):
\begin{align}
p(\bm{y}|\bm{u}) &\sim \mathcal{N} (\rho P \bm{u}, C_d + C_e) \\
p(\bm{u}) &\sim \mathcal{N}(\bm{\bar{u}},C_u)
\end{align}
%
That property makes the multiplication simple, again excluding the marginal likelihood:
\begin{equation}
p(\bm{u}|\bm{y}) \propto  \exp(  (\rho P \bm{u} -\bm{y})^T (C_d + C_e)^{-1} (\rho P \bm{u} -\bm{y})  ) \exp(  (\bar{\bm{u}} - \bm{u})^T C_u^{-1}  )
\end{equation}
Following \cite{girolami2021}, with
\begin{align}
\begin{split}
B &= \rho^2 P^T (C_d +C_e)^{-1} P + C_u^{-1} \\
\bm{a} &= \rho P^T (C_d +C_e)^{-1} \bm{y} + C_u^{-1} \bm{\bar{u}}
\end{split}
\end{align}
there holds
\begin{align}
\begin{split}
p(\bm{u}|\bm{y}) &\propto \exp(\bm{u}^T B \bm{u} - 2\bm{a}^T\bm{u} + ... ) \\
&= \exp( (B^{-1} \bm{a} - \bm{u} )^T  B (B^{-1} \bm{a} - \bm{u} ) ) \;.
\label{eqn:p_u_y_unnorm}
\end{split}
\end{align}
Comparing \ref{eqn:p_u_y_unnorm} to \ref{eqn:MultivariateGaussian} it can be argued that \ref{eqn:p_u_y_unnorm} can be normalized to 
\begin{equation}
p(\bm{u}|\bm{y}) = \frac{1}{\sqrt{(2\pi)^{n_u} \lvert B \rvert}} \exp( (B^{-1} \bm{a} - \bm{u} )^T  B (B^{-1} \bm{a} - \bm{u} ) )
\end{equation} 
%
The result of this operation is
%
\begin{equation}
p(u|y) = \mathcal{N}(\bar{u}_{|y}, C_{u|y})
\end{equation}
%
with
%
\begin{equation}
\bm{\bar{u}}_{|y} = C_{\bm{u}|\bm{y}} \left(   \rho P^T  (C_d + C_e)^{-1}  y  +  C_u^{-1}  \bar{u}   \right)
\end{equation}
and
\begin{equation}
C_{\bm{u}|\bm{y}} = \left(      \rho^2  P^T   (C_d + C_e)^{-1}  P  +  C_u^{-1}    \right)^{-1} \;.
\end{equation}
To avoid having to invert matrices with the size of $n_u \times n_u$, the Sherman-Morrison-Woodbury identity \cite{riedel1992} is applied \cite{girolami2021} in order to only invert matrices with the size of $n_y \times n_y$ which yields
\begin{equation}
C_{\bm{u}|\bm{y}} = C_u - C_u P^T (\frac{1}{\rho^2} (C_d + C_e) + P C_u P^T)^{-1} P\,C_u \;.
\end{equation}

\subsection{find Posterior mean and variance}


\subsection{Modeling Parameters of the PDE}

\subsection{Probabilistic FEM Prior}
The prior before observation is obtained by computing the FEM response $u_h$ of the PDE given all parameters. The random parameters are modeled as GPs. Thereby result a mean $\bar{u}_h$ and a covariance matrix $C_u$. For this being the prior, the hyperparameters for the source term GP are chosen deterministically. The covariance matrix is obtained by calculating $C_u = A^{-1} C_{par} A^{-T}$ with $A$ the FEM system matrix and $C_{par}$ the covariance matrix of the respective random parameter's GP. The mean can easily be determined by calculating the deterministic mean of the FEM by using only the GP's mean.
The resulting response $u_h$ is, since it is discretized and not continuous, a multivariate Gaussian distribution.
%
To check convergence of the FEM model, it can be compared to the exact analytically determined system response $u$. 
%
It is assumed that there is a difference between the exact and the calculated FEM response to the true system response. That difference will be reduced by updating the prior with observations.
%
With the basic FEM code set up for a random source term and a deterministic diffusion coefficient, $\bar{u}$ can be obtained by calling


$C_u$ can be obtained by calling

For numerical reasons a small nugget is added to the diagonal of $C_u$. This does not significantly change the result but allows using the Cholesky decomposition.
With $\bar{u}$ and $C_u$ at hand the prior GP is defined as $u \sim \mathcal{GP}(\bar{u},C_u)$. To check if the covariance matrix is correct, a MC approximation $\bar{u}_{MC}$ of the mean can be computed by evaluating the GP $n_{MC}$ times and taking the average. As visible in Figure \ref{fig:u_mean_conv}, the error of $\bar{u}_{MC}$ converges to zero with a slope of 2 on a logarithmic scale. Therefore, $C_u$ indeed has a mean of $\bar{u}$.
The MC samples can be drawn via
\begin{python}
def samplePrior(self):
	U_mean = self.get_U_mean()
	C_u = self.get_C_u()
	priorGP = np.random.multivariate_normal(
		mean = U_mean, cov=C_u,
		size=self.nMC)
	return priorGP
\end{python}

Code for calculating u
direct calculation
MC approximation from GP samples
-error bands

Code for calulating Cu
direct calucaltion
MC approximation
error bands


\begin{figure}[h]
\includegraphics[width=0.6\textwidth]{pics/MCerrorConv.pdf}
\centering
\caption{Convergence of the error norm for the FEM prior mean}
\end{figure}
\label{fig:u_mean_conv}



\subsection{Step 2: Estimation of Hyperparameters }

Below it was already stated that the hyperparameters for computing the model discrepancy covariance matrix $\bm{C_d}$ are not known. The scaling factor $\rho$ is not known, as well. The measurement data can be used to estimate these parameters, collected in the hyperparameter vector $\bm{w}$, with Bayes' rule 
%
\begin{equation}
p(\bm{w}|\bm{y}) = \frac{P(\bm{y}|\bm{w}) p(\bm{w})}{\int P(\bm{y}|\bm{w}) p(\bm{w}) \, \mathrm{d}\bm{w}}
\end{equation}
%
where the denominator, which only serves as a normalization constant, can be dropped if only a point estimate of the hyperparameters is needed. This approach is indeed non-bayesian because no probability density of the hyperparameters is computed. For multiple measurements $y_i$ with a total number of $n_o$ there holds, because it is assumed that individual measurements are independent [Rasmussen p.9],
\begin{equation}
P(\bm{Y}|\bm{w}) = \prod_{i=1}^{n_o} p(\bm{y_i}|\bm{w}) \; 
\end{equation}
which replaces $P(\bm{y}|\bm{w})$.
If no prior knowledge of the parameters is available, it is possible to use an uninformative prior $p(\bm{w}) = 1$.
The equation then basically states that $p(\bm{w}|Y) \propto P(Y|\bm{w})$ which means that maximizing  $P(\bm{Y}|\bm{w})$, i.e. the likelihood to measure $Y$ given a set of hyperparameters, yields the optimal vector of hyperparameters for the given data. 
The marginal likelihood $P(\bm{y}|\bm{w})$, marginal because it has been marginalized over the function values $\bm{u}$ as in \ref{sec:Marg}, can be expressed as 
\begin{equation}
p(\bm{y}|\bm{w}) = \mathcal{N}(\rho \bm{P}\bar{u}, \bm{C_d}+\bm{C_e} + \rho^2 \bm{P} \bm{C_u} \bm{P}^T)
\end{equation}
because, as stated above, all components of the statistical generating model are Gaussian.
The equation for the corresponding PDF reads
\begin{equation}
p(\bm{y}|\bm{w}) = \frac{1}{(2 \pi)^{n/2} |\bm{K}|^{1/2} } \exp \left(   -\frac{1}{2} (\bm{y} - \rho \bm{P}\bar{u})^T \bm{K}^{-1} (y - \rho \bm{P}\bar{u})   \right)
\end{equation}
with
\begin{equation}
\bm{K} = \bm{C_d}+\bm{C_e} + \rho^2 \bm{P} \bm{C_u} \bm{P}^T \; .
\end{equation}
%
To make further calculations easier the exponential function can be removed by taking the negative $\log$ of the function (Rasmussen):
\begin{equation}
- \log p(\bm{y}|\bm{w}) = \frac{n}{2} \log(2 \pi) + \frac{1}{2} \log |\bm{K}| + \frac{1}{2}(\rho \bm{P}\bar{u} - \bm{y})^T \bm{K}^{-1} (\rho \bm{P}\bar{u} - \bm{y}) 
\label{eqn:neglog}
\end{equation}
This also improves numerical stability because products of possibly very small factors can become smaller than machine accuracy. The log replaces products with sums.
Because of taking the negative $\log$, (\ref{eqn:neglog}) now needs to be minimized instead of maximized. 

\paragraph{Cholesky Decompositon}
\cite[p.93-95]{chambers1998}
For numerical stability reasons $\bm{K}^{-1}$ should not be calculated directly. Instead, the linear system is solved. Additionally, if $\bm{K}$ is symmetric and positive-semidefinite, computing the lower triangular matrix using the Cholesky decomposition 
\begin{equation}
\bm{K} = \bm{L}\bm{L}^T
\end{equation}
makes the calculation quicker and yields the also needed determinant of $\bm{K}$ as a by-product. 
A generic linear system is defined as $A x = b$. It is solved for $x$, so the inverse of $A$ is needed. In this case for (\ref{eqn:neglog}) there holds $A=\bm{K}$ and $b = y$, since the inverse of $\bm{K}$ is meant to be found. Defining $x = \bm{K}^{-1}y$ there holds
\begin{equation}
\bm{K} \bm{K}^{-1}  y = y
\end{equation}
%
With $\bm{L}$ the lower triangular matrix of $\bm{K}$ obtained by 
\begin{lstlisting}[language=Python]
L = scipy.linalg.cho_factor(K)
\end{lstlisting}
the linear system can now be solved for $x = \bm{L}y$ with
\begin{lstlisting}[language=Python]
K_inv_y = scipy.linalg.cho_solve(L, y) .
\end{lstlisting}

The determinant of any triangular matrix is defined as the product of its diagonal entries \cite{tomaskovic-moore}. Therefore the lower triangular matrix $\bm{L}$ can be used to efficiently calculate the determinant of $\bm{K}$. There holds
\begin{equation}
\mathrm{det} \bm{K} = \mathrm{det} \bm{L} \mathrm{det} \bm{L}^T = \mathrm{det} \bm{L} ^2
\end{equation}
and therefore
\begin{equation}
\mathrm{det} \bm{K} = \left(    \prod_{i=1}^{n_y}  \bm{L}_{i,i}  \right)^2
\label{eqn:CholDet}
\end{equation}

Since in (\ref{eqn:neglog}) the $\log$ likelihood is used, (\ref{eqn:CholDet}) can be simplified to a sum:

\begin{align*}
\log{\mathrm{det} K} &= \log{\mathrm{det} L}^2 \\
&= 2 \sum_{i=0}^{n_y} \log{L_{i,i}}
\end{align*}

\subsubsection{Minimization of the Negative log-Likelihood}
The best possible set of hyperparameters $\bm{w}$ for (\ref{eqn:neglog}) to be as small as possible has to be found. That can be achieved by using a gradient based optimizer or by sampling the parameter space with, e.g., a MCMC approach.
\paragraph{MCMC}
Markov Chain Monte Carlo (MCMC) is a very efficient sampling method. Just as standard MC, it can be used to draw samples from a given distribution. The advantage of MCMC is now that an algorithm finds a relatively small set of samples which is able to describe the distribution very accurately. The result is a distribution for the hyperparameters of which e.g. the mean can be deduced. One common algorithm is the metropolis algorithm.
The metropolis algorithm is a special case of MCMC for a symmetric proposal distribution.
Metropolis algorithm as code. 



\paragraph{L-BGFS}
L-BGFS is a gradient based optimizer. It doesn't deliver a distribution for the hyperparameters but only a point estimate.
For the optimization with L-BGFS the derivative of (\ref{eqn:neglog}) is needed. It can be computed as follows.





\subsection{Step 3: Estimating the Optimum Mesh Parameters}


\chapter{Application of statFEM in Vibroacoustics}

\section{Simple 1D example}
\paragraph{Choice of PDE}
The Poisson equation 
%%
\begin{equation}
- \nabla \cdot \mu(x) \nabla u(x) = f(x)
\label{eqn:Poisson}
\end{equation}
%%
is chosen as the governing equation. It is an elliptic partial differential equation (PDE). In this work it is used as a simple 1D example to illustrate how the statistical FEM and especially the Gaussian Process Regression works.
The most standard form of it does not, contrary to this example, include $\mu(x) \in \mathbb{R}^+$ which is the diffusion coefficient dependent on the spatial variable $x$. The right-hand side consists of the source term $f(x)\in \mathbb{R}$. Both $\mu (x)$ and $f(x)$ are the free parameters in this case. The equation is solved for the unknown  $u(x)$ in the domain $\Omega = (0,1)$ with the boundary condition $u(x) = 0$ on $x=0$ and $x=1$.
%
For a first example there holds $\mu(x)=1$. $f(x)$ is modeled as a Gaussian Process [Cirak]
%
\begin{equation}
f(\bm{x}) \sim \mathcal{GP} \left( \bar{f}(\bm{x}), c_f(\bm{x},\bm{x}')\right) \;.
\end{equation}
%
\paragraph{Construction of the GP}
The mean function of the GP is set to $\bar{f}(x) = 1.0$. For the covariance function at first a squared exponential kernel 
%
\begin{equation}
c_f(\bm{x},\bm{x}') =    \sigma_{f}^2 \exp \left(-  \frac{\left \| \bm{x}-{\bm{x}}' \right \|^2}{2l_{f}^2} \right )       
\label{eqn:sqEx_f}
\end{equation}
%
is used with the standard deviation $\sigma_{f} = 0.1$ and the length scale $l_{f} = 0.4$.
In Python the kernel is directly implemented as a function which takes two lists and the parameters as input variables [Murphy]:

%
Here, the parameters are fixed but in a later example a method on how to infer the optimum position for these will be studied.

Having prepared the mean function and the kernel, which can be considered the prior in a GP regression setting, a sample of the GP can be drawn. For this points have to be chosen on which the kernel is evaluated. These are the test points and, according to [Cirak], correspond to the center of the FEM cells. This implies that there are as many test points as FEM cells and therefore the coordinates of the FEM mesh can directly be used to compute the covariance matrix. For that (\ref{eqn:sqEx_f}) is evaluated at $\bm{x} = \bm{x}'$ with $\bm{x}$ the vector of test points. 

A GP has the marginalization property: if you sample it at a finite number of points it yields a multivariate Gaussian distribution $\mathcal{N}(\bar{\bm{f}},\bm{C_f})$ with $\bar{\bm{f}}$ the mean vector and $\bm{C_f}$ the covariance matrix while still describing the underlying continuous sample.
According to [Rasmussen] sampling from a multivariate Gaussian distribution works as follows: To obtain $n$ samples from the prior, at first $n$ samples of a standard normal distribution, also called Gaussian white noise, $e \sim \mathcal{N}(0,1)$ have to be drawn. Computing the Cholesky decomposition of the covariance matrix $\bm{C_f} = LL^T$, which can also be thought of as taking the square root of a matrix, yields the lower triangular matrix $L$. Samples can now easily be drawn from $\bm{f} = \bar{\bm{f}} + Le$ which is a multivariate Gaussian distribution with a mean $\bm{f}$ and a covariance $\bm{C_f}$.

Sample: Normal distribution times std dev. GP with n points is basically a multivariate gaussian with n dimensions. therefore we need the std dev. for a univariate gaussian thats . 

For the observations made in a later step it is assumed that there is some measurement noise. This is modeled as an added variance $\sigma_{n}^2$ on the diagonal of the prior covariance matrix. An additional effect of this added noise is the improved numerical stability of the covariance matrix which is important for computing the Cholesky decomposition.


\subsection{Posterior}
Figure \ref{fig:DataCloseToCI} shows that, if the observations are made slightly out of the FEM confidence band, a posterior can be inferred which fits to the data.
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{pics/DataCloseToCI.pdf}
\centering
\caption{Per observation point 50 individual observations were used with a standard deviation of $2.5e-3$. 30 Elements.}
\end{figure}
\label{fig:DataCloseToCI}


Figure \ref{fig:DataPriorConflict} shows the inferred solution in the case of the data being measured as far out of the FEM prior confidence bands. A prior/data-conflict is visible: The solution isn't scaled correctly according to either data or FEM prior. The data lies way outside the possible FEM solutions. Observing more data would eventually move the curve further in direction of the data because the FEM prior looses importance as the data gains more weight. The posterior confidence band grows.
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{pics/DataPriorMismatch.pdf}
\centering
\caption{Per observation point 50 individual observations were used with a standard deviation of $2.5e-3$. 30 Elements.}
\end{figure}
\label{fig:DataPriorConflict}


\section{1D Vibroacoustics Example}
Helmholtz Equation. Looks similar to the Poisson equation but the right side differs. It's the eigenvalue problem for the laplace operator.



\subsection{FEM prior}


Figure \ref{fig:FEMGP} shows, how the FEM prior is formed. On the left face of the domain, the source term GP is applied. The mean and confidence bands are visible in red. Exciting the domain with the source leads to an uncertain FEM solution in the 2D domain. At $y_c = 0.7$, an arbitrary value, the domain is cut and the mean and error bands for that position are displayed in green. Clearly, uncertainty in the source leads to uncertainty in the FEM prior. A MC simulation for the variance converges to the calculated variance.
\begin{figure}[h]
\includegraphics[width=0.8\textwidth]{pics/SolutionCustom2D.pdf}
\centering
\caption{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 222Hz.}
\end{figure}
\label{fig:FEMGP}
Figure \ref{fig:varFieldPrior} shows the FEM prior variance for every point in the 2D solution space for $222Hz$. It is visible that the variance in some regeions is higher than in others. The regions of low variance correlate with the troughs of the mode shape.
%
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{pics/VarField.pdf}
\centering
\caption{The variance of the FEM prior for $222Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible.}
\end{figure}
\label{fig:varFieldPrior}
%
Figure \ref{fig:FEMGP320} shows again an overviewe for the FEM prior, but for $320 Hz$. 
%
\begin{figure}[h]
\includegraphics[width=0.8\textwidth]{/home/lucas/Documents/Masterarbeit/git/statfem/MA_LucasHermann/Python/SolutionCustom320Hz.pdf}
\centering
\caption{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 320Hz.}
\end{figure}
\label{fig:FEMGP320}
%
In Figure \ref{fig:varFieldPrior320} it is visible that the variance pattern changed to a periodic one.
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{/home/lucas/Documents/Masterarbeit/git/statfem/MA_LucasHermann/Python/VarField320Hz.pdf}
\centering
\caption{The variance of the FEM prior for $320Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible.}
\end{figure}
\label{fig:varFieldPrior320}
%
If instead of a sinusoidal source a flat one is chosen, the solution changes significantly, see Figure \ref{fig:FEMGPFlat320}. Certain mode shapes can be excited more strongly by using a corresponding boundary function.
\begin{figure}[h]
\includegraphics[width=0.8\textwidth]{/home/lucas/Documents/Masterarbeit/git/statfem/MA_LucasHermann/Python/SolutionCustomFlatSource320.pdf}
\centering
\caption{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 320Hz.}
\end{figure}
\label{fig:FEMGPFlat320}
The variance field, on the other hand, does not change significantly, see Figure \ref{fig:varFieldPriorFlat320}
\begin{figure}[h]
\includegraphics[width=0.7\textwidth]{/home/lucas/Documents/Masterarbeit/git/statfem/MA_LucasHermann/Python/VarFieldFlatSource320.pdf}
\centering
\caption{The variance of the FEM prior for $320Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible.}
\end{figure}
\label{fig:varFieldPriorFlat320}

\chapter{Conclusion and Discussion}

\lipsum[1-3] %\cite{Langer2019}

\section{Section 1}

\lipsum[4-7] %\cite{Yaghoubi:2017}


\printbibliography


%\begin{thebibliography}{4}
%
%\bibitem{campolina}
%5Campolina, B.: Vibroacoustic modelling of aircraft double-walls with structural links using %tatistical
%Energy Analysis (SEA). Acoustics [physics.class-ph]. Université de Sherbrooke; Université %Pierre
%et Marie Curie - Paris VI (2012)

%\bibitem{Langer2019}
%Langer, S.C., Blech, C.: Cabin noise prediction using wave‐resolving aircraft models. Proc. %Appl. Math. Mech., Vol. 12 (1) (2019): e201900388. doi:10.1002/pamm.201900388

%\bibitem {Yaghoubi:2017} Yaghoubi, V., Marelli, S., Sudret, B., Abrahamsson, T.: Sparse %polynomial chaos expansions of frequency response functions using stochastic frequency %transformation, Probabilistic Engineering Mechanics, volume 48, pages 39-58 (2017)
%\end{thebibliography}

\end{document}
