\documentclass[%
  a4paper,oneside,%
  %arial,
  11pt,% <10pt, 9pt>
  smallchapters,
  style=printdev,
  extramargin,
  %sender=bottom,
  green,% <orange, green, violet>
  rgb, <cmyk>
  %mono
  ]{tubsbook}

\usepackage[backend=biber,style=ieee,backref=true,citestyle=numeric-comp]{biblatex}

\addbibresource{Library.bib}

\usepackage[verbose]{placeins}

\usepackage{nomencl}
%----------------------------------------------
\usepackage{etoolbox}
\renewcommand\nomgroup[1]{%
  \item[\Large\bfseries
  \ifstrequal{#1}{N}{List of Symbols}{%
  \ifstrequal{#1}{A}{List of Abbreviations}{}}%
]\vspace{10pt}} % this is to add vertical space between the groups.
%----------------------------------------------
\makenomenclature
\usepackage[title]{appendix}
\usepackage{siunitx}
\sisetup{load-configurations = abbreviations,output-exponent-marker=\ensuremath{\mathrm{e}}}  
  
\usepackage{listings}  % for code stuff
\usepackage{minted}

\usepackage{booktabs}
\usepackage{multirow}

\usepackage{physics}

\usepackage{bbm} %for probability notation
\usepackage{subcaption}

\usepackage{tikz}
\usepackage{tikzit}
\input{MA.tikzstyles}
\newcommand{\rn}[2]{%% "rn": "remember node"
	\tikz[remember picture,baseline=(#1.base)]\node [inner sep=0] (#1)  {$#2$};%
}

%\usepackage{hyperref}

\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    morekeywords={self},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}


\newcommand\pythonstyle{\lstset{
	language=Python,
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    morekeywords={self},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}




%lstset{style=mystyle}

\usepackage{bm} 
\usepackage{amsmath}
 
\renewcommand{\familydefault}{\sfdefault}
\usepackage[utf8]{inputenc}
\RequirePackage{scrlfile}
\ReplacePackage{scrpage2}{scrlayer-scrpage}

\usepackage[ngerman,english]{babel}

\usepackage{lipsum} % Blindtext-Paket
\definecolor{InAGreen}{RGB}{172,193,58}

% Titelseiten-Elemente
\title{A Statistical Approach for the Fusion of Data and
Finite Element Analysis in Vibroacoustics}
\subtitle{Untertitel}
\author{Lucas Hermann}
%\logo{Institut fuer Lorem Ipsum}
\logo{\includegraphics{InA-Logo-rgb.pdf}}
\titleabstract{\lipsum[2]}
\titlepicture{infozentrum.jpg}
% Rückseiten-Elemente
\address{%
  Herr Mustermann\\
  Schlossallee 1\\
  33333 Darmstadt}
\backpageinfo{%
  \lipsum[5]
}
\usepackage{pdfpages}								% Einbinden von PDF-Pages
\usepackage[colorlinks,pdfpagelabels,pdfstartview = FitH,bookmarksopen = true,bookmarksnumbered = true,linkcolor = black,plainpages = false,hypertexnames = false,citecolor = black] {hyperref}
\usepackage[figure]{hypcap} 

\begin{document}

%\maketitle[image,logo=left]%[<plain/image/imagetext>,<logo=left/right>]
%\makebackpage[trisec]%[<plain/info/addressinfo>]
\includepdf[pages=-]{./Deckblatt/2099_StA_Name_Deckblatt.pdf}

\chapter*{Declaration}
Hiermit versichere ich, Lucas Hermann, durch meine Unterschrift, dass ich die
vorliegende Masterarbeit mit dem Titel ``A Statistical Approach for the Fusion of Data and Finite Element Analysis in Vibroacoustics'' selbständig und ohne Benutzung
anderer als der angegebenen Hilfsmittel angefertigt habe. Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten oder unveröffentlichten Schriften entnommen sind, habe ich als
solche kenntlich gemacht. Insbesondere sind auch solche Inhalte gekennzeichnet, die von
betreuenden wissenschaftlichen Mitarbeiterinnen und Mitarbeitern des Instituts für Akustik eingebracht wurden.

Die Arbeit oder Auszüge daraus haben noch nicht in gleicher oder ähnlicher Form dieser
oder einer anderen Prüfungsbehörde vorgelegen.

Mir ist bewusst, dass Verstöße gegen die Grundsätze der Selbstständigkeit als Täuschung
betrachtet und entsprechend der Prüfungsordnung geahndet werden.

\begin{flushright}
Braunschweig, \today
\end{flushright}

\vspace{2cm}
\hspace{2cm}\rule{5cm}{1pt}

\hspace{2cm}\small{Lucas Hermann} 

\chapter*{Abstract}
A problem with Finite Element Method (FEM) simulations is that they often do not reflect reality close enough in order to properly match given measurement data of a real system. In this thesis, the statFEM, i.e. statistical FEM, approach is adopted to fuse sensor data with an FEM model of a 2D vibroacoustics problem. The air in a cavity is excited on the boundary of the domain where the boundary function is considered random. The FEM model is considered as a Bayesian prior and the uncertainty is quantified in form of a Gaussian Process (GP). By conditioning the prior on given data, which are created synthetically, it is shown that a posterior Gaussian Process can be generated for the behavior of the model solution in the domain. Also, the data is decomposed in three parts: Model, misspecification and noise. The misspecification term is also based on a GP and the hyperparameters are learned according to the data. This combination makes it possible to describe the data much better in terms of an estimated mean and also the variance for each point in the domain. 

An introduction to vibroacoustics, FEM, GPs and statFEM is given before the methods are applied. It is then shown for a 1D and a 2D example that already comparatively few sensor locations and observations per sensor lead to a much more accurate posterior model. By adding more sensor locations and observations, the posterior variance in the whole domain can be decreased. The dependence of sensor placement is shown (the variance is decreased more strongly around sensors) and the limits of the method are explained.

\tableofcontents



\listoffigures
\listoftables





\nomenclature[N]{$\sim$}{distributed according to}
\nomenclature[N]{$\propto$}{proportional to}
\nomenclature[N]{$m$}{mass of a gas volume}
\nomenclature[N]{$v$}{velocity}
\nomenclature[N]{$\dot{v}$}{temporal derivative of v}
\nomenclature[N]{$p$}{sound pressure [Pa]}
\nomenclature[N]{$\Delta x$}{length of a gas volume in x direction}
\nomenclature[N]{$\Delta$}{Laplacian}
\nomenclature[N]{$t$}{time [s]}
\nomenclature[N]{$n_e$}{number of Finite Elements in a domain}
\nomenclature[N]{$\bm{K}^{(e)}$}{elemental matrix}
\nomenclature[N]{$\omega$}{angular frequency [rad]}
\nomenclature[N]{$c_0$}{speed of sound [m/s]}
\nomenclature[N]{$f$}{frequency [Hz]}
\nomenclature[N]{$k$}{wave number [1/m]}
\nomenclature[N]{$S$}{surface area}
\nomenclature[N]{$\rho$}{density [$\mathrm{kg/m^3}$]}
\nomenclature[N]{$\nabla$}{nabla operator}
\nomenclature[N]{$\bar{p}(x)$}{prescribed pressure at a Dirichlet boundary [Pa]}
\nomenclature[N]{$\Omega$}{Calculation Domain}
\nomenclature[N]{$\Omega_e$}{a single finte element as domain}
\nomenclature[N]{$\Gamma_D$}{Dirichlet boundary}
\nomenclature[N]{$\Gamma_N$}{Neumann boundary}
\nomenclature[N]{$\Gamma_{N0}$}{homogeneous Neumann boundary}
\nomenclature[N]{$\Gamma_R$}{Robin boundary}
\nomenclature[N]{$\bm{U}$}{Displacement on a Neumann boundary [m]}
\nomenclature[N]{$\vec{n}$}{normal direction at the boundary}
\nomenclature[N]{$\beta$}{specific normalized acoustic admittance [$\mathrm{(m^3/s)/Pa}$]}
\nomenclature[N]{$\mu$}{diffusion coefficient [$\mathrm{m^2/s}$]}
\nomenclature[N]{$f(x)$}{source term in the Poisson equation}
\nomenclature[N]{$e$}{approximation error}
\nomenclature[N]{$p_e$}{exact solution}
\nomenclature[N]{$R$}{Residual}
\nomenclature[N]{$V,W$}{Function spaces}
\nomenclature[N]{$\phi_i$}{FEM basis function}
\nomenclature[N]{$c_i$}{coefficient of an FEM basis function}
\nomenclature[N]{$v$}{test function}
\nomenclature[N]{$\oint_{\Gamma}$}{integral over the whole boundary}
\nomenclature[N]{$\int_{\Gamma_N}$}{integral over a part of the boundary}
\nomenclature[N]{$\mathcal{N}$}{distributed as a normal distribution}
\nomenclature[N]{$\mathcal{U}$}{distributed as a uniform distribution}
\nomenclature[N]{$\mathcal{GP}$}{distributed as a Gaussian Process}
\nomenclature[N]{$\delta_{ij}$}{Kronecker delta}
\nomenclature[N]{$\bm{\phi}$}{vector of all FEM basis functions}
\nomenclature[N]{$\delta\hat{\bm{p}}$}{vector of arbitrary variations}
\nomenclature[N]{$\hat{\bm{p}}$}{vector of weight factors in the FEM}
\nomenclature[N]{$\bm{A}$}{FEM dynamic stiffness matrix or also system matrix}
\nomenclature[N]{$\bm{K}_1,\bm{K}_2$}{elemental FEM system matrices}
\nomenclature[N]{$\bm{b}$}{FEM system of equations right hand side vector}
\nomenclature[N]{$\mu$}{mean of a normal distribution}
\nomenclature[N]{$\sigma$}{standard deviation of a normal distribution}
\nomenclature[N]{$f_X(x)$}{probability density function of random variable X}
\nomenclature[N]{$\mathbb{E}$}{expected value of a random variable}
\nomenclature[N]{$\mathbb{V}$}{variance of a random variable}
\nomenclature[N]{$\mathrm{cov}[X,Y]$}{covariance of the random variables X and Y}
\nomenclature[N]{$\bm{\Sigma}$}{covariance matrix}
\nomenclature[N]{$|\bm{\Sigma}|$}{determinant of $\bm{\Sigma}$ }
\nomenclature[N]{$f(\bm{u}|\bm{y})$}{probability of random variable $\bm{u}$ conditioned on random variable $\bm{y}$}
\nomenclature[N]{$\epsilon$}{Gaussian error term}
\nomenclature[N]{$\bm{\Phi}(x)$}{vector of polynomial basis functions}
\nomenclature[N]{$\mu$}{mean of a normal distribution}
\nomenclature[N]{$k$}{kernel function}
\nomenclature[N]{$l$}{characteristic length scale of a kernel}
\nomenclature[N]{$\Gamma(\nu)$}{Gamma function}
\nomenclature[N]{$K_{\nu}$}{Bessel function}
\nomenclature[N]{$\bm{L}$}{lower triangular matrix}
\nomenclature[N]{$\bm{y}$}{vector of measurements}
\nomenclature[N]{$\bm{z}$}{true underlying process GP}
\nomenclature[N]{$\bm{e}$}{measurement error GP}
\nomenclature[N]{$\bm{d}$}{model discrepancy GP}
\nomenclature[N]{$\bm{f}$}{source term GP}
\nomenclature[N]{$\bm{P}$}{projection matrix}
\nomenclature[N]{$\bm{u}$}{FEM model solution GP}
\nomenclature[N]{$\rho$}{scaling parameter}
\nomenclature[N]{$n_y$}{number of sensor locations}
\nomenclature[N]{$n_u$}{number of test points}
\nomenclature[N]{$\bm{w}$}{vector of hyperparameters}
\nomenclature[N]{$\bm{C}_d$}{covariance matrix for the model discrepancy GP, similar for other GPs}
\nomenclature[N]{$\mathcal{M}_j$}{model}
\nomenclature[N]{$\sigma_d$}{scaling hyperparameter for $\bm{d}$, similar for other GPs}
\nomenclature[N]{$l_d$}{length scale hyperparameter for $\bm{d}$, similar for other GPs}
\nomenclature[N]{$\bm{\kappa}$}{diffusion coefficient GP}
\nomenclature[N]{$\bm{\Psi}$}{orthogonal basis function}
\nomenclature[N]{$s[\bm{\tilde{u}}$]}{support of $\bm{\tilde{u}}$ }
\nomenclature[N]{$c_f(x,x')$}{kernel function for the source term GP, similar for other GPs}
\nomenclature[N]{$\odot$}{element-wise multiplication}
\nomenclature[N]{$\mathbb{1}$}{identity matrix}
\nomenclature[N]{$n_p$}{number of training points}
\nomenclature[N]{$n_o$}{number of observations}

%
%

\nomenclature[A]{BC}{Boundary Condition}
\nomenclature[A]{FEM}{Finite Element Method}
\nomenclature[A]{statFEM}{statistical Finite Element Method}
\nomenclature[A]{GP}{Gaussian Process}
\nomenclature[A]{L-BFGS}{Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm}
\nomenclature[A]{LU dec.}{lower-upper decomposition}
\nomenclature[A]{MCMC}{Markov Chain Monte Carlo}
\nomenclature[A]{MLE}{Maximum Likelihood Estimation}
\nomenclature[A]{ML}{Machine Learning}
\nomenclature[A]{MSE}{Mean Squared Error}
\nomenclature[A]{PDE}{Partial Differential Equation}
\nomenclature[A]{PDF}{Probability Density Function}
\nomenclature[A]{POD}{Proper Orthogonal Decomposition}
			







%\renewcommand{\nomname}{List of Symbols}
\printnomenclature[6em]


\chapter{Introduction}

%\textcolor{tubsSecondary}{Dies ist ein Text in \texttt{tubsSecondary}.}
%\textcolor{tubsViolet}{Dies ist ein Text in \texttt{tubsViolet}.}
%\textcolor{tubsGreenDark}{Dies ist ein Text in \texttt{tubsGreenDark}.}\bigskip
A core discipline in engineering is numerical simulation. It is needed in basic research and in the design process of any product to reduce the amount of practical experiments which are costly and time-consuming. In a numerical simulation, the relevant processes of reality are abstracted by a mathematical model, which a computer is able to solve \cite{Weisberg2015}.  However, a model is only able to describe reality up to a certain degree. It usually consists of a set of equations and corresponding parameters which can be optimized in order to make it behave as closely to the desired functionality as possible or to match data from measurements. Uncertainty in the parameters can be propagated to the solution, leading to a probability density for the solution which assigns probabilities to different possible outcomes. However, as this is a valid approach, as soon as the real problem behaves in a more complex manner than the model is able to describe, the procedure is reaching its limits. The real behavior of the problem is not considered in the solution space of the model anymore. %The headroom given by the probability densities in the parameters just isn't enough to tweak them in a way that makes the model describe reality accurately. 
There is now a gap between model and reality which has to be filled.

A common approach to form a model in engineering is the Finite Element Method (FEM) \cite{larson2013}. At the base of it stands a partial differential equation (PDE), which serves as a mathematical description of a model of reality. That model is a simplified version of reality. Assumptions are made and only certain phenomena are considered. The PDE is simplified using so-called ansatz functions which have weights that are optimized in order to get the most accurate picture of reality. Traditionally, the FEM is purely deterministic but, nevertheless, it is rather simple to form a Bayesian approach \cite[p. 151 ff.]{murphy2012} to FEM, in which the parameters and the solution are not scalars but random variables which have an underlying probability distribution. 
Recently a method called statFEM (statistical FEM) was developed \cite{girolami2021} which also allows for bridging the gap between a Bayesian FEM model and reality. This is in principle done by assuming the discrepancy between model and reality to behave in terms of a Gaussian Process (GP) \cite{rasmussen2006}. A GP can be, unlike a probability distribution over a parameter, considered as a probability distribution over a whole function. Since there are no parameters left in the FEM model to optimize, this is the only way to go: The most suitable function to fill the model discrepancy gap has to be found without building another parameterized model. The GP is based on measurement data: By comparing the FEM model with observed data and then updating it in a Bayesian manner, a distribution over functions can be found, which makes the now updated FEM model behave more closely to reality.

In this thesis, a statFEM approach for the field of vibroacoustics is formulated. The physical problem under consideration is an acoustical cavity which is excited at its boundary. The behavior of the fluid inside the cavity is described by the Helmholtz equation and depends strongly on the chosen boundary conditions \cite{atalla2015}. Obviously, there is a lot of uncertainty involved in the modeling of the boundary conditions: It is not possible to completely describe the behavior of the vibrating plate or the partially absorbing walls of the cavity. Nevertheless, it is possible to measure the sound pressure inside the cavity: Assuming the sound field to behave in terms of a GP makes it possible to form a posterior solution with reduced model error and uncertainty using the measured data and the prior in form of the FEM solution. The statFEM approach is therefore, in order to reduce uncertainty, able to fuse an FEM simulation with measured data and also to give an approximation of the mentioned gap between model and reality.

In the first chapters, the theory behind GPs, the FEM and statFEM is explained. Then, the methods are explained and validated using the academic example of the Poisson equation before they are applied to the more complex vibroacoustics problem. The behavior of the new model is explained and discussed in detail and information on possible use cases and limitations are given.

\paragraph{Literature Review}
\cite{langtangen2019} and \cite{larson2013} give a thorough explanation about the theoretical basis of the FEM. \cite{langtangen2016} gives details about the implementation of the FEM in Python using FEniCS. On a more abstract level, \cite{Lanczos1986} gives insight on variational formulations of PDEs which are essential for the FEM. For an explanation of FEM with a focus on acoustics, \cite{atalla2015} provides information and examples tailored to acoustics. \cite{moser2005} is a comprehensive standard reference for acoustics. 

Regarding GPs and the underlying probability theory, \cite{murphy2012} is a comprehensive and modern reference to get a good overview of the subject. It is an especially good choice to read for learning about probability theory and Bayesian Inference. The subject of kernels is described in much detail, also because it is an important building block in the big field of Machine Learning and not only GPs. For a thorough explanation of GPs themselves, \cite{rasmussen2006} is the best choice and the standard reference on the subject. statFEM was first introduced in \cite{girolami2021}. \cite{Duffin2020} modifies statFEM to work in a time-dependent context and provides a detailed mathematical derivation of the method. A first practical application is documented in \cite{Febrianto2021}: Sensor data from a railway bridge is fused with an FEM model of the bridge. Besides GPs, statFEM builds up upon the statistical generating model derived in \cite{kennedy2001}. statFEM focuses on finding an approximation of the model error, opposed to the well-established stochastic FEM \cite{Stefanou2009} which focuses on propagating uncertainty in the PDE parameters through a discretized model. A recently developed method to improve the capabilities of the FEM as a prior in Bayesian inversion, in the context of the stochastic FEM, is presented in \cite{Abdulle2021}. 
Another approach on approximating a posterior by providing a prior and conditioning it on measurements, hence very similar to statFEM, can be found in the area of model order reduction: Gappy POD, or Gappy proper orthogonal decomposition, first derived in \cite{Everson1995} and well explained in \cite{StevenL.Brunton2019}, is an adaptation of the well known Karhunen-Loeve transform \cite{Rao2001, Loeve1994}. 




\chapter{FEM Solution of the Helmholtz Equation}
This thesis is based on two pillars: Solving the governing equation for acoustics using the FEM and implementing a probabilistic approach using GPs and statFEM. For the first one, the equations for the Helmholtz equation representing wave propagation in an incompressible fluid and the FEM are derived in this chapter.

An acoustic field is represented in time domain by the wave equation. It returns the acoustical pressure at a point in space $x$ for a time variable $t$.
The Helmholtz equation describes the wave equation in frequency domain. It has a dependency on the frequency, so the goal is to solve the Helmholtz equation for a range of frequencies in order to obtain a frequency response plot which gives insight into the modal behavior of the system. Here, the system is, without loss of generality, solved for only one frequency.



\section{Wave Equation and Helmholtz Equation}
In a typical vibroacoustic problem, for example an interior structural acoustic coupling problem \cite{atalla2015}, there are usually two coupled domains: At first, there is some kind of solid medium which radiates sound. The movement of that solid can be described within the field of elastodynamics using for instance (a) Bernoulli or Timoshenko beam theory for 1D beam-like structures, (b) Kirchhoff or Reissner-Mindlin plate theory for 2D plate-like structures. The focus of this thesis is another: As soon as the sound is radiated from the solid, the wave equation is used to describe the propagation of sound waves through an acoustic medium such as air. 
The following closely follows \cite{larson2013} and \cite{moser2005}.
The behavior of a gas in a domain $\Omega$ can be described using the density $\rho$, the pressure $p$ and the velocity $v$. By changing the velocity or pressure locally by e.g. a speaker or a vibrating plate, this change is propagated through the medium. One can think of the gas as a system of infinitesimally small spring-mass systems. Deflecting one spring yields a deflection of the adjacent spring, or gas volume, but time-delayed due to the gas's inertia. This leads to a wave propagating through the medium. 
Newton's second law states that mass times acceleration equals force \cite{Gross2016}. If a gas volume is accelerated in one direction, a force on the neighboring particle is applied \cite{moser2005}. The equation reads
%
\begin{equation}
m \dot{v} = S  \left[  p(x) -p(x + \Delta x)  \right] \;,
\end{equation}
with $m$ the mass of the gas volume, $S$ the surface of the contact area of the two volumes  and $\Delta x$ the length of a gas volume in the direction of motion. Considering $m = \Delta x S \rho$ there holds
\begin{equation}
\rho \dot{\bm{v}} = - \frac{p(x) -p(x + \Delta x)}{\Delta x} \; .
\label{eqn:waveEqDeriv1}
\end{equation}
Assuming interaction between infinitessimaly small gas particles and therefore taking the limit of the difference quotient in (\ref{eqn:waveEqDeriv1}) yields the differential quotient and hence
\begin{equation}
\rho \dot{v} = - \nabla p \;.
\label{eqn:Tragheitsges}
\end{equation}
In the above equation, there holds for the nabla operator \begin{equation}
\nabla =\left( \pdv{}{x_1},\pdv{}{x_2},...,\pdv{}{x_n} \right)
\end{equation}
which consists of the partial derivatives in direction of the used coordinates. For a cartesian coordinate system, i.e. for $n=3$, applied as a product to a quantity, it describes the gradient of the quantity, i.e.
\begin{equation}
\nabla p = \mathrm{grad}p = \left( \pdv{p}{x},\pdv{p}{y},\pdv{p}{z} \right) \;.
\end{equation}



If a volume is expanded, the pressure of the contained gas drops and vice versa. Therefore a change of volume is proportional to a negative change of pressure. An expanding volume is also represented by the divergence of the velocity $\nabla \cdot \bm{v}$ \cite[p. 134 ff.]{larson2013}.  Imagine quickly pulling a piston out of an airtight cylinder: The air particles will move in the direction of the piston movement while the volume expands and the pressure drops. Hence, there holds
\begin{equation}
\dot{p} = -k \nabla \cdot \bm{v},
\label{eqn:Constit}
\end{equation}
with $k$ a proportionality constant dependent on the medium.
Differentiating (\ref{eqn:Constit}) with respect to $t$ yields 
\begin{equation}
\ddot{p} = -k \nabla \cdot \dot{\bm{v}} \;.
\end{equation}
Inserting \eqref{eqn:Tragheitsges} and considering $c^2 = k/\rho$, there holds for the acoustic wave equation
\begin{equation}
\ddot{p} = c^2 \nabla^2 p 
\label{eqn:WaveEq}
\end{equation}
which involves derivatives in both time and space. To obtain a general representation of the behaviour of a pressure field independent of time, the Helmholtz equation can be derived as follows.
Considering a separation ansatz \cite[p.659]{westermann2015} 
\begin{equation}
p(x,t) = X(x) \cdot T(t) \; ,
\end{equation}
with $X(x)$ and $T(t)$ each representing only location or time, respectively,
there holds for \eqref{eqn:WaveEq}

\begin{align}
X(x) \cdot \pdv[2]{T(t)}{t} &= c^2 \nabla^2 X(x) \cdot T(t)\;,\\
\frac{1}{T(t)} \pdv[2]{T(t)}{t} &= c^2 \frac{1}{X(x)} \nabla^2 X(x) = const. = - \omega^2 \; .
\end{align}
With the wave number $k = \omega / c$,
\begin{equation}
\nabla^2 X(x) + k^2 X(x) = 0
\label{eqn:HelmPre}
\end{equation}
is the Helmholtz equation which is a representation of the wave equation independent of time \cite[p. 1083 ff.]{arens2015}.
%
With $p [\SI{}{\pascal}]$ the pressure field and $k$ the wave number, \eqref{eqn:HelmPre} can be written as
\begin{equation}
\nabla^2 p + k^2 p = 0 \;.
\label{eqn:Helmholtz}
\end{equation}
For $\nabla^2$, which can also be expressed as the Laplace operator $\nabla^2 = \Delta$, there holds
\begin{equation}
\nabla^2 = \left( \pdv[2]{}{x},\pdv[2]{}{y},\pdv[2]{}{z} \right) \;.
\end{equation}
With $\omega$[\SI{}{\radian}] the angular frequency, $f$[\SI{}{\per\s}] the frequency and $c_0[\SI{}{\metre\per\s}]$ the speed of sound in the considered medium $k$ is defined as
\begin{equation}
k = \frac{\omega}{c_0} = \frac{2 \pi f}{c_0} \;.
\end{equation}
Equation (\ref{eqn:Helmholtz}) is solved as a boundary value problem which means that the behavior of the system has to be described beforehand for the boundaries of the domain. This is done using boundary conditions (BCs).



\paragraph{Boundary Conditions}
The following closely follows \cite[p. 10 ff.]{atalla2015}. The major types of BCs are Dirichlet, Neumann and Robin type BCs. Neumann BCs are also called natural BCs because these automatically arise in the equation while deriving the weak form of the problem. They are defined using the derivative on the boundary. Dirichlet BCs, on the other hand, don't arise automatically. They are called essential BCs and have to be applied by hand after deriving the weak form and building the linear system of equations. They are defined using the function values at the boundary. Robin BCs are also called mixed BCs. They contain both the function values and their derivative.
%\paragraph{Dirichlet}
The Dirichlet BC for the Helmholtz equation is simply a prescribed pressure $\bar{p}(x)$ at the $\Gamma_D$ part of the boundary,
\begin{equation}
p(x) = \bar{p}(x) \quad \forall x \in \Gamma_D \;.
\end{equation}



	
%\paragraph{Neumann}
The Neumann boundary condition over $\Gamma_N$ reads
\begin{equation}
\pdv{p(x)}{n} = \rho \omega^2 \bm{U}(x) \cdot \vec{n} \quad \forall x \in \Gamma_N \;,
\label{eqn:NeumannBCDef}
\end{equation}
with $\bm{U}$ the displacement and $\vec{n}$ the outer normal direction of the surface. The displacement can be thought of as a piston moving with circular frequency $\omega$ at the boundary which creates a velocity and pressure change. Choosing a homogeneous Neumann BC $\pdv{p(x)}{n} = 0$ resembles a reflecting boundary. The part of the boundary, where a homogeneous Neumann BC is applied, is going to be called $\Gamma_{N0}$.

%\paragraph{Robin}
The Robin or impedance boundary condition at $\Gamma_R$ reads
\begin{equation}
\pdv{p(x)}{n} + ik \beta p(x) = 0 \quad \forall x \in \Gamma_R \;,
\end{equation}
with $\beta$ the specific normalized acoustic admittance. The Robin boundary condition is also called impedance boundary condition because an acoustic impedance can be simulated with it which leads to, e.g., partially reflecting walls.

%Figure \ref{fig:BCEx} shows an example on how boundary conditions could be applied on a 2D domain. 

Figure \ref{fig:BCTh} shows the setting chosen in this thesis: All walls are reflecting and on the left side of the domain a Neumann boundary conditon is imposed which is used to model the previously mentioned sound source. 
$\Gamma_{N0}$ BCs are the mentioned natural boundary conditions which don't have to be applied explicitly. Still, a Neumann BC $\Gamma_{N}$ can be imposed onto the system which behaves differently. 

%\begin{figure}[!ht]
%begin{center}
%\includegraphics[width=0.4\textwidth]{pics/BCsExample}
%\caption{Two dimensional representation of various boundary conditions. In this example, the left side is a Neumann boundary, the upper and lower boundaries are totally reflecting and the right side consists of an impedance BC and a Dirichlet BC.}
%\label{fig:BCEx}
%\end{center}
%\end{figure}


\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{pics/BCsThesis1}
\caption[BCs for the 2D Helmholtz equation example]{BCs of the problem considered for this thesis. A Neumann boundary is applied on the side of the domain. The piston displacement $\bm{U}$, which is part of the definition of the BC, is going to be modeled as a GP. The rest of the domain's boundary is considered as reflecting, i.e. $\Gamma_N = 0$.}
\label{fig:BCTh}
\end{center}
\end{figure}


\paragraph{Poisson Equation}
Besides the Helmholtz equation, also the Poisson equation is going to be used in this thesis in order to compare the results of the implemented methods with other references such as \cite{girolami2021}. For the one dimensional Poisson equation on a domain $\Omega \subset \mathbb{R} $ there holds
\begin{equation}
-\nabla \cdot (\mu(x) \nabla p(x)) = f(x),
\label{eqn:PoissonEq}
\end{equation}
with $p(x)\in \mathbb{R}$ the pressure field, as in \eqref{eqn:Helmholtz}, $\mu(x) \in \mathbb{R}^+$ the diffusion coefficient and $f(x) \in \mathbb{R}$ the source term.
Equation \eqref{eqn:PoissonEq} is handled as a stochastic PDE, i.e., $\mu(x)$ and $f(x)$ are defined as random, here in form of GPs, and therefore also $p(x)$ for which the equation is going to be solved \cite[p. 4]{girolami2021}. 


Having the problem defined mathematically, the equations can now be prepared for being solved by FEM.




\section{The Classical Finite Element Method}


The FEM is a procedure which makes it simple to approximately solve Partial Differential Equations (PDEs) which would be very hard or even impossible to solve analytically. As the name suggests, the calculation domain is split into $n_e$ individual elements on which the PDE is approximated using so-called ansatz functions. The process of dividing the domain into $n_e$ elements is called discretization. The polynomial degree of the ansatz functions and the number of elements determine the accuracy of the approximation. In order to set up the FEM, the PDE needs to be in the so-called variational, or weak, formulation \cite{Lanczos1986}. 
After discretization of the domain and deriving the weak formulation, the PDE can be approximated on the individual elements. Assembling the element matrices yields a global system of equations which can, after boundary conditions are applied, be solved for the solution vector.


\subsection{Integral and Weak Formulation}
\label{sec:HelmholtzFormulations}
The Helmholtz equation is the so-called governing equation for the problem considered in this thesis. The governing equation, which is here to be solved for the sound pressure $p$, is the strong form and is usually of higher order. This property makes it hard to solve the equation, both analytically and numerically. For being able to apply the FEM, the order of the PDE has to be lowered. That is achieved by first bringing the equation into the weak formulation. 
One way to derive the weak formulation of the governing equation is the method of weighted residuals. 
\paragraph{The Weigthed Residuals Method}
This follows \cite[p.146 ff.]{langtangen2019}. The approximation error $e = p - p_e$ is the difference between the FEM approximation and exact solution $p_e$ and therefore a measure for the quality of the approximation. The exact solution is usually not known. Therefore, another method is used: The residual $R$ is formed by inserting the approximation of the solution into the PDE. The result is non-zero and a measure for the quality of the approximation, as well \cite{langtangen2019}. To increase the quality, the residual has to be minimized. 
An FEM approximation, see Section \ref{chap:ApproxFem} for details, uses FEM basis functions which are weighted with some factors $c_j$. The goal is finding the factors which minimize $R$. The seeked approximated solution function to the Helmholtz equation $p$ is part of a function space $V$, i.e., $p \in V$, which is spanned by the FEM basis functions $\varphi_i$. A scalar product of two functions on a domain $\Omega$ is defined in \cite[p.145]{langtangen2019} as 
\begin{equation}
(f,g) = \int_{\Omega}f(x) g(x) \, \mathrm{d}x \;.
\end{equation}
Enforcing the scalar product of the residual and a test function $v$ out of some other function space $W$ to be zero, expressed as
\begin{equation}
(R,v) = 0, \quad \forall v \in W \;,
\end{equation}
i.e., enforcing that they are orthogonal, can be used to minimize $R$. $W$ can be, approximately, spanned by the FEM basis functions as well. Nevertheless, it doesn't necessarily need to be defined in the same space as $p$. Choosing the same space would be called the Galerkin method, which is usually done. Applying the weighted residuals method to the Helmholtz equation \eqref{eqn:Helmholtz} is executed as follows.
At first, the strong formulation (PDE) is multiplied with the test function $v$ and integrated over the domain $\Omega$ to get the so-called integral formulation \cite{langtangen2019}, namely
\begin{equation}
\int_{\Omega} v \nabla^2 p \,\mathrm{d}\Omega + \int_{\Omega} k^2 pv \,\mathrm{d}\Omega = 0 \;.
\label{eqn:IntForm}
\end{equation}
%

The approximated function throughout all elements, here it is $p$, is called the trial function. It is part of a constrained space of trial functions or termed as the trial space. The test function $v$ is also part of a test space. Both trial and test spaces have to fulfill the Dirichlet boundary conditions imposed on the PDE.
The Helmholtz equation and therefore also the now formed integral formulation contain derivatives of second order. However, the FEM works by adjoining individual element functions to a global function. Therefore, the global solution is a piecewise polynomial \cite{langtangen2019} and causes problems when calculating higher-order derivatives because those become discontinuous. In consequence, integration by parts is performed using Green's first identity \cite[pp. 53,54]{atalla2015} on the higher-order terms to get lower order derivatives. It makes use of the divergence theorem, also called Gauss's theorem \cite[p. 1006]{arens2015}, for which there holds

\begin{equation}
\int_{\Omega}  \nabla \cdot \bm{F} \,\mathrm{d}\Omega = \oint_{\Gamma} \bm{F} \cdot \vec{n} \,\mathrm{d}\Gamma \; .
\label{eqn:DivTheo}
\end{equation}
It states that all of the sources and sinks inside a domain (which generate divergence) must be equal to the amount of flux over the boundary of that domain. 
By defining  $\bm{F} = v \bm{u}$ with $\bm{u} = \nabla p$ and making use of the product rule of vector calculus $\nabla \cdot (v \bm{u}) = v \nabla \cdot \bm{u} + \bm{u} \cdot \nabla v$ there holds for Green's first identity
\begin{equation}
\oint_{\Gamma} v \nabla p \cdot \vec{n} \,\mathrm{d}\Gamma = \int_{\Omega} \nabla p \cdot \nabla v \,\mathrm{d}\Omega + \int_{\Omega}  v\nabla^2 p \,\mathrm{d}\Omega \; .
\end{equation}
%
Applying it to (\ref{eqn:IntForm}) yields
\begin{equation}
-\int_{\Omega} \nabla p \cdot \nabla v \,\mathrm{d}\Omega + \oint_{\Gamma} v \nabla p \cdot \vec{n} \,\mathrm{d}\Gamma+ \int_{\Omega} k^2 pv \,\mathrm{d}\Omega = 0 \;,
\label{eqn:WeakHelmholtz}
\end{equation}
with the boundary of the domain $\Gamma$.
The resulting equation is now called the weak formulation as opposed to the strong formulation, since the trial function now only needs to be differentiable once.
The boundary term can be decomposed into three parts as
\begin{equation}
\oint_{\Gamma} v \nabla p \cdot \vec{n} \,\mathrm{d}\Gamma = \int_{\Gamma_D} v \nabla \bar{p}  \,\mathrm{d}\Gamma + \int_{\Gamma_N} v (\rho \omega^2 \bm{U} \cdot \vec{n})  \,\mathrm{d}\Gamma + \int_{\Gamma_R} v(-ik\beta p)  \,\mathrm{d}\Gamma \;,
\label{eqn:BCsHelmholtz}
\end{equation}
with $\int_{\Gamma_D} v \nabla \bar{p}  \,\mathrm{d}\Gamma = 0$ for the Dirichlet part because if the value at the boundary is known, then $\bar{f}$ is a scalar quantitity of which the derivative in outward normal direction of the boundary is zero.

\subsection{Discretization}
In this thesis, two different domains are used: In a simple example, a 1D domain is going to serve as an illustration of how GPs and statFEM work. For the actual vibroacoustics problem, a 2D domain will be examined. In this chapter, the FEM discretization for both domains is discussed.
%
Discretization means that the domain $\Omega$ is split into $n_e$ individual elements $\Omega^{(e)}$ as
\begin{equation}
\Omega = \bigcup_{e=1}^{n_e} \Omega^{(e)} \;.
\end{equation}
This holds not only for 1D meshes but for 2D and 3D meshes as well.
For 1D elements with linear ansatz functions, such as in Figure \ref{fig:1DDom}, each element has two so-called nodes. This is because, between the element boundaries, a linear function is spanned which needs two support points to be defined.  In a 2D domain, in case of a triangular element with linear shape functions, an element would have 3 nodes, see Figure \ref{fig:2DDom}. Also different shapes of elements, e.g. quadratic elements with 4 nodes, are possible. The collection of nodes and elements is called a mesh. The total number of nodes in a 1D mesh with linear ansatz functions is $n_n = n_e + 1$. In a 2D structured mesh, as it is used here (see Figure \ref{fig:meshFEM}), it is $n_n = n_x \times n_y$ with $n_x$ and $n_y$ the number of nodes in each coordinate. The minimum degree of the used function is determined by the weak form: It always needs to be differentiable, therefore a constant approximation function would not work \cite{atalla2015}. 

Figures \ref{fig:1DDom} and \ref{fig:2DDom} illustrate the discretization of a 1D and 2D domain, respectively.  For the 1D domain, the first ansatz functions are sketched: It is visible that for each node there is an ansatz function that yields the value $1$ only at that node and $0$ at all other nodes in the domain.
%
\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/1dDomainNew}
\caption[FEM mesh of the 1D example]{A discretized 1D domain with nodes in green and linear ansatz functions shown for the first four nodes. The ansatz functions are $1$ on their respective nodes and are $0$ on all other nodes.}
\label{fig:1DDom}

\end{center}
\end{figure}
%
\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.4\textwidth]{pics/2dDomain}
\caption[FEM mesh for a 2D example]{A discretized 2D domain using triangular elements with linear ansatz functions. The nodes can be seen in green. Also here, the ansatz functions are defined so that they are 1 at their respective nodes and zero at all other nodes. This mesh is structured: The nodes follow the square geometry of the domain, like a grid. Unstructed would mean that the distance between neighbouring nodes isn't fixed but can vary in defined bounds.}
\label{fig:2DDom}

\end{center}
\end{figure}
%
The finer the mesh, i.e. the more elements are used in a domain, the closer the FEM approximation comes to the exact solution. Since computing very fine meshes is computationally expensive, a convergence study is conducted in order to find the element size from which on the accuracy is sufficient. Also, the higher the chosen frequency in \eqref{eqn:Helmholtz}, i.e., the shorter the wavelength, the finer the FEM mesh needs to be discretized in order to capture the wave characteristics. As a rule of thumb, $6$ nodes per wavelength are recommended \cite{Marburga}.
\FloatBarrier
FEniCS \cite{langtangen2016} is a Python/C++ library for solving PDEs with the FEM. It makes it easy to go directly from the variational formulation of a problem to solving it. In this thesis some code samples will be shown. They are all very basic and stem from the FEniCS book \cite{langtangen2016}.
Using FEniCS, a 1D mesh can be created by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
dom_a = 0.0 #domain boundaries
dom_b = 1.0
ne = 60 #number of elements
mesh = IntervalMesh(self.ne,self.dom_a,self.dom_b) #define 1D mesh
\end{minted}
The interval of the mesh and the desired number of elements have to be provided.
%
A 2D mesh can, for example, be obtained by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
a,b = 2,2
mesh = UnitSquareMesh(self.a,self.b) #define square 2D mesh
\end{minted}
In this case a unit square is chosen, which means that the domain size will be quadratic and of length $1.0$ in every direction. Two elements were chosen for every dimension which leads to a mesh as depicted in Figure \ref{fig:2DDom}. Of course every desired 2D shape can be used as a mesh, it doesn't have to be rectangular: For example unstructured meshes \cite[p. 119 ff.]{langtangen2019} make it possible to mesh any desired geometrical shape.

\subsection{Approximation and Building Element Matrices}
\label{chap:ApproxFem}
In FEM, the values between nodes within an element are interpolated with basis functions $\varphi_i$. 
For each degree of freedom in the domain, an FEM basis function is defined. By weighting those basis functions, the solution can be approximated with 
\begin{equation}
p(x) \approx \sum_{j = 1}^n c_j  \varphi_j(x) \; .
\label{eqn:WeighingFunction}
\end{equation}
(\ref{eqn:WeighingFunction}) means that the value of one particular point $x_i$ is the sum of all products of all weight coefficients $c_j$ and test functions $\varphi_j$ evaluated at that point. One popular choice of FEM basis functions is the Lagrange family of polynomials.  Instead for $p(x)$, the equation can also be expressed for the test function $v(x)$ which is later going to be done.

\paragraph{Lagrange Polynomials}
Following \cite{langtangen2019}, Lagrange polynomials are one possible choice for FEM basis functions and are used in this thesis. They can be calculated for any desired degree $N$ and their main property is that they are $1$ at only a single node on the domain, $0$ at all other nodes and that they are nonzero on only a very limited fraction of the domain, as opposed to e.g. basis functions constructed from $\sin$ and $\cos$ terms. The Lagrange polynomials are defined by
\begin{equation}
\varphi_i(x) = \prod_{j=0,j\neq i}^N \frac{x-x_j}{x_i-x_j} \;.
\end{equation}
They are frequently used in FEM because there holds, if at $x_s$ there is a node of the mesh,
\begin{equation}
\varphi_i(x_s) = \delta_{is}, \quad \delta_{is}  =     \left\{ \begin{array}{rcl} 1 & \mbox{for}& i = s  \\ 0 & \mbox{for} & i \neq s\end{array}\right .
\end{equation}
which means that the polynomial yields the value $1$ only at one node in the domain. At all other nodes it is $0$. Between nodes, the Lagrange polynomial is spanned. This behavior simplifies (\ref{eqn:WeighingFunction}), if $x_i$ is the location of a node of the mesh:
\begin{equation}
p(x_i) = \sum_{j = 1}^n c_j  \varphi_j(x_i) = c_i \varphi_i(x_i) = c_i \;.
\end{equation}
The weight factor $c_i$ therefore gives an approximation of the true value at the node with number $i$ \cite[pp. 78, 79]{langtangen2019}. 
Another asset of using polynomials with $\varphi_i(x) = 0$ in most parts of the domain is that they lead to sparse matrices which can be solved much quicker than densely packed ones.
%
Now, to apply it to the weak form of the Helmholtz equation (\ref{eqn:WeakHelmholtz}), an approximation ansatz has to be made for both the trial and the test function. Equation \eqref{eqn:WeighingFunction} can be written in matrix form and there holds for each element, if the same basis functions are used for both trial and test spaces,
\begin{align}
\begin{split}
p^{(e)}(x) &= \bm{\varphi}^T \hat{\bm{p}} \;, \\
v^{(e)}(x) &= \bm{\varphi}^T \delta\hat{\bm{p}}\;,
\end{split}
\label{eqn:FEMAnsatz}
\end{align}
with $\delta\hat{\bm{p}}$ the vector of arbitrary variations \cite{Langer} of the pressure, which gets cancelled out later on because the test function is present in every term. $\hat{\bm{p}}$ is the vector of weight factors for every node. $\bm{\varphi}$ is the vector of the shape functions corresponding to the regarded element.
Inserting (\ref{eqn:FEMAnsatz}) in the weak form, considering no Robin boundary conditions and as the domain only a single element $\Omega_e$, results in
\begin{equation}
\sum_e \int_{\Omega_e} (\nabla \bm{\varphi} \nabla \bm{\varphi}^T)\,\mathrm{d}\Omega_e \hat{\bm{p}} -k^2 \sum_e \int_{\Omega_e} (\bm{\varphi} \bm{\varphi}^T)\,\mathrm{d}\Omega_e \hat{\bm{p}} = \rho \omega^2 \sum_e \int_{\Gamma_{e,N}} \bm{\varphi} U_n \,\mathrm{d}\Gamma_{e,N} \;.
\label{eqn:DiscretizedFEM}
\end{equation}
The sum $\sum_e$ means assembling the global system of matrices out of individual element matrices. In short, (\ref{eqn:DiscretizedFEM}) can be written as
\begin{equation}
\left( \sum_e \bm{K}_1^e - k^2 \sum_e \bm{K}_2^e \right)\hat{\bm{p}} = \rho \omega^2 \sum_e \bm{f}^e 
\end{equation}
with $\bm{K}_1^e$ and $\bm{K}_2^e$ the elemental system matrices and $\bm{f}^e$ the elemental source vector, which contains the Neumann BC. After assembly there holds for the global linear system
\begin{equation}
(\bm{K}_1 - k^2 \bm{K}_2) \hat{\bm{p}} = \rho \omega^2 \bm{f}
\label{eqn:LinSys}
\end{equation}
which is now to be solved for $\hat{\bm{p}}$.
To illustrate the assembly process, the so-called dynamic stiffness matrix $\bm{A}$ is introduced as
\begin{equation}
\bm{A} = (\bm{K}_1 - k^2 \bm{K}_2) \;.
\end{equation}
It is assembled using the individual matrices $\bm{A}^e$ for each element as visible in (\ref{eqn:Assembly}).
The goal is to solve the linear system of equations for the weight factors of the basis functions $\hat{\bm{p}}$, also called the degrees of freedom.
% It can be notated as
%\begin{equation}
%\sum_j A_{i,j} c_j = b_i
%\end{equation}
%with
%\begin{align}
%A_{i,j} &= \varphi_j(x_i), \\
%b_i &= f(x_i) \;.
%\end{align}
The calculation is usually done for one element at a time. Thereby arises an element matrix $\bm{A}^{(e)}$ whose individual entries can be computed as
\begin{equation}
A_{r,s}^{(e)} = \int_{\Omega^{(e)}} \nabla \varphi_r \cdot \nabla \varphi_s \, \mathrm{d}\Omega^{(e)} - k^2 \int_{\Omega^{(e)}} \varphi_r \varphi_s \, \mathrm{d}\Omega^{(e)} \;,
\end{equation}
with $r,s$ the indices of the individual nodes in an element. For 1D linear Lagrange elements, i.e. elements with two nodes each having one degree of freedom per node, a $2\times 2$ matrix is obtained and for 1D quadratic Lagrange elements with three nodes each, a $3\times3$ matrix is obtained.
%
Also the right hand side vector, now called $\bm{b}$, is computed element-wise using
\begin{equation}
b_r^{(e)} = \rho \omega^2  \int_{\Gamma_{e,N}} \varphi_r U_n \,\mathrm{d}\Gamma_{e,N} \;.
\end{equation}


The element matrices are then assembled to form the global system matrix. Within an element, the coordinates of the nodes are $[0,...,d]$ with $d$ the dimension of the element. For linear elements, the coordinates are $[0,1]$. For assembly, they need to be mapped to the global coordinate system. This is accomplished by using a mapping function, noted as $q$, which maps $[r,s]$ of the elements to $[i,j]$ in the global system: $i=q(e,r)$ and $j = q(e,s)$ with $e$ the number of the element \cite{langtangen2019}. For the global system matrix there now holds
\begin{equation}
A_{q(e,r), q(e,s)} =  \sum_e A_{r,s}^{(e)}
\label{eqn:SysMatAssem}
\end{equation}
for adding the entries of an element matrix to the global matrix.
The resulting matrix after assembly is sparse, here for three elements with linear ansatz functions:
\begin{equation}
\bm{A}  =
\begin{bmatrix}
\textcolor[rgb]{0.49,0.83,0.13}{A_{0,0}^{( 0)}} & \textcolor[rgb]{0.49,0.83,0.13}{A}\textcolor[rgb]{0.49,0.83,0.13}{_{0,1}^{( 0)}} & 0 & 0\\
\textcolor[rgb]{0.49,0.83,0.13}{A}\textcolor[rgb]{0.49,0.83,0.13}{_{1,0}^{( 0)}} & \textcolor[rgb]{0.49,0.83,0.13}{A}\textcolor[rgb]{0.49,0.83,0.13}{_{1,1}^{( 0)}}\textcolor[rgb]{0,0,0}{+}\textcolor[rgb]{0.49,0.83,0.13}{\ }\textcolor[rgb]{0.82,0.01,0.11}{A_{0,0}^{( 1)}} & \textcolor[rgb]{0.82,0.01,0.11}{A}\textcolor[rgb]{0.82,0.01,0.11}{_{0,1}^{( 1)}} & 0\\
0 & \textcolor[rgb]{0.82,0.01,0.11}{A}\textcolor[rgb]{0.82,0.01,0.11}{_{1,0}^{( 1)}} & \textcolor[rgb]{0.82,0.01,0.11}{A}\textcolor[rgb]{0.82,0.01,0.11}{_{1,1}^{( 1)} \ }\textcolor[rgb]{0,0,0}{+}\textcolor[rgb]{0.29,0.56,0.89}{A_{0,0}^{( 2)}} & \textcolor[rgb]{0.29,0.56,0.89}{A}\textcolor[rgb]{0.29,0.56,0.89}{_{0,1}^{( 2)}}\\
0 & 0 & \textcolor[rgb]{0.29,0.56,0.89}{A}\textcolor[rgb]{0.29,0.56,0.89}{_{1,0}^{( 2)}} & \textcolor[rgb]{0.29,0.56,0.89}{A}\textcolor[rgb]{0.29,0.56,0.89}{_{1,1}^{( 2)}}
\end{bmatrix} \ \ 
\label{eqn:Assembly}
\end{equation}
%
For the global right-hand-side vector there holds
\begin{equation}
b_{q(e,r)} = b_r^{(e)} \; 
\end{equation}
which is, again, done for each elemental source vector.



Using FEniCS, the Neumann boundary conditions can be applied to the system of equations by 
%
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
#find the correct nodes on the boundary:
class BoundaryX_L(SubDomain):
	self.tol = 1E-14
	def inside(self, x, on_boundary):
		return on_boundary and near(x[0], 0, tol)# and (x[1] < 0.3)
bxL = BoundaryX_L()
bxL.mark(boundary_markers,0) # left side is marked as "0"
#define a new operator for the boundary
ds = Measure('ds', domain=self.mesh, subdomain_data=boundary_markers) 
\end{minted}
%
%
This construction can then be used when defining the variational problem:
%
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
U = 0.0001 #Piston displacement
g = rho * omega**2 * U
u = TrialFunction(V) # test and trial function in the same function space
v = TestFunction(V)
#variational Problem:	
a = inner(nabla_grad(u), nabla_grad(v))*dx - \
 k**2 * inner(u,v)*dx 
L = (v*g)*ds(0) # 0 is the chosen boundary marker
\end{minted}

With FEniCS, the system is assembled using 
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
A = assemble(a)
b = assemble(L) .
\end{minted}
The left-hand side matrix is called the bilinear form since for both trial and test functions the basis functions are used. The right-hand side vector is called the linear form because only basis functions for the test functions are included.
Dirichlet boundary conditions can be applied after assembling the system by 
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def boundaryDiri(x):
	return x[0] > (1.0 - DOLFIN_EPS) and x[1] < 0.0 + DOLFIN_EPS
bcDir = DirichletBC(self.V, Constant(0.0), boundaryDiri) #define the bc on the vector space
bcDir.apply(A, b) .
\end{minted}

\FloatBarrier


\subsection{Solving and Convergence}
To obtain the solution vector $\hat{\bm{p}}$, the linear system \eqref{eqn:LinSys} has to be solved. This could be done by inverting the dynamic stiffness matrix $\bm{A}$ and multiplying it from the left with both sides of the equation. However, for numerical reasons, directly inverting a matrix is almost never done. \cite{Druinsky2012} and \cite{Higham2002} thoroughly discuss that issue: Inverting a matrix is slower and less accurate, especially for rather ill-conditioned matrices, than directly solving the linear system. Solving the system can for example be done using the well-known Gaussian elimination \cite[p.509]{arens2015}, the Cholesky decomposition, which will be explained in Section \ref{sec:Cholesky}, or the LU decompositon. Reference \cite{Davis2006} explains the theory behind direct solvers in detail.

The FEM linear system of equations involves sparse matrices, i.e. matrices which are zero in most of their entries. This characteristic makes it possible to use solvers designed for matrices of this kind which are significantly faster than others. Reference \cite{Elman2014} gives insight on solvers for sparse systems and iterative solvers. Iterative solvers use a so-called preconditioner, which decreases the condition number of a matrix. The lower the condition number, the more stable the system. The condition number of a matrix is defined by the ratio of the largest entry to the smallest entry, according to \cite{MathWorks} and \cite{Belsley1980}.

The system of equations can be solved for the vector of unknowns by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
u = Function(self.V) 
U = u.vector()
solve(A, U, b)
\end{minted}

using FEniCS. It automatically chooses a suitable solver for the given problem. Per default, i.e., for rather small meshes, a sparse LU decomposition, also called Gaussian elimination, is used \cite{langtangen2016}.
\FloatBarrier
Solving the system for a range of frequencies and averaging the absolute pressure over the domain leads to the frequency response plot, see Figure \ref{fig:frf}.
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.9\textwidth]{pics/frf}
\caption[Frequency Response Function for the 2D Helmholtz equation example]{The frequency response plot shows the eigenfrequencies of the fluid in the domain. Exciting the fluid with an eigenfrequency leads to a comparatively strong response.}
\label{fig:frf}
\end{center}
\end{figure}
%
Figure \ref{fig:Eigen243} shows the mean and variance for the second eigenfrequency at $243\;,\mathrm{Hz}$. A symmetric pattern is clearly visible in both mean and variance. The higher the absolute pressure in the mean, the higher the variance. Hence, both mean and variance share the locations of zero mean, respectively variance. Plots for the first and third eigenfrequencies can be found in Appendix A. An animation with a frequency sweep, therefore also showing the behavior of the prior in non-eigenfrequency regions, can be found \href{https://github.com/herluc/Masterarbeit/blob/master/MA_LucasHermann/Python/Results/2D/FreqVar.gif}{online}(https://bit.ly/3AUuYjt).
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.9\textwidth]{pics/Freq243}
\caption[The mode shape of the second eigenfrequency of the 2D Helmholtz equation example]{The mean and variance for the second eigenfrequency of the 2D field.}
\label{fig:Eigen243}
\end{center}
\end{figure}

\FloatBarrier
Having explained the numerical setting, GPs and also statFEM can now be discussed.








\chapter{Gaussian Processes and the Statistical Finite Element Method}
In this chapter, an introduction to GPs and, building on it, statFEM is given. GP regression is an established method in Machine Learning and statFEM makes use of it to fuse FEM with data. At first, some important basic concepts of probability theory are revised.



\section{Basic Probability Theory}
statFEM is based on GPs and Bayesian inference. To be able to work with both, some basic understanding of probability theory is necessary.


\subsection{Multivariate Gaussian Distribution}
\label{sec:MultiGauss}

A random variable $X$ is considered normally, or Gaussian, distributed if $X \sim \mathcal{N}(\mu, \sigma)$ with $\mu$ the mean and $\sigma$ the standard deviation. The shape of the probability density function (PDF) is a Bell curve, see \eqref{eqn:GaussianPDF}.

Following \cite[pp. 33,34]{murphy2012}, for a normally distributed random variable, the moments can be deduced . Using the PDF there holds for the mean
\begin{equation}
\mu = \mathbb{E}[X] = \int_{\mathbb{R}} x f_X(x) \, \mathrm{d}x
\end{equation}
%
and for the variance
\begin{equation}
\mathbb{V}[X] = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2] - \mathbb{E}[X]^2 \;.
\end{equation}
%
For the covariance between two random variables $X$ and $Y$ there holds (see \cite[p. 1434]{arens2015})
\begin{equation}
\mathrm{cov}[X,Y] = \mathbb{E}[(X-\mathbb{E}[X])(Y-\mathbb{E}[Y])] = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y]\;.
\label{eqn:Covariance}
\end{equation}
It is clearly visible that $\mathrm{cov}[X,X] = \mathbb{V}[X]$.
%
For the PDF of the Gaussian distribution there holds \cite[p. 38]{murphy2012}
\begin{equation}
f_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp(-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2) \;.
\label{eqn:GaussianPDF}
\end{equation}
%
If multiple random variables are considered, which are all normally distributed, they can be aggregated in a so-called multivariate Gaussian disribution \cite[p. 46]{murphy2012}. Instead of a random variable, the mutlivariate Gaussian is a random vector which is defined by a mean vector and a covariance matrix. The matrix is named covariance and not variance matrix because not only the variance of the individual random variables but also the covariance among the different variables makes up entries of it. The covariance is related to auto- and cross correlation, both described in a single matrix.
For the PDF of a multivariate Gaussian distribution there holds \cite[p. 46]{murphy2012}
\begin{equation}
f_X(\bm{x}) = \frac{\exp(-\frac{1}{2}(\bm{x}-\bm{\mu})^T \Sigma^{-1}(\bm{x}-\bm{\mu}))}{\sqrt{(2\pi)^k \lvert \Sigma \rvert}}
\label{eqn:MultivariateGaussian}
\end{equation}
with $\Sigma$ the covariance matrix, $\bm{\mu}$ the mean vector and $k$ the dimension, i.e. the number of random variables, of the distribution.
An example of a multivariate Gaussian is visible in Figure \ref{fig:MultiGauss}. The underlying mean vector and covariance matrix read
\begin{equation}
\bm{\mu} = \begin{bmatrix}
           0 \\
           1
         \end{bmatrix}
, \quad     
\Sigma = \begin{bmatrix}
1 & -0.7 \\
-0.7 & 1.5 
\end{bmatrix} \;.
\end{equation}
The smaller the absolute value of the non-diagonal entries of the covariance matrix is, the less correlated the individual variables are.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.6\textwidth]{pics/Gaussians}
\caption[A bivariate Gaussian distribution]{A multivariate Gaussian distribution with two dimensions. A strong correlation between the two random variables is visible: No correlation would mean a perfect circle in the lower 2D plot. The more correlated the two dimensions are, the more elliptical the plotted PDF becomes \cite{Virtanen2020}.  }
\label{fig:MultiGauss}
\end{center}
\end{figure}

\subsection{Bayesian Inference}
\label{sec:BayesInf}
This section closely follows \cite{murphy2012, rasmussen2006}.
The first step towards Bayesian inference is to assume that all parameter involved in a model and therefore also the solution of the model are random variables. Hence, there is a probability density defined for each parameter and also the model outcome.
Bayesian Inference uses Bayes' law, i.e., $\text{posterior} = \frac{\text{likelihood} \times \text{prior}}{\text{marginal likelihood}}$, to infer a posterior density from a prior distribution and data. The data "updates" the prior. For distributions, the law states that
\begin{equation}
f(\bm{u}|\bm{y}) = \frac{f(\bm{y}|\bm{u})f(\bm{u})}{f(\bm{y})}
\label{eqn:BayesRule}
\end{equation}
where $f(\bm{u}|\bm{y})$ is the posterior, $f(\bm{y}|\bm{u})$ is the likelihood, $f(\bm{u})$ is the prior and $f(\bm{y})$ is the marginal likelihood, also called evidence. The posterior to be inferred is the new probability density describing the sought quantity after observing data $\bm{y}$. The data, i.e. measurements, aid understanding the problem which leads to a smaller variance in the posterior as comapared to the prior. The prior density describes the prior belief of the underlying process. It can be deduced from expert knowledge or from previous measurements. 
The likelihood describes the probability of the data to be measuered, at all. Given that, in the case of this thesis, the FEM solution holds the likelihood can be formulated as a question: "How likely is the measured data to be true, according to the model?".
The marginal likelihood (see Section \ref{sec:Marg} for more details) serves as a normalization constant and describes how likely it is to measure the observed data.

\paragraph{Occam's Razor}

One very useful aspect of Bayesian inference is that the so-called Occam's razor is inherently part of it. Occam's razor is a principle which states that the inference neither favors a too simple or too complex model as a posterior. This means that if the probability for the given data to explain a certain event is equal for a model with many parameters and a model with few parameters, the simpler model is "chosen" by Bayes' rule to be the best model to explain the data. This happens automatically and no effort has to be put in the inference to make Occam's razor happen \cite{Tipping2004, MacKay1991}. This is true even for uninformative priors, i.e. priors which are always $f(\bm{u})=1$.

\section{Gaussian Process Regression}
statFEM is based on GPs, a method known from Machine Learning \cite{murphy2012}. GPs can be used for regression, i.e. to find a suitable function which describes a given set of points as good as possible, but also for classification and clustering \cite{gortler2019} . 
GPs are a class of Bayesian non-parametric models. Non-parametric doesn't mean that there are no parameters involved but rather that there is an infinite number of them \cite{gortler2019}. Each realization of a GP doesn't yield a scalar or vector but a function. One can think of a GP as a collection of infinitely many normally distributed random variables, i.e. a generalization of a Gaussian distribution: A vector with infinitely many entries is basically a function. By picking out a finite set of those random variables when discretizing e.g. on an FE mesh, one obtains a multivariate distribution which is determined by a mean and a covariance matrix \cite[p. 2]{rasmussen2006}. Hence, GPs are a method to obtain a distribution over functions: Around a mean function a variance is defined. Possible samples from the GP are functions within these confidence bands. In statFEM, the FE prior and also the solution are modeled as GPs. This provides an elegant way to incorporate uncertain data in an FE solution to eventually improve it. To understand how this works, the general idea of regression and the mathematical basis of GPs have to be explained.
\subsection{Linear Regression}
Following \cite{murphy2012}, regression, in most cases linear regression, is a parametric model. This means that there are certain parameters involved in a model which can be changed to match given data as closely as possible regarding some measure of accuracy. Following \cite[p. 19]{murphy2012}, a regression is linear if the output of a function depends linearly on its inputs, as 
%
\begin{equation}
y(\bm{x}) = \bm{w}^T \bm{x} + \epsilon = \sum_{j = 1}^D w_jx_j + \epsilon \;,
\label{eqn:linReg}
\end{equation}
%
where $\bm{x}$ is the input vector, $\bm{w}$ the corresponding vector of input weights and $\epsilon$ is the error between the output of the regression and the true or measured, values. The error term is usually modeled as a Gaussian $\epsilon \sim \mathcal{N}(\mu,\sigma^2)$ with zero mean $\mu = 0$ and some variance. Because of the added Gaussian, every output of $y(\bm{x})$ is now not only a scalar but a normally distributed random variable. Therefore, the whole function can be expressed as a probability density:
%
\begin{equation}
f(y|\bm{x}) = \mathcal{N}(\bm{w}^T\bm{x},\sigma^2)
\end{equation}
%
Notice that the mean $\mu(\bm{x})$ now depends on the input vector: For a linear approximation there holds
\begin{equation}
\mu (\bm{x}) = w_0 + w_1x= \bm{w}^T\Phi(\bm{x}) 
\end{equation}
with $\Phi(\bm{x}) = [1,x]$. 
Unintuitively, using a basis function expansion, linear regression can be used to model a non-linear behaviour \cite[p. 20]{murphy2012}. If polynomials are used as basis functions, one speaks of polynomial regression. The probability density would read
\begin{equation}
f(y|\bm{x}) = \mathcal{N}(\bm{w}^T\Phi(\bm{x}),\sigma^2)
\end{equation}
with 
\begin{equation}
\Phi(\bm{x}) = [1,x,x^2,x^3,...,x^d]
\end{equation}
where the complexity of the approximation rises with $d$ \cite[p. 20]{murphy2012}. 
The model now has to be fit to given data, i.e. the input weight vector has to be estimated. This works, for example, by using maximum likelihood estimation (MLE), which ultimately results in a least squares approach.




%"Regression is used to find a function that rerpesents a set of data points as closely as possible" \cite{gortler2019}
%\cite{gortler2019} "A Gaussian process is a probabilistic method that gives confidence for the predicted function"

%"GPs allow us to make predictions about our data incorporating prior knowledge"



%he following closely follows \cite{rasmussen2006}.




%A GP is described by its mean function and the covariance function, also called kernel.






\subsection{Gaussian Processes}
\label{sec:GPIntro}
This section closely follows \cite{rasmussen2006, gortler2019}.
GP regression is another type of regression but ultimately has the same goal as, for instance, a polynomial regression. The difference is that a GP isn't constrained to a parametric model, i.e. it is more flexible. Instead of using a polynomial or some other equation as a basis function, GPs utilize a kernel function which is able to output an infinite amount of different functions. An explanation of kernels is given in Section \ref{sec:Kernels}.
A GP is more flexible than a standard linear or polynomial regression: A GP regressor finds a suitable function out of infinitely many possible functions. For a linear regression, the function type and therefore basis functions need to be known beforehand in order to achieve accurate results. For a problem for which the function shape cannot be determined beforehand, a general GP can in many cases yield better results than a linear regression with an inadequately chosen basis function. An example is shown in Figure \ref{fig:GPvsReg}. For both plots the same polynomial regressor and GP were used.
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]{pics/GPvsReg}
\caption[GP regression versus Linear regression]{On the left: Regression for a 2nd degree polynomial. Both linear regression and GP approximate it fine. On the right: The same GP as on the left is able to model the ground truth accurately, the polynomial regression does not. The GP behaves more flexible in this case.}
\label{fig:GPvsReg}
\end{center}
\end{figure}
GPs are non-parametric: This means that, as opposed to standard linear regression where parameters are optimised, a GP does not have a certain number of parameters. Rather, the number of parameters depends on the number of test and training points and could in theory be infinitely large \cite{murphy2012} . 
GPs are constructed using multivariate Gaussians which were explained in Section \ref{sec:MultiGauss}. Also, as mentioned before, they represent a distribution over functions: The term "process" refers to the generalization from a Gaussian random variable to a random function \cite[p.13]{rasmussen2006}. The relationship between multivariate Gaussians and functions is explained here following \cite{damianou2021}.

The simplest discrete GP is a bivariate Gaussian. For each of the two random variables, i.e. dimensions, a mean and a variance are defined. Also, the covariance between both random variables is given. Now, samples can be drawn from each random variable. As an example see Figure \ref{fig:bivarGP}. In the lower half of the plot, the samples are visible in the already described 2D plane. The upper half shows the individual samples compared to each other. Drawing samples from a bivariate Gaussian leads to different results depending on the correlation between the two random variables. The more correlated they are, the smaller the difference between the sampled values in each dimension.
%
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]{pics/bivarGP}
\caption[Visual explanation of the intuition behind GPs 1]{A bivariate Gaussian can be interpreted as a GP. The more correlated the random variables are, the closer together the sampled values are. The plots on the top show the samples drawn from the well-known 2D PDF in another way: Sampling from a multivariate Gaussian yields a vector of correlated samples which can be interpreted as a subset of an infinitely dimensional vector, i.e. a function.}
\label{fig:bivarGP}
\end{center}
\end{figure}
%
Of course, this also holds true for multivariate Gaussians with even more random variables, as visible in Figure \ref{fig:multiGP}.
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=1\textwidth]{pics/multivarGP}
\caption[Visual explanation of the intuition behind GPs 2]{The more random variables are used to construct the multivariate Gaussian, the more the result looks like a smooth function. Since each variable is a random variable, there is also a variance available for every dimension, visible in grey.}
\label{fig:multiGP}
\end{center}
\end{figure}
%
In theory, a GP can consist of infinitely many correlated random variables. Drawing samples from a GP can be imagined as drawing correlated samples from a large set of random variables. However, a GP is, at first, defined continuously: To construct a GP, a mean function and a covariance function (also called kernel function) have to be given. The mean function can be any function, but the covariance function has to obey certain rules. Working with continuous GPs \cite{Saerkkae2013} can be numerically hard, but there are approximation methods available, see e.g. \cite{Durbin1985}. This thesis focuses on discrete GPs. To use Bayesian inference with a discrete GP, the continuous GP has to be represented as a multivariate Gaussian, i.e., discretely, again. To understand how and why this is done, the concepts of marginalization and conditioning have to be explained at first.

\paragraph{Marginalization and Conditioning} 
\label{sec:Marg}
This section follows \cite{gortler2019}.
Marginalization means that out of a multivariate Gaussian the single variables can directly be taken out of the mean vector and the covariance matrix. E.g. for a point on the axis described by the GP, see e.g. Figure \ref{fig:multiGP}, a single random variable can be picked out. A univariate Gaussian is created by simply using the value of the mean vector at that point and the $i,i \mathrm{th}$ index of the covariance matrix. This means that the underlying distributions for single variables do not change if a GP is used to describe an arbitrary set of variables \cite[p. 13]{rasmussen2006}, \cite{gortler2019}.
From a multivariate, in this case bivariate, Gaussian expressed as
\begin{equation}
\begin{bmatrix}
           X \\
           Y
         \end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}
           \mu_X \\
           \mu_Y
         \end{bmatrix}, \begin{bmatrix}
\Sigma_{XX} & \Sigma_{XY} \\
\Sigma_{YX} & \Sigma_{YY} 
\end{bmatrix}  \right)
\end{equation}
one single random variable, say $X$, can be marginalised out by simply only looking at the entries corresponding to $X$:
\begin{equation}
X \sim \mathcal{N}(\mu_X, \Sigma_{XX}) \;.
\end{equation}
If the covariance matrix and mean vector are not known, a random variable can be marginalised out of an arbitrary distribution by
\begin{equation}
f_X(x) = \int_y f_{X,Y}(x,y)\,\mathrm{d}y = \int_y f_{X|Y}(x|y)p_Y(y)\,\mathrm{d}y
\label{eqn:margi}
\end{equation}
which intuitively means that summing the probability for $X$ at every possible $Y$ gives the total probability for $X$ independent of $Y$. 

(\ref{eqn:margi}) already uses the principle of conditioning. The condition operator $|$ yields the probability of one variable under the condition that another event is true. As an illustration in words one could ask: "What is the probability of event $X$ to happen under the condition that the event $Y=y$ happens simultaneously?".
For the multivariate Gaussian there holds for the conditioning
\begin{equation}
X|Y \sim \mathcal{N}(\mu_X +\Sigma_{XY} \Sigma_{YY}^{-1}(Y-\mu_Y), \Sigma_{XX} - \Sigma_{XY}\Sigma_{YY}^{-1}\Sigma_{YX} ) \;.
\label{eqn:condMat}
\end{equation}
%
Conditioning yields a so-called modified Gaussian distribution which can be imagined as a "cut" through a multivariate Gaussian $f(X,Y)$ at a certain point $y$. A new Gaussian with the dimension reduced by $1$ arises as illustrated in Figure \ref{fig:GaussCut3d}.

%
%
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.6\textwidth]{pics/Gaussians3dCut}
\caption[Conditioning of a Gaussian]{Conditioning a bivariate Gaussian leads to a new Gaussian with only one dimension. This example shows the result for conditioning a bivariate Gaussian at the dashed line. The new Gaussian has a new mean and variance.}
\label{fig:GaussCut3d}
\end{center}
\end{figure}



\subsection{Kernel Functions}
\label{sec:Kernels}
This section closely follows \cite[p. 79 ff.]{rasmussen2006}.
 %They are what makes a GP regression different from a standard linear regression approach: 
% A kernel function allows to not form a distribution over parameters of some predefined function but rather over the output functions themselves. 
 
Kernel functions were introduced in Section \ref{sec:GPIntro} as one of the building blocks of a GP. 
In a kernel function, no direct assumptions for the shape of the approximated function have to be made. For example, a polynomial could be resembled by a GP with a kernel function which is not defined using any polynomial itself. The same kernel function can be used to resemble a sine wave or any other function. If linear regression is used, there needs to be much more prior knowledge about the shape of the underlying function of the observed data to make accurate predictions: Approximating a sine wave needs periodical basis functions and approximating polynomials of a certain degree needs polynomial basis functions of at least that degree. Nevertheless, the kernel function still serves as a prior for the expected shape of the function. For example, an expected periodicity on the data or the smoothness of the underlying function can be taken into account by choosing an adequate kernel function and corresponding hyperparameters \cite{rasmussen2006}.

A kernel $k$ is a function which takes two points $x \in \mathbb{R}^n$ and $x' \in \mathbb{R}^n$ and maps them into $\mathbb{R}$ \cite[p.80]{rasmussen2006}, i.e. it returns a scalar which serves as a similarity measure between the two points \cite{gortler2019}. There holds according to \cite{gortler2019}
%
\begin{equation}
k: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}, \quad \Sigma = \mathrm{Cov}(x,x') = k(x,x') \;.
\end{equation}
%
If a kernel function is evaluated at a finite number of points, say all the test points, a covariance matrix $\Sigma$ is created. It can be used as the covariance of a multivariate Gaussian from which samples can be drawn. The values of each entry are a function over the input points of the kernel function and describe how much the two points are correlated \cite[p.14]{rasmussen2006}. 

In a covariance function all the assumptions of the function to be modeled are contained. The kernel defines or rather assesses how close or similar a value of one point is to the one of another. A training point which is close to a test point therefore gives information on how the value at the test point should be \cite[p.79]{rasmussen2006}. 
A kernel function is symmetric, i.e. ($k(x,x') = k(x',x)$), because it serves as a mesaure of similarity \cite[p.481]{murphy2012}. $x$ is as similar to $x'$ as vice versa.
Free parameters inside a kernel function are called hyperparameters. The term 'hyper' states that they are not parameters of the model itself but only of the kernel which defines properties of the model.

In the following, some important terminologies regarding kernel functions are introduced. Also, examples of common kernel functions are given and their characteristics are explained.

\paragraph{Stationarity}
Kernels which are a function of $x-x'$ are called \emph{stationary} kernels. They are stationary because it doesn't matter what value $x$ and $x'$ have as a starting value, the difference is always the same. Example: $110\mathrm{Pa} -100\mathrm{Pa} = 10\mathrm{Pa}, 1010\mathrm{Pa}-1000\mathrm{Pa} = 10\mathrm{Pa}$.
%
A radial basis function is a function which is only a function of $r=x-x'$ and which is therefore isotropic, i.e. there are no rigid motions \cite[p.80]{rasmussen2006}. 
Kernel functions always have to be symmetric, i.e. $k(x-x') = k(x'-x)$, because $x-x'$ is a distance measure.
The so-called Gram matrix is the function containing $x-x'$ evaluated at a set of points $x_i$. If the function is a covariance function, the Gram matrix is called covariance matrix.


\paragraph{Positive Definiteness} Following \cite[pp. 79, 80]{rasmussen2006}, a covariance function has to be positive semidefinite, i.e. all eigenvalues of the corresponding covariance matrix are greater or equal zero. A covariance matrix $\Sigma$ is positive semidefinite when $Q(\bm{v}) = \bm{v}^T \Sigma \bm{v} \geq 0, \quad \forall \bm{v} \in \mathbb{R}^n$, where $Q(\bm{v})$ is the quadratic form. A kernel function is positive semidefinite if it only outputs matrices which are positive semidefinite. 


\paragraph{The Characteristic Length Scale} The number of "upcrossings" in the interval, i.e. the number of crossings of the x axis can be thought of as the characteristic length scale of the kernel function. It can be calculated with \cite[Eq. 4.3]{rasmussen2006}. The more upcrossings, the shorter the length scale and the more jagged the function looks. Long length scales lead to rather smooth looking functions.

A comprehensive overview of different kernel functions and their behavior is given in \cite{Duvenaud} and \cite{gortler2019}. Nevertheless, some important kernels will be introduced below.
%

\paragraph{Linear Kernel Functions}
A general linear kernel is, following \cite{Duvenaud} defined as
\begin{equation}
k_l(x,x') = \sigma_b^2 + \sigma^2 (x-c) (x'-c) \;.
\end{equation}
As the name suggests, samples drawn from a GP with a linear kernel are linear functions. The parameter $c$ controls the point which all samples pass. Depending on the added noise, the variance around that point is much lower as in the other intervals. Figure \ref{fig:Linear} shows exemplaric samples. The mentioned common point can be found at $x=1$. Different to the other kernels introduced here, the linear kernel is non-stationary, i.e. the absolute position of the two inputs matters.
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.6\textwidth]{pics/LinearKernel.pdf}
\caption[Samples from a GP with linear kernel]{Samples drawn from a GP with a linear covariance function. The area represented in green shows the variance of which there is a region with relatively low variance through which all samples pass. The position can be moved by varying the hyperparameter $c$ of the kernel.}
\label{fig:Linear}
\end{center}
\end{figure}



\paragraph{Squared Exponential Kernel Functions}
The squared exponential kernel function is the most commonly used kernel function for GPs. It is stationary and infinitely differentiable, hence very smooth. A GP with this kernel function is equivalent to the above discussed linear regression model with infinitely many basis functions \cite{rasmussen2006}. This makes clear why GPs are considered non-parametric models. Not the parameters are optimized but the function itself. It wouldn't be possible to learn an infinite amount of parameters.
The squared exponential kernel function is defined as
\begin{equation}
k_{SE} = \sigma \exp(-\frac{r^2}{2l^2})
\end{equation}

\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/sqexp_f_sampled}
\caption[Samples from a GP with squared exponential kernel]{The source term GP of $f(x)$ sampled for a squared exponential kernel with $l = 0.25$ and $\sigma = 0.3$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green. The samples are very smooth, which is a charcteristic of this particular kernel.}
\label{fig:Sqexp}

\end{center}
\end{figure}

\paragraph{Periodic Kernel Functions}
The periodic kernel is defined as
\begin{equation}
k_p = \sigma^2 \exp(- \frac{2 \sin^2 (\pi r /p)}{l^2}) \;.
\end{equation}
The parameter $p$ defines the periodicity, i.e. the higher the value of $p$, the larger is the period.  $\sigma$ and $l$ have the same role as in the squared exponential kernel \cite{Duvenaud}. 
It can be used if a periodic behavior in the modeled process is expected. Figure \ref{fig:CovMats} shows an example of the generated covariance matrix. The periodicity is visible as a periodic high correlation between the dimensions. Figure \ref{fig:Periodic} shows three exemplaric samples from a GP with zero mean and a periodic kernel.
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.55\textwidth]{pics/PeriodicKernel.pdf}
\caption[Samples from a GP with a periodic kernel]{Three samples from a periodic kernel. The periodicity is clearly discernible. The variance, here in green, is constant.}
\label{fig:Periodic}
\end{center}
\end{figure}



\paragraph{Mat\'ern Kernel Functions}
The squared exponential kernel is infinitely differentiable, which means that it behaves very smoothly \cite{rasmussen2006}. This is considered as unphysical by \cite{stein1999}. Therefore, the Matern class of covariance functions \cite{matern2013} is recommended by \cite{stein1999} which is finitely differentiable and therefore less smooth. It is called a "class" of kernels because by varying the parameters, very different behaviours can be obtained, as shown in the following. The squared exponential kernel is actually a special case of the Mat\'ern class.

The general Mat\'ern kernel is defined as
\begin{equation}
k_M(r) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)}  \left( \frac{\sqrt{2\nu}r}{l}  \right)^{\nu} K_{\nu}  \left(  \frac{\sqrt{2\nu}r}{l}   \right)
\label{eqn:MaternKernel}
\end{equation}
with $\Gamma(\nu) = (\nu - 1)!$ the Gamma function and $K_{\nu}$ a modified Bessel function \cite[p.84 ff.]{abramowitz2013}, \cite{rasmussen2006}. $\nu$ and $l$ are free positive parameters.
For $\nu \to \infty$  (\ref{eqn:MaternKernel}) becomes the squared exponential kernel.
The covariance function simplifies for half integer values of $\nu$ and can be expressed as a product of an exponential and a polynomial. According to \cite{rasmussen2006} $\nu = 3/2$ and $\nu = 5/2$ are commonly used for GPs; the simplest Mat\'ern kernel is obtained with $\nu = 1/2$. For the mentioned kernel functions there holds

\begin{align}
k_{M}(r)_{\nu = 1/2} &=   \sigma^2 \exp(- \frac{r}{l})  \; , \\
\label{eqn:Materns1}
k_{M}(r)_{\nu = 3/2} &=  \sigma^2 \left(      1+ \frac{\sqrt{3}r}{l}  	\right)  \exp(- \frac{\sqrt{3} r}{l})  \; , \\
k_{M}(r)_{\nu = 5/2} &=  \sigma^2\left(      1+ \frac{\sqrt{5}r}{l}  + \frac{5r^2}{3l^2}	\right)  \exp(- \frac{\sqrt{5} r}{l}) \; .
\label{eqn:Materns3}
\end{align}
%
The above kernel functions are used in this thesis. Figures \ref{fig:Matern1_2} - \ref{fig:Matern5_2} show the differences between the various choices for $\nu$: The higher it is chosen, the smoother the GP becomes and the closer it gets to the squared exponential kernel.
\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/matern1_2_f_sampled}
\caption[Samples from a GP with a Mat\'ern kernel 1]{The source term GP of $f(x)$ sampled for a Mat\'ern kernel with $\nu=1/2$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green. For this value of $\nu$, the samples behave very rough.}
\label{fig:Matern1_2}

\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/matern3_2_f_sampled}
\caption[Samples from a GP with a Mat\'ern kernel 2]{The source term GP of $f(x)$ sampled for a Mat\'ern kernel with $\nu=3/2$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green. For this value of $\nu$, the samples behave less rough.}
\label{fig:Matern3_2}

\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/matern5_2_f_sampled}
\caption[Samples from a GP with a Mat\'ern kernel 3]{The source term GP of $f(x)$ sampled for a Mat\'ern kernel with $\nu=5/2$. $\bar{f}(x)$ in solid black, $2\sigma$ confidence bands in green. For this value of $\nu$, the samples behave almost as smooth as in the Squared Exponential kernel.}
\label{fig:Matern5_2}

\end{center}
\end{figure}




\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.6\textwidth]{pics/CovMats.pdf}
\caption[Illustration of the covartiance matrices fpr different kernel functions]{The covariance matrices for the different kernel functions. 100 linearly spaced test points between $-5$ and $5$ were used as inputs. The Squared Exponential and Mat\'ern kernels yield similar results. For the periodic kernel, the periodicity is also visible in the covariance matrix. For the covariance matrix derived from the linear kernel, the region of lower variance is visible.}
\label{fig:CovMats}
\end{center}
\end{figure}



\paragraph{Combination of Different Kernel Functions}
Following \cite{Duvenaud, gortler2019}, different kernels can now easily be combined by adding or multiplying individual functions. For example, a periodic kernel can be superimposed on a linear kernel by addition which would lead to a periodic kernel with a mean that changes linearly. Multiplying two linear kernels leads to sampled functions which are quadratic. Multiplying across dimensions, i.e. for example
\begin{equation}
k_{new}(x,y,x',y') = k_x(x,x')k_y(y,y') \;,
\end{equation}
yields functions in which the function values are correlated on two axes \cite{Duvenaud}. 

\subsection{Bayesian Inference}
Now that the mathematical prerequisites have been discussed, the actual Bayesian inference with GPs can be explained. The goal is to condition a prior GP on observed data. This means that the variance over the mean function in a GP should become lower at positions where measurements have been taken, since at these locations the value of the measured quantity is known accurately. Also, the mean function will change to that now well known value. At first, knowledge of the measured quantity is collected as information within the Bayesian prior. This section follows \cite{gortler2019, rasmussen2006}.
\paragraph{Prior}
In a differential equation, such as the ones used in this thesis (see \eqref{eqn:PoissonEq}, \eqref{eqn:Helmholtz}), one or more spatial parameters or the source term can be considered as random. Assuming a probability distribution for these parameters according to already given knowledge and solving the PDE leads to the so called prior. The prior, because it is modeled as random, already involves a mean and a covariance for the parameters. Constructing the prior as a GP already at this point is a useful approach to make the further steps easier, since a covariance matrix and a mean function are already known, which are later used for inference. Therefore, the mean and covariance of the random parameters are constructed as mean and covariance \emph{functions} within the framework of a GP, which in turn yields a prior distribution over functions. To obtain the covariance matrix, a kernel function suitable to the problem has to be chosen. At this point, the hyperparameters are chosen manually according to prior belief and knowledge: They can for example be adjusted to make the function behave more smoothly or to change the confidence bands in which possible functions may be sampled. Then, evaluating the kernel function at a set of test points leads to the covariance matrix. The mean vector is chosen manually, as well.
To visualise the prior, samples from it can be drawn.
The following closely follows \cite[A2]{rasmussen2006}.
To obtain a sample from the multivariate Gaussian which represents the GP, at first the Cholesky decomposition of the covariance matrix $\bm{\Sigma} = \bm{L}\bm{L}^T$ has to be computed \cite[p. 29]{girolami2021}. Next, a sample from a standard multivariate normal distribution $\bm{u} \sim \mathcal{N} (\bm{0},\bm{I})$ is drawn. The final sample $\bm{x}$ can now be obtained using
\begin{equation}
\bm{x} = \bm{m} + \bm{L} \bm{u} 
\end{equation}
with the prescribed mean vector $\bm{m}$.

Having the prior GP defined, the posterior can be formed using observed data.
	
	

\paragraph{Posterior after observation}
\label{sec:GPcond}
As the observed data, also called the training data, is available, the prior GP can be conditioned on those in order to compute a posterior distribution. It would also be possible, but computationally demanding, to sample many functions out of the GP and only take those which go through the training points \cite[pp. 15, 16]{rasmussen2006}. If the observations are assumed to be noise-free, a joint distribution of the prior GP (here $X$ with the kernel function evaluated at the chosen test points to form the covariance matrix) and the training data GP (here $Y$) can be formed easily. The training data, even though noise-free, is a GP and therefore a multivariate Gaussian, because also here a covariance is formed by evaluating the kernel function at the sensor locations. The measured values form the mean vector. Hence, the joint density can be expressed as
%
\begin{equation}
f(X,Y) \sim \mathcal{N}(\bm{\mu},\Sigma) = \begin{bmatrix}
           X \\
           Y
         \end{bmatrix} \sim \mathcal{N}\left( \bm{\mu}, \begin{bmatrix}
\bm{K}_{XX} & \bm{K}_{XY} \\
\bm{K}_{YX} & \bm{K}_{YY} 
\end{bmatrix}  \right) \; .
\label{eqn:noisefreeJoint}
\end{equation}
The non-diagonal entries in \eqref{eqn:noisefreeJoint} are formed by evaluating the kernel function with both the test and training points as arguments: For $r = |\bm{x}-\bm{x'}|$ there holds $r = |\bm{x} - \bm{y}|$ with $\bm{x}$ and $\bm{y}$ the vectors of the spatial coordinates for test and training points.
This density now has to be conditioned on the training data. This means that all samples from the new, conditioned, density will pass through the training points. This process can be thought of as sampling one function at a time from \eqref{eqn:noisefreeJoint} and collecting all samples which pass through the training points, discarding all other ones. Of course, mathematically, this can be done using the already shown equations for conditioning densities, see Section \ref{sec:Marg} and \eqref{eqn:condMat}.

For noisy observations there is an uncertainty in the measurement method, a diagonal noise term is added to the observation GP. For the joint distribution there now holds
\begin{equation}
f(X,Y) \sim \mathcal{N}(\bm{\mu},\Sigma) = \begin{bmatrix}
           X \\
           Y
         \end{bmatrix} \sim \mathcal{N}\left( \bm{\mu}, \begin{bmatrix}
\bm{K}_{XX} & \bm{K}_{XY} \\
\bm{K}_{YX} & \bm{K}_{YY} + \sigma_n^2 \bm{\mathbb{1}} 
\end{bmatrix}  \right) \; ,
\label{eqn:noiseJoint}
\end{equation}
with $\sigma_n^2$ the variance of an independent and identically distributed Gaussian noise and $\mathbb{1}$ the identity matrix. Conditioning on $Y$ now yields
\begin{equation}
\begin{aligned}
X|Y \sim \mathcal{N}(\mu_X +\bm{K}_{XY} [\bm{K}_{YY}+ \sigma_n^2 I ]^{-1}(Y-\mu_Y), \\
\bm{K}_{XX} - \bm{K}_{XY}[\bm{K}_{YY}+ \sigma_n^2 \mathbb{1} ]^{-1}\bm{K}_{YX} ) \;.
\end{aligned}
\label{eqn:ConditioningDistr}
\end{equation}
The conditioned distribution is a GP again: Conditioning therefore basically means interpolating between and extrapolating from the sensor locations \cite{damianou2021}. 
%
%
Figure \ref{fig:GPExpl} shows both the GP prior and posterior. It is visible that the prior on the left has a zero mean. Arbitrarily many samples can be drawn from it which themselves do not necessarily have a zero mean, as well. If that prior GP is now conditioned on training points, two in this case, the mean of the newly formed GP will follow the training points and therefore deviate from the prior mean. All samples drawn from the posterior GP will go through the training points.  It is also visible that the variance decreases in the surrounding of a measurement: Exactly on the (here noise-free) training points, the variance is zero and in between them it is still much lower than the prior variance. Finding a new mean and variance between training points is called interpolation and, as visible, yields solutions with relatively low variance. However, extrapolation yields results with higher variance: If only on one side of the domain a training point is available, the variance and the mean quickly become equal to those of the prior again.
\begin{figure}[!ht]
\begin{center}

\includegraphics[width=1\textwidth]{pics/GPExpl}
\caption[Illustration of GP conditioning]{The prior GP on the left and the posterior GP after conditioning on two training points on the right. The variance approaches zero at the noise-less training points. The further away from the training points, the closer the variance is back at the prior variance.}
\label{fig:GPExpl}

\end{center}
\end{figure}

Figure \ref{fig:CovPOst} shows the new covariance matrix after conditioning. It is visible that the variance is highest in the top right corner, which corresponds to values around $1.0$ and is in agreement with Figure \ref{fig:GPExpl}: The entries of the covariance matrix become lower in regions around a sensor location.

\begin{figure}[!ht]
\begin{center}

\includegraphics[width=0.6\textwidth]{pics/CovPosterior}
\caption[Covariance Matrix before and after conditioning]{The covariance matrix before and after conditioning. The lighter areas show higher (co)variance. The part of the domain on the right, where no observations were made, is visible as entries with high variance in the top right.}
\label{fig:CovPOst}

\end{center}
\end{figure}

As introduced in Section \ref{sec:Marg}, the marginal likelihood can also be computed. It is used in the procedure for optimizing the hyperparameters of the chosen kernel function, as it will be explained in detail in Section \ref{sec:HyperParEst}. Marginalizing over the function values at the test points leaves behind the probability density for only the observations: In accordance with \eqref{eqn:margi} there holds for the marginal likelihood
\begin{equation}
f(\bm{y}) = \int f(\bm{y}|\bm{u}) f(\bm{u}) \, \mathrm{d}\bm{u} \;.
\label{eqn:marginGP}
\end{equation}
As until this point all necessary theory of GPs has been explained, statFEM can now be introduced in the next chapter.







\section{The Statistical Finite Element Method}
The FEM is based on a model in form of a differential equation which is used to describe some kind of phenomenon, in the case of this thesis vibroacoustics, as accurately as possible. However, every model is only an approximation of reality which gives rise to a model inadequacy error: There is some unknown physics hidden between the model and reality. By collecting data, basically measuring reality with some kind of measurement error, that discrepancy $d$ between model and reality can be inferred. That is one part of what statFEM does. It finds a new model, based on data, which compensates the model inadequacy of the FEM. Also, it makes it possible to take measurement noise and uncertainty in the model parameters into account. By design, statFEM weighs all sources of uncertainty and provides a posterior model which is a representation of both FEM and data and hence makes an accurate prediction of reality, also of the uncertainties involved, possible.

\paragraph{The Statistical Generating Model}
Measuring data always involves a measurement error: One can never measure any physical quantity without some measurement noise. Therefore, the so-called statistical generating model \cite{girolami2021,kennedy2001} for a vector of $n_y$ measurements $\bm{y} \in \mathbb{R}^{n_y}$, 
%
\begin{equation}
\bm{y} = \bm{z} + \bm{e} = \rho \bm{P} \bm{u} + \bm{d} + \bm{e} \; ,
\label{eqn:statGen}
\end{equation}
%
consists of the response of the true underlying process $\bm{z} \in \mathbb{R}^{n_y}$ at the measurement points and the measurement noise $\bm{e} \in \mathbb{R}^{n_y}$. The true process $\bm{z}$ can further be expressed as a combination of the used FEM model solution $\bm{u} \in \mathbb{R}^{n_u}$, which tries to describe the true process, and a model discrepancy $\bm{d} \in \mathbb{R}^{n_y}$ which accounts for the model not being a completely accurate representation of the true physics. The model is scaled using the scaling parameter $\rho \in \mathbb{R}$. It can be evaluated at $n_u$ points which are determined by the FEM mesh but only its evaluations at the measurement points are taken into account by using the projection matrix $\bm{P} \in \mathbb{R}^{n_y \times n_u}$. $n_u$ is the number of the so-called degrees of freedom of the domain, i.e. the number of nodes on which a solution is approximated.
%
$\bm{e}$ is modeled as a multivariate Gaussian 
%
\begin{equation}
\bm{e} \sim p(\bm{e}) = \mathcal{N}(\bm{0}, \bm{C_e})
\label{eqn:GeneratingNoise}
\end{equation}
with zero mean and the covariance matrix $\bm{C_e} = \sigma_e^2 \bm{I}$ which adds the measurement noise to each entry of $\bm{z}$.
%
The model discrepancy is, in order to account for a possibly complex behavior, modeled as a GP
\begin{equation}
\bm{d} \sim p(\bm{d} | \sigma_d, l_d) = \mathcal{N}(\bm{0},\bm{C_d})
\end{equation}
which $\sigma_d, l_d$ the unknown parameters of a squared exponential kernel function. Evaluating the kernel function at the measurement positions, the GP is represented by a multivariate Gaussian with a covariance matrix $\bm{C_d} \in \mathbb{R}^{n_y \times n_y}$.




\paragraph{Basic statFEM procedure}
statFEM can basically be broken down into three major steps \cite{girolami2021}. The first one is applying Bayesian inversion to the FEM part of the solution. The prior GP is constructed and evaluated before the data are introduced. Equations are derived which incorporate the data in order to compute a posterior density for the FEM solution GP. The equations for that solution are dependent on certain hyperparameters. These are estimated in the second step: For each of the hyperparameters a posterior density is calculated using a prior and the marginal likelihood. The third and last step is finding a mesh size which leads to the highest posterior probability given the data $p(\mathcal{M}_j|\bm{Y}_i)$ with $\mathcal{M}_j$ the different meshes. This last step may appear rather unimportant since usually in FEM the consensus is that a finer mesh leads to a more accurate result. However, it is demonstrated in \cite{girolami2021} that the optimum element size strongly depends on the value of certain hyperparameters estimated in step 2 and that there is a correlation between relatively inaccurate statFEM results and a too small element size. Step 3 is omitted in this thesis.

\subsection{Posterior of the FEM Forward Model}
In the following, the procedure to generate the posterior of the FEM forward model is explained. As a PDE, the Poisson equation \eqref{eqn:PoissonEq} serves as an example in this whole section.



\paragraph{Assembling the FEM System Matrix}
This follows \cite{langtangen2016}. The FEM assembly and solving is handled by FEniCS. To compute the system matrix and to later solve for the vector of unknowns, the PDE has to be defined in a variational formulation as a boundary value problem.
At first, the FEM solver is initiated by defining the domain, the desired number of elements and the kind of elements to be used. In this example, a 1-dimensional mesh with Lagrange basis functions is used.
\begin{python}
dom_a = 0.0 #domain boundaries
dom_b = 1.0
ne	= 100 #number of elements
mesh = IntervalMesh(ne,dom_a,dom_b) #define mesh
coordinates = mesh.coordinates() # vertices vector
V = FunctionSpace(esh, "Lagrange", 1) # Function space
\end{python}
The variational formulation for the chosen PDE consists of a linear form $L$ and a bilinear form $a$. These need to be specified individually for FEniCS. The boundary condition is set to $u_0 = 0.0$ for both boundaries in the 1D mesh. The system matrix $\bm{A}$ can be returned by calling \mintinline{python}{A = assemble(a)}. The boundary conditions are applied to $\bm{A}$ after assembly. A complete example of a Python script using FEniCS to solve an FE model can be found in the appendix, Section \ref{sec:FEMsolveCode}.



\paragraph{Generate and Sample From Source Term GP}
The differential equation is treated from a Bayesian viewpoint: All parameters are random variables. For the Poisson equation \eqref{eqn:PoissonEq} these parameters are $\bm{\kappa}(x)$ and $\bm{f}(x)$, for the Helmholtz equation \eqref{eqn:Helmholtz} the random parameters could be the wave number $k$ or, as it will be shown in Section \ref{sec:2DFEMprior}, a parameter in the BCs such as the displacement $\bar{\bm{U}}$. For distributed parameters, such as a source term $\bm{f}(x)$ dependent on the spatial coordinate, not a single distribution but a GP is applied. Therefore prior to solving the FEM linear system, the parameter $ \mathcal{GP} ( \bar{\bm{f}},\bm{C}_f)$ is sampled. That sample is evaluated for each cell in the FEM mesh which makes it necessary to assemble the system matrix with that in mind. 
$\bar{\bm{f}}$ is set to a prescribed value. $\bm{C}_f$ is calculated using the prescribed hyperparameters of the chosen kernel function. As an example, a sample from $ \mathcal{GP} ( \bar{\bm{f}},\bm{C}_f)$ can be drawn for a Mat\'ern kernel by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def sample_f(self):
	#matern covariance function:
	c_f = matern_log(coordinates,
		coordinates, l=np.log(lf), sig=np.log(sigf))
	f_mean = (np.pi)**2*(1/5)*np.ones(ne+1) #constant mean
	#sampling:
	fGP = np.random.multivariate_normal(
		mean = f_mean, cov=c_f,
		size=10)
\end{minted}
Here, the Mat\'ern kernel accepts only the $\log$ of the parameters $\text{lf}$ and $\text{sigf}$.



\paragraph{Generate and sample from the diffusion coefficient GP}
If the diffusion coefficient $\kappa$ is assumed to be a random variable and dependent on $x$, it can be modeled as a GP as well and there holds
\begin{equation}
\bm{\kappa} \sim \mathcal{GP}(\bar{\bm{\kappa}},\bm{C}_{\kappa}) \;.
\end{equation}

\paragraph{Computing the FEM Prior}
Now that having the GPs for the input parameters defined, the FEM system of equations can be solved. With the input parameters defined as a GP, also the FEM solution is going to be a GP if the problem is linear \cite[p. 5]{girolami2021}. This holds for the Poisson equation \eqref{eqn:Poisson} if the uncertainty is only in the source term. Hence,
\begin{equation}
\bm{u} \sim \mathcal{GP}(\bar{\bm{u}}, \bm{C}_u)
\label{eqn:SolutionGP}
\end{equation}
is to be calculated. Multiplying the FEM system of equation, see \eqref{eqn:LinSys}, by $\bm{A}^{-1}$ from the left there holds
\begin{equation}
\bm{u} = \bm{A}^{-1}\bm{f} \;.
\end{equation}
\cite[p. 1428]{arens2015} states that the expectation operator $\mathbb{E}$ is linear for independent random variables $X_1,X_2,...,X_n$ what means that
\begin{equation}
\mathbb{E}\left[  \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \mathbb{E}[X_i] 
\end{equation}
and
\begin{equation}
\mathbb{E}\left[ \alpha X   \right] =\alpha \mathbb{E}\left[  X   \right]
\end{equation}
for a deterministic scalar $\alpha$.
A GP can be described by a set of independent normally distributed random variables. Because of the linearity there holds, also in matrix notation,
\begin{equation}
\bar{\bm{u}} = \mathbb{E}[\bm{A}^{-1} \bm{f}] = \bm{A}^{-1} \mathbb{E}[\bm{f}] = \bm{A}^{-1} \bar{\bm{f}}
\end{equation}
and, acc. to \cite{schon2011, bardsley2018}, making use of the definition of covariance (\ref{eqn:Covariance}),
\begin{align}
\begin{split}
\bm{C}_u &= \mathbb{E}[(\bm{u}-\bar{\bm{u}})(\bm{u}-\bar{\bm{u}})^T] \\
	&= \mathbb{E}\left[\left((\bm{A}^{-1} \bm{f}) - (\bm{A}^{-1} \bar{\bm{f}}) \right) \left( (\bm{A}^{-1} \bm{f}) - (\bm{A}^{-1} \bar{\bm{f}}) \right)^T   \right]\\
	&= \mathbb{E}\left[  \left( \bm{A}^{-1}(\bm{f}-\bar{\bm{f}})   \right)  \left( \bm{A}^{-1}(\bm{f}-\bar{\bm{f}})   \right)^T  \right]\\
	&= \mathbb{E} \left[    \bm{A}^{-1} ( \bm{f} - \bar{\bm{f}} ) ( \bm{f} - \bar{\bm{f}} )^T \bm{A}^{-T}   \right]\\
	&= \bm{A}^{-1}   \mathbb{E}\left[ (\bm{f}-\bar{\bm{f}}) (\bm{f}-\bar{\bm{f}})^T   \right] \bm{A}^{-T} \\
	&= \bm{A}^{-1} \bm{C}_f \bm{A}^{-T}  \;.
\end{split}
\label{eqn:Covarf}
\end{align}
%	 \\
%	 \\
%	
Therefore, there holds for the multivariate Gaussian, which describes the new $\bm{u} \sim \mathcal{GP}(\bar{\bm{u}}, \bm{C}_u)$
\begin{equation}
\bm{u} \sim p(\bm{u}) = \mathcal{N} \left( \bm{A}^{-1} \bar{\bm{f}}, \bm{A}^{-1} \bm{C}_f \bm{A}^{-T}  \right) \;.
\label{eqn:u_GP_disc}
\end{equation}
As visible in (\ref{eqn:u_GP_disc}), there holds for $\bar{\bm{u}}$
\begin{equation}
\bar{\bm{u}} = \bm{A}^{-1} \bar{\bm{f}} \; ,
\label{eqn:meanEqSys}
\end{equation}
which can be computed using FEniCS by calling
\begin{python}
def get_U_mean():
	u_mean = Function(V)
	U_mean = u_mean.vector()
	b_mean = assemble(f_mean*v*dx) 
	bc.apply(b_mean)
	A = doFEM()  #get FEM system matrix, see appendix
	solve(A,U_mean,b_mean) #solve the system
	return U_mean .
\end{python}
\label{lst:get_u_mean}
%$\bm{C}_u$ is obtained by calculating
%\begin{equation}
%\bm{C}_u = \bm{A}^{-1} \bm{C}_f \bm{A}^{-T} \; .
%\end{equation}
In Python code there holds for $\bm{C}_u$
\begin{python}
def get_C_u(self):
	C_f = get_C_f() #get the parameter covariance
	A = doFEM() #get FEM system matrix, see appendix
	ident = np.identity(len(self.coordinates))
	A = A.array()
	A_inv = np.linalg.solve(A,ident)
	C_u = np.dot( np.dot(A_inv,C_f), np.transpose(A_inv)) #calculate C_u
	C_u = C_u + 1e-16*(sigf**2)*ident # add a nugget
	return C_u .
\end{python}
\label{lst:get_C_u}
With $\bar{\bm{u}}$ and $\bm{C}_u$ the GP for the prior is completely defined. 

To check if the covariance matrix is correct, a MC approximation $\bar{\bm{u}}_{MC}$ of the mean can be computed by evaluating the GP $n_{MC}$ times and taking the average. As visible in Figure \ref{fig:u_mean_conv}, the error of $\bar{u}_{MC}$ converges to zero with a slope of 2 on a logarithmic scale. Therefore, $\bm{u}$ indeed has a mean of $\bar{\bm{u}}$.
\begin{figure}[!ht]
\includegraphics[width=0.6\textwidth]{pics/MCerrorConv.pdf}
\centering
\caption[Convergence of the error norm of the FEM prior]{Convergence of the error norm for the FEM prior mean with a slope of 2 on a log-log scale. The error converges to zero. This means that the covariance matrix has been estimated correctly.}
\label{fig:u_mean_conv}
\end{figure}

The MC samples can be drawn via
\begin{python}
def samplePrior():
	U_mean = get_U_mean()
	C_u = get_C_u()
	nMC = 200 # number of MC samples. The more, the more accurate the result.
	priorGP = np.random.multivariate_normal(
		mean = U_mean, cov=C_u,
		size=nMC)
	return priorGP
\end{python}

If there is uncertainty also in the diffusion coefficient $\mu(x)$, the problem becomes non-linear. In order to still get a closed form for $\bm{C}_u$, \cite[p. 7]{girolami2021} proposes to use a pertubation method which leads to a first order approximation of \eqref{eqn:SolutionGP}: Without proof, there holds 
\begin{equation}
\bm{C}_u = \bm{A}^{-1} \bm{C}_f \bm{A}^{-T} + \sum_e \sum_d (\bm{C}_{\kappa})_{ed} \bm{A}^{-1} \pdv{\bm{A}}{\kappa_e} \bm{A}^{-1} (\bm{C}_f + \bar{\bm{f}} \otimes \bar{\bm{f}}) \bm{A}^{-T}\pdv{\bm{A}^T}{\kappa_d} \bm{A}^{-T} \;.
\end{equation}


\paragraph{Find Prior Mean and Variance}
Once the prior is computed, it can be sampled. The mean is already given but the parameters of the underlying GP are unknown. It is possible to evaluate $\bm{C}_u$ for the standard deviation with
\begin{equation}
\bm{\sigma_u}^2 =  \mathrm{diag}(\bm{C}_u).
\label{eqn:varCu}
\end{equation}


\paragraph{The Projection Matrix $\bm{P}$}
The projection matrix $\bm{P} \in \mathbb{R}^{n_y \times n_u}$ is constructed by evaluating all $n_u$ FEM ansatz functions of the mesh at the $n_y$ measurement positions $y_i, \; i=1,...,n_y$. For each node there is a corresponding ansatz function which can be evaluated all over the domain. The result is a sparse matrix with most entries zero. Only those entries are non-zero which correspond to the ansatz functions $\phi_i(\bm{\bm{x}})$ which are associated with the finite element in which the measurement positions lie. The matrix is used below to project the matrices resulting from the FEM prior onto the coordinates of the observations. Figure \ref{fig:PIllust} shows how the individual entries of $\bm{P}$ are created for a 1D problem. Observation points, i.e. the training points, are displayed in red whereas all other points, i.e. test points, are displayed in green. The hat functions are $1$ only at one single node.
%
\begin{figure}[!ht]
\begin{center}

\begin{tikzpicture}[remember picture,scale=\textwidth/25.0cm,samples=200]
	\begin{pgfonlayer}{nodelayer}
		\node [style=none] (1) at (7, 0) {};
		\node [style=none] (2) at (-7, 0) {};
		\node [style=none] (3) at (-5, 0) {};
		\node [style=none] (4) at (-3, 0) {};
		\node [style=none] (5) at (-1, 0) {};
		\node [style=none] (6) at (1, 0) {};
		\node [style=none] (8) at (5, 0) {};
		\node [style=none] (9) at (3, 0) {};
		\node [style=none] (10) at (-7, 3) {};
		\node [style=none] (11) at (8, 0) {};
		\node [style=none] (12) at (7, 0) {};
		\node [style=none] (13) at (8, 0) {};
		\node [style=none] (14) at (8, 0) {};
		\node [style=green] (15) at (-7, 0) {};
		\node [style=green] (16) at (-5, 0) {};
		\node [style=green] (17) at (-3, 0) {};
		\node [style=green] (18) at (-1, 0) {};
		\node [style=green] (19) at (1, 0) {};
		\node [style=green] (20) at (3, 0) {};
		\node [style=green] (21) at (5, 0) {};
		\node [style=green] (22) at (7, 0) {};
		\node [style=none] (23) at (8.25, -0.25) {};
		\node [style=none] (24) at (8.25, 0) {$x$};
		\node [style=none] (32) at (-7.25, 2) {};
		\node [style=none] (33) at (-6.75, 2) {};
		\node [style=none] (34) at (-7.25, 2) {};
		\node [style=none] (35) at (-7.5, 2) {$1$};
		\node [style=none] (36) at (-7, 2) {};
		\node [style=none] (37) at (-5, 2) {};
		\node [style=none] (38) at (-3, 2) {};
		\node [style=none] (39) at (-1, 2) {};
		\node [style=none] (40) at (1, 2) {};
		\node [style=none] (41) at (3, 2) {};
		\node [style=none] (42) at (5, 2) {};
		\node [style=none] (43) at (7, 2) {};
		\node [style=none] (44) at (-7, -0.5) {$0$};
		\node [style=none] (45) at (-5, -0.5) {$1$};
		\node [style=none] (46) at (-3, -0.5) {$2$};
		\node [style=none] (47) at (-1, -0.5) {$3$};
		\node [style=none] (48) at (1, -0.5) {$4$};
		\node [style=none] (49) at (3, -0.5) {$5$};
		\node [style=none] (50) at (5, -0.5) {$6$};
		\node [style=none] (51) at (7, -0.5) {$7$};
		\node [style=none] (52) at (-5, 2.5) {$\varphi_1$};
		\node [style=none] (53) at (-3, 2.5) {$\varphi_2$};
		\node [style=none] (54) at (-1, 2.5) {$\varphi_3$};
		\node [style=none] (55) at (1, 2.5) {$\varphi_4$};
		\node [style=none] (56) at (3, 2.5) {$\varphi_5$};
		\node [style=none] (57) at (5, 2.5) {$\varphi_6$};
		\node [style=none] (58) at (7, 2.5) {$\varphi_7$};
		\node [style=none] (59) at (-6.5, 2.5) {$\varphi_0$};
		\node [style=new style 0] (60) at (-5, 0) {};
		\node [style=new style 0] (61) at (1, 0) {};
		\node [style=new style 0] (62) at (3, 0) {};
		\node [style=none] (63) at (-5, -1.25) {$y_0$};
		\node [style=none] (64) at (1, -1.25) {$y_1$};
		\node [style=none] (65) at (3, -1.25) {$y_2$};
	\end{pgfonlayer}
	\begin{pgfonlayer}{edgelayer}
		\draw [in=180, out=0] (2.center) to (1.center);
		\draw [style=arrow] (14.center) to (12.center);
		\draw [style=arrow, in=90, out=-90] (10.center) to (2.center);
		\draw [style=line] (15) to (22);
		\draw [style=line] (32.center) to (33.center);
		\draw [style=blue line] (36.center) to (16);
		\draw [style=red line] (15) to (37.center);
		\draw [style=red line] (37.center) to (17);
		\draw [style=red line] (17) to (18);
		\draw [style=red line] (18) to (19);
		\draw [style=red line] (19) to (20);
		\draw [style=red line] (20) to (21);
		\draw [style=red line] (21) to (22);
		\draw [style=green line] (16) to (38.center);
		\draw [style=green line] (38.center) to (18);
		\draw [style=green line] (18) to (19);
		\draw [style=blue line] (16) to (17);
		\draw [style=blue line] (17) to (39.center);
		\draw [style=blue line] (39.center) to (19);
		\draw [style=red line] (18) to (40.center);
		\draw [style=red line] (40.center) to (20);
		\draw [style=green line] (19) to (41.center);
		\draw [style=green line] (41.center) to (21);
		\draw [style=blue line] (20) to (42.center);
		\draw [style=blue line] (42.center) to (22);
		\draw [style=red line] (21) to (43.center);
	\end{pgfonlayer}
\end{tikzpicture}

\begin{equation}
P =
\begin{bmatrix}
0 & \rn{101}{1} & 0 & 0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & \rn{102}{1} & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 0 & \rn{103}{1} & 0 & 0
\end{bmatrix}
\end{equation}

\caption[Illustration of the constitution of the projection matrix P]{The matrix $P$ contains the values of the FEM ansatz functions belonging to the nodes on which observations are available. Since linear Lagrange elements yield $1$ at their respective node number and $0$ on all other nodes, $P$ consists of only $1$s and $0$s.}
\label{fig:PIllust}


\begin{tikzpicture}[overlay,remember picture]
	\draw [->] (3) to [out=-40,in=30] (101);
	\draw [->] (6) to [out=-50,in=60] (102);
	\draw [->] (9) to [out=-120,in=60](103);
\end{tikzpicture}
\end{center}
\end{figure}
%
This computation for linear ansatz functions can easily be implemented in Python using FEniCS:
\begin{python}
def getP(y_points):
	ny = len(y_points) #number of sensors
	P = np.zeros((ny, ne+1), dtype = float) #with ne the number of elements
	for j,point in enumerate(y_points):
		x = np.array([point])
		x_point = Point(x)
		bbt = mesh.bounding_box_tree()
		cell_id = bbt.compute_first_entity_collision(x_point)
		cell = Cell(mesh, cell_id)
		coordinate_dofs = cell.get_vertex_coordinates()
		values = np.ones(1, dtype=float)
		for i in range(el.space_dimension()): 
			phi_x = el.evaluate_basis(np.array(i),x 
				,np.array(coordinate_dofs),cell.orientation())
			P[j,cell_id+i] = phi_x
	return P
\end{python}
\label{lst:getP}
\paragraph{Calculate $\bm{C}_e$}
$\bm{C}_e$ is the measurement error term. It is defined in Python by
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def get_C_e(size):
	sige_square = 2.5e-5
	C_e = sige_square * np.identity(size)
	return C_e .
\end{minted}
It is the identity amtrix multiplyed with the desired measurement error.
\paragraph{Calculate $\bm{C}_d$}
The model discrepancy term $\bm{d}$ has a covariance matrix $\bm{C}_d$ and is modeled as a GP with mean $\bar{\bm{d}} = \bm{0}$. Therefore it can be generated by calling
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def get_C_d(y_points,ld, sigd):
	C_d = matern_log(y_points, y_points, l=ld, sig=sigd)
	return C_d .
\end{minted}
The points in the domain and the hyperparameters have to be provided in order to calculate the covariance function. The latter ones are not yet known and need to be estimated, see below for a thorough explanation on that.

\subsection{Inference of the Posterior Density}
This follows \cite[p.10 ff.]{girolami2021}.
To infer the posterior density, observed data is necessary. These can be obtained by either actually measuring a physical process or by generating synthetic data which allows developing and improving the statFEM model already before having to set up an experiment and take real measurements beforehand. Equipping a real system with sensors and then collecting data is expensive, therefore it is reasonable to use synthetic data to test all used methods before taking real measurements. Here, only synthetic data are going to be used.
%
Sampling from a synthetic source or from the real physical response yields the observation vector
%
\begin{equation}
\bm{y} = \bm{z} + \bm{e} \, ,
\end{equation}
%
with $\bm{z}$ the real response and $\bm{e}$ the measurement error which is also a part of $\bm{y}$ for the synthetic observations. It can be seen that the model mismatch error $\bm{d}$ is, as opposed to (\ref{eqn:statGen}), not part of the equation because it is only part of the FEM modeling error. 
%
The data is observed on multiple measurement points $y_i$ throughout the domain. It can be chosen if the data is assumed to be deterministic or to be random. For the deterministic case only one value is measured for each measurement point. In the random case many measurements are taken per point what leads to a probability distribution for each point.
%
Now, Bayes' rule can be applied to update the prior with the observations \cite{girolami2021}: 

\begin{equation}
p(\bm{u}|\bm{y}) = \frac{p(\bm{y}|\bm{u})p(\bm{u})}{p(\bm{y})} \propto p(\bm{y}|\bm{u})p(\bm{u}) \;.
\label{eqn:bayesPrior}
\end{equation}
The marginal likelihood is, in form of a normalization term, going to be reintroduced in the end. Both densities $p(\bm{y}|\bm{u})$ and $p(\bm{u})$ are considered to be multivariate Gaussian, see (\ref{eqn:statGen}) and (\ref{eqn:MultivariateGaussian}):
\begin{align}
p(\bm{y}|\bm{u}) &\sim \mathcal{N} (\rho \bm{P} \bm{u}, \bm{C}_d + \bm{C}_e) \\
p(\bm{u}) &\sim \mathcal{N}(\bm{\bar{u}},\bm{C}_u)
\end{align}
%
This property makes the multiplication simple, again excluding the marginal likelihood:
\begin{equation}
p(\bm{u}|\bm{y}) \propto  \exp(  (\rho \bm{P} \bm{u} -\bm{y})^T (\bm{C}_d + \bm{C}_e)^{-1} (\rho \bm{P} \bm{u} -\bm{y})  ) \exp(  (\bar{\bm{u}} - \bm{u})^T \bm{C}_u^{-1}  (\bar{\bm{u}} - \bm{u})^T) 
\end{equation}
Following \cite{girolami2021}, with
\begin{align}
\begin{split}
\bm{B} &= \rho^2 \bm{P}^T (\bm{C}_d +\bm{C}_e)^{-1} \bm{P} + \bm{C}_u^{-1}\;, \\
\bm{a} &= \rho \bm{P}^T (\bm{C}_d +\bm{C}_e)^{-1} \bm{y} + \bm{C}_u^{-1} \bm{\bar{u}} 
\end{split}
\end{align}
there holds
\begin{align}
\begin{split}
p(\bm{u}|\bm{y}) &\propto \exp(\bm{u}^T \bm{B} \bm{u} - 2\bm{a}^T\bm{u} + ... ) \\
&= \exp( (\bm{B}^{-1} \bm{a} - \bm{u} )^T  \bm{B} (\bm{B}^{-1} \bm{a} - \bm{u} ) ) \;.
\label{eqn:p_u_y_unnorm}
\end{split}
\end{align}
Comparing \eqref{eqn:p_u_y_unnorm} to \eqref{eqn:MultivariateGaussian} it can be argued that \eqref{eqn:p_u_y_unnorm} can be normalized to 
\begin{equation}
p(\bm{u}|\bm{y}) = \frac{1}{\sqrt{(2\pi)^{n_u} \lvert \bm{B} \rvert}} \exp( (\bm{B}^{-1} \bm{a} - \bm{u} )^T  \bm{B} (\bm{B}^{-1} \bm{a} - \bm{u} ) ) \;.
\end{equation} 
%
The result of this operation is
%
\begin{equation}
p(\bm{u}|\bm{y}) = \mathcal{N}(\bar{\bm{u}}_{|\bm{y}}, \bm{C}_{u|\bm{y}})
\label{eqn:statFEMConditioned}
\end{equation}
%
with
%
\begin{equation}
\bm{\bar{u}}_{|\bm{y}} = \bm{C}_{\bm{u}|\bm{y}} \left(   \rho \bm{P}^T  (\bm{C}_d + \bm{C}_e)^{-1}  \bm{y}  +  \bm{C}_u^{-1}  \bar{u}   \right)
\end{equation}
and
\begin{equation}
\bm{C}_{\bm{u}|\bm{y}} = \left(      \rho^2  \bm{P}^T   (\bm{C}_d + \bm{C}_e)^{-1}  \bm{P}  +  \bm{C}_u^{-1}    \right)^{-1} \;.
\end{equation}
To avoid having to invert matrices with the size of $n_u \times n_u$, the Sherman-Morrison-Woodbury identity \cite{riedel1992} is applied \cite{girolami2021} in order to only invert matrices with the size of $n_y \times n_y$ which yields
\begin{equation}
\bm{C}_{\bm{u}|\bm{y}} = \bm{C}_u - \bm{C}_u \bm{P}^T (\frac{1}{\rho^2} (\bm{C}_d + \bm{C}_e) + \bm{P} \bm{C}_u \bm{P}^T)^{-1} \bm{P}\,\bm{C}_u \;.
\end{equation}

Equation \eqref{eqn:statFEMConditioned} can be adapted to support data with multiple observations per sensor as
\begin{equation}
\bm{\bar{u}}_{|\bm{y}} = \bm{C}_{\bm{u}|\bm{y}} \left(   \rho \bm{P}^T  (\bm{C}_d + \bm{C}_e)^{-1}  \sum_{i=1}^{n_o}\bm{y}_i  +  \bm{C}_u^{-1}  \bar{u}   \right)
\end{equation}
and
\begin{equation}
\bm{C}_{\bm{u}|\bm{y}} = \left(      \rho^2 n_o \bm{P}^T   (\bm{C}_d + \bm{C}_e)^{-1}  \bm{P}  +  \bm{C}_u^{-1}    \right)^{-1} 
\end{equation}
with $n_o$ the number of observations per sensor.


The same result can be achieved using the approach discussed in Section \ref{sec:GPcond}. The joint probability for the FEM prior and the observed data states
\begin{equation}
\begin{bmatrix}
           \bm{u}\\
          \bm{y}
         \end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}
           \bar{\bm{u}}  \\
           \bar{\bm{y}}
         \end{bmatrix}, \begin{bmatrix}
\bm{C}_u & \rho \bm{C}_u \bm{P}^T \\
\rho \bm{P} \bm{C}_u  & \rho^2 \bm{P} \bm{C}_u \bm{P}^T + \bm{C}_d + \bm{C}_e 
\end{bmatrix}  \right) = p(\bm{u},\bm{y})
\end{equation}
which can now be conditioned according to \eqref{eqn:ConditioningDistr}, which ultimately leads to \eqref{eqn:statFEMConditioned}.

\paragraph{Estimated True System Response}
Having estimated a prior solution GP, also the true system response $\bm{z}$ according to the statistical generating model \eqref{eqn:statGen} can be estimated by adding the found model discrepancy GP $\bm{d}$ to \eqref{eqn:statFEMConditioned} \cite[p. 11]{girolami2021}. Then, for the estimation there holds
\begin{equation}
p(\bm{z}|\bm{y}) = \mathcal{GP}(\rho\bm{P}\bar{\bm{u}}_{|y}, \rho^2\bm{C}_{u|y}\bm{P}^T + \bm{C}_d) \;.
\label{eqn:trueProcEstEq}
\end{equation}
Notice that $p(\bm{z}|\bm{y}) \in \mathbb{R}^{n_y}$. 


\subsection{Estimation of Hyperparameters }
\label{sec:HyperParEst}
This section follows \cite[p. 12 ff.]{girolami2021}.The hyperparameters for computing the model discrepancy covariance matrix $\bm{C_d}$ are not known beforehand and need to be estimated. The scaling factor $\rho$ is not known, as well. The measurement data can be used to estimate these parameters, collected in the hyperparameter vector $\bm{w}$. This is done using Bayes' rule again. There holds
%
\begin{equation}
p(\bm{w}|\bm{y}) = \frac{p(\bm{y}|\bm{w}) p(\bm{w})}{\int p(\bm{y}|\bm{w}) p(\bm{w}) \, \mathrm{d}\bm{w}}
\label{eqn:bayesHyperp}
\end{equation}
%
for the posterior vector of hyperparameters. Notice that since Bayes' rule is used again to estimate $\bm{w}$ instead of directly maximizing the likeliood in \eqref{eqn:bayesPrior}, Occam's razor (see Section \ref{sec:BayesInf}) comes into effect. This means that the resulting $\bm{w}$ won't yield an overfit, i.e. overly complex, model \cite{girolami2021,MacKay1991}. The posterior $p(\bm{w}|\bm{y})$ states that the vector of hyperparameters $\bm{w}$ depends on the measurements data $\bm{y}$, which makes sense because the data is meant to be used to estimate an optimal $\bm{w}$. $p(\bm{w})$ is the prior for the hyperparameters. It is visible that maximizing the likelihood $p(\bm{y}|\bm{w})$ maximizes the posterior, if the prior is uniformly distributed. This basically means finding the set of hyperparameters which are most probable to explain $\bm{y}$. Comparing the likelihood $p(\bm{y}|\bm{w})$ with Bayes' rule as it was applied for the FEM solution prior in \eqref{eqn:bayesPrior} shows that it has already been introduced before, namely as the marginal likelihood $p(\bm{y})$ in that case. It makes sense stating that $p(\bm{y}) = p(\bm{y}|\bm{w})$ because, following the statistical generating model \eqref{eqn:statGen}, $\bm{y}$ is defined using the hyperparameters and therefore its probability function is conditional on $\bm{w}$ \cite[p.12]{girolami2021}. 
The denominator in \ref{eqn:bayesHyperp}, which only serves as a normalization constant, can be dropped if only a point estimate of the hyperparameters is needed. This approach is then indeed non-Bayesian because no probability density of the hyperparameters is computed.
For multiple measurements $y_i$ with a total number of $n_o$ there holds
\begin{equation}
p(\bm{Y}|\bm{w}) = \prod_{i=1}^{n_o} p(\bm{y_i}|\bm{w}) \; 
\label{eqn:MultipleObs}
\end{equation}
which replaces $p(\bm{y}|\bm{w})$.
Equation \eqref{eqn:MultipleObs} is valid, because it is assumed that individual measurements are independent \cite[p.9]{rasmussen2006}.
If no prior knowledge of the parameters is available, it is possible and even common to use an uninformative prior $p(\bm{w}) = 1$ \cite[p.433]{kennedy2001}. 
The equation then basically states that $p(\bm{w}|\bm{Y}) \propto p(\bm{Y}|\bm{w})$ which means that maximizing  $p(\bm{Y}|\bm{w})$, i.e. the likelihood to measure $\bm{Y}$ given a set of hyperparameters, yields the optimal vector of hyperparameters for the given data. 
The marginal likelihood $p(\bm{y}|\bm{w})$, marginal because it has been marginalized over the function values $\bm{u}$ as shown in \eqref{eqn:marginGP}, can be expressed as a Gaussian normal distribution
\begin{equation}
p(\bm{y}|\bm{w}) = \mathcal{N}(\rho \bm{P}\bar{\bm{u}}, \bm{C_d}+\bm{C_e} + \rho^2 \bm{P} \bm{C_u} \bm{P}^T)
\end{equation}
because, as stated above, all components of the statistical generating model are Gaussian.
The equation for the corresponding PDF reads, in agreement with Section \ref{sec:MultiGauss},
\begin{equation}
p(\bm{y}|\bm{w}) = \frac{1}{(2 \pi)^{n/2} |\bm{K}|^{1/2} } \exp \left(   -\frac{1}{2} (\bm{y} - \rho \bm{P}\bar{\bm{u}})^T \bm{K}^{-1} (\bm{y} - \rho \bm{P}\bar{\bm{u}})   \right)
\end{equation}
with
\begin{equation}
\bm{K} = \bm{C_d}+\bm{C_e} + \rho^2 \bm{P} \bm{C_u} \bm{P}^T \; .
\end{equation}
%
To make further calculations easier the exponential function can be removed by taking the negative $\log$ of the function \cite[p.113]{rasmussen2006}:
\begin{equation}
- \log p(\bm{y}|\bm{w}) = \frac{n}{2} \log(2 \pi) + \frac{1}{2} \log |\bm{K}| + \frac{1}{2}(\rho \bm{P}\bar{\bm{u}} - \bm{y})^T \bm{K}^{-1} (\rho \bm{P}\bar{\bm{u}} - \bm{y}) 
\label{eqn:neglog}
\end{equation}
This also improves numerical stability, because products of possibly very small factors can become smaller than machine accuracy. 
Because of taking the negative $\log$, (\ref{eqn:neglog}) now needs to be minimized instead of maximized. 
The individual terms fulfill different roles: the last term which is the only one that contains $\bm{y}$ is the data fit, the middle term is the complexity penalty and the first term is a normalization constant. As \cite{rasmussen2006} shows, the complexity penalty becomes larger the smaller the characteristic length scale is and hence more complex models have a smaller probability which corresponds to Occam's razor, see Section \ref{sec:BayesInf}. \cite{rasmussen2006} also shows that the data-fit term becomes larger with an increasing length scale because a model with a large length scale is less flexible.

%\paragraph{Non-identifiability}
%priors need to be informative, p12 statFEM
%\cite{Eberly2000} MCMC can lead to wrong sets of parameters or doesn't converge \cite{Bayarri} \cite{kennedy2001}.


\paragraph{Cholesky Decompositon}
\label{sec:Cholesky}
This follows \cite[p.93-95]{chambers1998}.
For numerical stability reasons, $\bm{K}^{-1}$ should not be calculated directly. Instead, the linear system is solved. Additionally, if $\bm{K}$ is symmetric and positive-semidefinite, computing the lower triangular matrix using the Cholesky decomposition 
\begin{equation}
\bm{K} = \bm{L}\bm{L}^T
\end{equation}
makes the calculation quicker and yields the additionally required determinant of $\bm{K}$ as a by-product. 
A generic linear system is defined as $\bm{A} \bm{x} = \bm{b}$. It is solved for $\bm{x}$, so the inverse of $\bm{A}$ is needed. In this case for (\ref{eqn:neglog}) there holds $\bm{A}=\bm{K}$ and $\bm{b} = \bm{y}$, since the inverse of $\bm{K}$ is meant to be found. Defining $\bm{x} = \bm{K}^{-1}\bm{y}$ there holds
\begin{equation}
\bm{K} \bm{K}^{-1}  y = y
\end{equation}
%
With $\bm{L}$ the lower triangular matrix of $\bm{K}$ obtained by 
\begin{python}
L = scipy.linalg.cho_factor(K)
\end{python}
the linear system can now be solved for $\bm{x} = \bm{L}y$ with
\begin{python}
K_inv_y = scipy.linalg.cho_solve(L, y) .
\end{python}

The determinant of any triangular matrix is defined as the product of its diagonal entries \cite{tomaskovic-moore}. Therefore the lower triangular matrix $\bm{L}$ can be used to efficiently calculate the determinant of $\bm{K}$. There holds
\begin{equation}
\mathrm{det} (\bm{K}) = \mathrm{det} (\bm{L}) \mathrm{det} (\bm{L}^T) = \mathrm{det} (\bm{L} ^2)
\end{equation}
and therefore
\begin{equation}
\mathrm{det} (\bm{K}) = \left(    \prod_{i=1}^{n_y}  L_{i,i}  \right)^2
\label{eqn:CholDet}
\end{equation}
%
Since in (\ref{eqn:neglog}) the $\log$ likelihood is used, (\ref{eqn:CholDet}) can be simplified to a sum:
%
\begin{align*}
\log{\mathrm{det} (\bm{K})} &= \log{\mathrm{det} (\bm{L}^2)} \\
&= 2 \sum_{i=0}^{n_y} \log{L_{i,i}}
\end{align*}

\subsubsection{Minimization of the Negative log-Likelihood}
The best possible set of hyperparameters $\bm{w}$ for (\ref{eqn:neglog}) to be as small as possible has to be found, i.e. the negative log-likelihood has to be minimized in order to get a model which represents given data the best. This can be achieved by using a gradient based optimizer or by sampling the parameter space with, e.g., a MCMC approach. Both find a point estimate for the set of hyperparameters which lead to the minimal negative log-likelihood. Caution has to be taken for problems in which the negative log-likelihood yields multiple local minima \cite{Svensson2015}.  These and another, more simple, approach to find the minimum are given:
\paragraph{Random Sampling in the Parameter Space}
This method is used in this thesis because of its simplicity in implementation. For real-world applications it is not recommended since it is the slowest of all three proposed methods.
\begin{figure}[!ht]
\includegraphics[width=0.9\textwidth]{pics/RandomHyperParSearch.pdf}
\centering
\caption[Point cloud of the random search for the hyperparameters which yield the minimum negative log-likelihood]{See the \href{https://github.com/herluc/herluc.github.io/blob/main/MLL.gif}{Animation} (https://bit.ly/3nUQNvs) to get a better understanding of where the minimum lies. Sampling throughout a carefully predefined parameter space and solving for the negative log-likelihood yields a field of solutions. The set of hyperparameters is chosen which yields the smallest negative log-likelihood. The scaling factor $\rho$ is also part of the random sampling but not shown here. The found minimum is shown as a red dot.}
\label{fig:RandSampHyper}
\end{figure}
It works by sampling randomly in the hyperparameter space, solving \eqref{eqn:neglog} and keeping the hyperparameter set which yields the lowest value. The better the hyperparameter-space is chosen, i.e. the better the prior describes the best combination of parameters already, the better the approximation considering a fixed number of possible samples. For example, it makes sense to sample all parameters only for positive values,  the distance measure $l$ only for values smaller than the domain and the variance $\sigma$ only in the same order of magnitude as the FEM prior. Figure \ref{fig:RandSampHyper} gives an impression of how the samples are distributed.
The more samples are drawn, the more accurate the estimated set of hyperparameters will be.
\FloatBarrier


\paragraph{MCMC}
Markov Chain Monte Carlo (MCMC) is a very efficient sampling method. Just as standard MC, it can be used to draw samples from a given distribution. The advantage of MCMC is now that an algorithm finds a relatively small set of samples which is able to describe the distribution very accurately. The result is a distribution for the hyperparameters of which e.g. the mean can be deduced. One common algorithm is the metropolis algorithm.
The metropolis algorithm is a special case of MCMC for a symmetric proposal distribution.
According to \cite[p.31]{girolami2021} the MCMC metropolis algorithm can be implemented with the following steps:
\begin{enumerate}
\item Sample a proposal density, e.g. a normal distribution, $q(\bm{w}|\bm{w}^{(i)}) = \mathcal{N}(\bm{w}^{(i)},\sigma^2_q I)$ with $\bm{w}^{(i)}$ some initial guess and $\sigma^2_q$ a parameter which influences the acceptance ratio. It can be changed by hand to find and acceptance ratio of about $0.25$, as proposed by \cite{girolami2021}.
\item Compute the acceptance probability $\alpha = \mathrm{min}(0,\mathrm{ln}(p(\bm{w}|\bm{Y}))  - \mathrm{ln}(p(\bm{w}^{(i)}|\bm{Y})))$

\item A sample from $a \sim \mathcal{U}(0,1)$ is drawn for randomisation around the current sample
\item If $\mathrm{ln}(a)<\alpha$: $\bm{w}^{(i+1)} = \bm{w}$, else $\bm{w}^{(i+1)} = \bm{w}^{(i)}$. 
\end{enumerate}
The steps are repeated until $\bm{w}$ converges according to some measure.



\paragraph{L-BFGS}
L-BFGS is a gradient based optimizer. It doesn't deliver a distribution for the hyperparameters but only a point estimate.
For the optimization with L-BFGS the derivative of (\ref{eqn:neglog}) is needed, hence the function to be optimized needs to be differentiable. It is a very popular method in ML because, as the letter "L" suggests, it is designed for low amounts of available memory and is hence qutie efficient. Here, it will not be explained in detail but \cite{Zhu1997} gives a thorough explanation. 




\subsubsection{Other Methods to Fuse Data and FEM}
There are other methods which aim to solve a similar problem as statFEM does.  At first a standard Bayesian updating approach is explained and then the so-called Gappy POD is described.
\paragraph{Bayesian Update of FEM}
This follows \cite[p.19-21]{girolami2021}.
statFEM is not the only method to fuse data and FEM which uses a Bayesian approach. The most standard approach to update the FEM using data is by viewing the problem as a Bayesian inverse problem. Considering all parameters in the model as random variables makes it possible to form a Bayesian prior over the parameters. Observing data leads to a probability distribution for the data. Applying Bayes' rule \eqref{eqn:BayesRule} at this point leads to a posterior, i.e. an updated, probability distribution for the parameters. Often, this posterior density is analytically intractable. Hence, a lot of research is being conducted, see e.g. \cite{Straub2015}, to find efficient approximation methods to determine the probability density. Popular methods are the already mentioned MCMC, Importance Sampling MC \cite{Glynn1989}, Subset Sampling \cite{Au2001} or Polynomial Chaos methods \cite{Crestaux2009}.

As Bayesian inversion can be used for FEM, an interesting application to the Beaysian update of the parameters is to not only update scalar parameters which are defined in the underlying PDE but also parameters which are a function over the domain, as already done in statFEM. For example, take a parameter $f(x)$ in a 1D setting. It is a function over the whole domain. In the previous chapters it was explained how this function can be approximated using a GP and then be pushed forward through the PDE as a Bayesian forward problem in order to get a Bayesian prior for the FEM approximation of $u(x)$. In an inverse scenario, the prior is defined over the parameter. For a parameter as $f(x)$, which is not a scalar, it has to be discretized over the domain at first using a basis function expansion, for which there holds
\begin{equation}
f(x) \approx \sum_i^n \phi_i(x)f_i \; ,
\end{equation}
with $\phi_i(x)$ one of $n$ basis functions and $f_i$ one of $n$ coefficients. The basis functions are usually of e.g. Lagrange, B-Spline or Karhunen-Loeve type. The Bayesian prior is now formed over these coefficients which serve as free scalar parameters in the FEM setting. By introducing data $\bm{y}$, a posterior over the coefficients can be formed which can then be evaluated with the mentioned methods such as MCMC. The chosen method yields an estimation for each coefficient and the corresponding basis function expansion therefore yields an estimation for the whole parameter $f(x)$.



\paragraph{Gappy POD}
This follows \cite{Everson1995} and \cite{StevenL.Brunton2019}.
Gappy POD has a great resemblance to statFEM: It works by mapping a complete parameter space $\bm{u}$ to a reduced, hence "gappy", parameter space $\tilde{\bm{u}}$ using a projection matrix $\bm{P}$ as
\begin{equation}
\tilde{\bm{u}} = \bm{P} \bm{u}\;. 
\label{eqn:GappyPod1}
\end{equation}
The reduced space can be seen as the few sensor locations where measurement data is observed in the domain whereas the complete space can be seen as all the test points within the domain. By applying Gappy POD, the behavior in the whole domain can be estimated having only the reduced space of the data available. Gappy POD is based on the Karhunen Loeve transform. The Karhunen Loeve transform uses the eigenfunctions of a matrix as an orthogonal basis for an expansion \cite{Everson1995}.
Approximating \eqref{eqn:GappyPod1} using a POD such as the Karhunen Loeve transform approach leads to
\begin{equation}
\tilde{\bm{u}} \approx \bm{P} \sum_{k=1}^r \tilde{a}_k \bm{\psi}_k
\end{equation}
with $\tilde{a}_k$ a coefficient of the orthogonal basis function $\bm{\psi_k}$ and $r$ the number of measurements. The problem is now that each $\bm{\psi_k}$ is of order $n$ because it represents a mode of the whole space and $\tilde{\bm{u}}$ is only of order $r$. This means that the modes $\bm{\psi_k}$ are orthogonal on the whole space but need not be orthogonal over the dimensionally reduced set of points where $\tilde{\bm{u}}$ lives. This set is also called the support of $\tilde{\bm{u}}$, denoted as $s[\tilde{\bm{u}}]$. Despite the possible lack of orthogonality, the vector of all coefficients $\tilde{\bm{a}}$ can be estimated using a least squares approach by minimizing the error
\begin{equation}
E = \int_{s[\tilde{\bm{u}}]} \mathrm{d}\bm{x} \left[ \tilde{\bm{u}} - \sum_{k=1}^r \tilde{a}_k \bm{\psi}_k \right]^2	\;.
\end{equation}
This is done by enforcing orthogonality between the residual and the used orthogonal modes over $s[\tilde{\bm{u}}]$ as
\begin{equation}
\left( \tilde{\bm{u}} - \sum_{k=1}^r \tilde{a}_k \bm{\psi}_k, \bm{\psi}_j  \right)_{s[\tilde{\bm{u}}]} = 0
\end{equation}
for $k=1...n$ and $j=1...r$.
There holds for the Hermitian matrix $\bm{M}$
\begin{equation}
M_{kj} = (\bm{\psi}_k, \bm{\psi}_j )_{s[\tilde{\bm{u}}]} \neq 0
\end{equation}
and with
\begin{equation}
f_k=(\bm{u}, \bm{\psi}_k)_{s[\tilde{\bm{u}}]} \;,
\end{equation}
\begin{equation}
\bm{M}\tilde{\bm{a}} = \bm{f}
\end{equation}
can be solved for the unknown coefficients $\tilde{\bm{a}}$.
The interpolated full solution is now approximated with
\begin{equation}
\bm{u}(x) \approx \bm{\psi} \tilde{\bm{a}} \;.
\end{equation}
As this approach does something similar to statFEM, i.e. finding a new global solution estimate by only providing sparse data, this is contrary to statFEM in the fact that it is not a Bayesian approach. Gappy POD therfore doesn't require a Bayesian prior and is, in a basic setting, not inherently random but yields a deterministic result.




\chapter{A statFEM Approach for Vibroacoustics}
In the previous chapters, the theoretical basis for an application of statFEM has been built. So now at first, in order to verify the methods and the respective code implementations, a simple 1D problem of the Poisson equation \eqref{eqn:PoissonEq} is considered and solved in the statFEM framework. Although having no direct connection to acoustics, it is a very common and  basic problem for numerical methods and thus there are many resources \cite{girolami2021,langtangen2019} with which the results can be compared.

Having the methods verified, the complexity can be increased to a 2D problem where the Helmholtz equation \eqref{eqn:Helmholtz} is solved. A suitable kernel function is chosen and the boundary conditions are applied. The problem is considered to stem from the vibroacoustics field because one boundary condition resembles a vibrating plate under uncertainty and the acoustical behavior of the adjoining domain is studied.

\section{Simple 1D example}
\paragraph{Choice of PDE}
The Poisson equation 
%%
\begin{equation}
- \nabla \cdot \kappa(x) \nabla u(x) = f(x)
\label{eqn:Poisson}
\end{equation}
%%
is chosen as the governing equation. It is an elliptic PDE. In this work, it is used as a simple 1D example to illustrate how the statistical FEM and especially the GP Regression works.
The most standard form of it does not, contrary to this example, include $\kappa(x) \in \mathbb{R}^+$ which is the diffusion coefficient as a function of the spatial variable $x$. The right-hand side consists of the source term $f(x)\in \mathbb{R}$. Both $\kappa (x)$ and $f(x)$ are the free, generalized, parameters in this case. The equation is solved for the unknown  $u(x)$ in the domain $\Omega = (0,1)$ with the boundary condition $u(x) = 0$ on $x=0$ and $x=1$.
%
For a first example there holds $\mu(x)=1$. $f(x)$ is modeled as a the GP \cite{girolami2021} 
%
\begin{equation}
f(x) \sim \mathcal{GP} \left( \bar{f}(x), c_f(x,x')\right) \;.
\end{equation}
%
Pushing it forward through the FEM system of equations \eqref{eqn:meanEqSys} leads to the FEM prior.




\subsection{FEM Prior}
\paragraph{Construction of the GP}
The mean function of the GP is, in accordance with \cite{girolami2021}, set to $\bar{f}(x) = \pi^2 / 5$. For the covariance function at first a squared exponential kernel 
%
\begin{equation}
c_f(x,x') =    \sigma_{f}^2 \exp \left(-  \frac{\left \| x-x' \right \|^2}{2l_{f}^2} \right )       
\label{eqn:sqEx_f}
\end{equation}
%
is used with the standard deviation $\sigma_{f} = 0.1$ and the length scale $l_{f} = 0.4$.
In Python, the kernel is directly implemented as a function which takes two lists and the parameters as input variables \cite{murphy2012}. Figure \ref{fig:Sqexp} depicts samples from the kernel.

%
Here, the parameters are fixed but in a later example, see Section \ref{sec:HyperParEst}, a method on how to infer the optimum values for the hyperparameters according to data will be studied.

Having prepared the mean function and the kernel, which can be considered the prior in a GP regression setting, a sample of the GP can be drawn. For this, points have to be chosen on which the kernel is evaluated. These are the test points and, according to \cite{girolami2021}, correspond to the center of the FEM elements. This implies that there are as many test points as FEM cells and therefore the coordinates of the FEM mesh can directly be used to compute the covariance matrix. For this, (\ref{eqn:sqEx_f}) is evaluated at $\bm{x} = \bm{x}'$ with $\bm{x}$ the vector of test points. 

A GP has the marginalization property. If it is sampled at a finite number of points $n_p$, it yields a multivariate Gaussian distribution $\mathcal{N}(\bar{\bm{f}},\bm{C_f})$ with $\bar{\bm{f}}$ the mean vector of length $n_p$ and $\bm{C_f}$ the covariance matrix of size $(n_p \times n_p)$ while still describing the underlying continuous sample.
According to \cite[p. 201]{rasmussen2006}, sampling from a multivariate Gaussian distribution works as follows: To obtain $n_s$ samples from the prior GP, at first $n_s$ samples of a multivariate standard normal distribution, also called Gaussian white noise, $\bm{q} \sim \mathcal{N}(\bm{0},\mathbb{1})$ with $\mathbb{1}$ the identity matrix (of size $(n_p \times n_p)$, $n_p$ is the number of test points) have to be drawn. Computing the Cholesky decomposition of the covariance matrix $\bm{C_f} = \bm{L}\bm{L}^T$ yields the lower triangular matrix $\bm{L}$. For the $n_s$ samples of the prior there now holds $\bm{f} = \bar{\bm{f}} + \bm{L}\bm{q}$ which is a multivariate Gaussian distribution with a mean $\bm{f}$ and a covariance $\bm{C_f}$.

The mean, $2\sigma$ confidence bands and realisations of the FEM prior are visible in Figure \ref{fig:FEMGP1D}. As expected, the general shape of the solutions follows the smooth behaviour of the Laplacian. The variance becomes larger the further away from the Dirichlet zero-boundary conditions. For the FEM, $32$ elements and linear Lagrange ansatz functions were used.

\begin{figure}[H]
\includegraphics[width=0.8\textwidth]{pics/FEMprior_Z.pdf}
\centering
\caption[FEM prior and ground truth GPs for the 1D example]{FEM Prior and ground truth for the observations for the 1D example. The mean and confidence bands as well as $n_s = 5$ samples drawn from the GPs are shown. As the boundary conditions imply, both mean and variance are zero on the boundary of the domain.}
\label{fig:FEMGP1D}
\end{figure}






\subsection{Posterior}
To form a posterior, observations have to be taken on which the prior is conditioned.
For the observations it is assumed that there is some measurement noise. This is modeled as an added variance $\sigma_{n}^2$ on the diagonal of the prior covariance matrix. An additional effect of this added noise is the improved numerical stability of the covariance matrix which is important for computing the Cholesky decomposition (Section \ref{sec:Cholesky}) \cite[p.14]{girolami2021}.

The observations $\bm{y} = \bm{z} +\bm{e}$ have been taken on a ground truth which is based on samples of a GP 
\begin{equation}
\bm{z} \sim \mathcal{GP}(\bar{\bm{z}},\bm{A}^{-1}c_z(x,x')\bm{A}^{-T})
\label{eqn:zGP}
\end{equation}
which uses the vector of evaluations of the harmonic function
\begin{equation}
\bar{z}(x) = 0.2\sin(\pi x) + 0.02\sin(\pi x)
\end{equation}
at the test points as a mean and a Mat\'ern kernel with the hyperparameters $l_z=0.5$ and $\sigma_z = 0.15$ as covariance function. $\bm{A}$ is the FEM system matrix \eqref{eqn:SysMatAssem}, hence the covariance matrix is formed in a similar way as e.g. in \eqref{eqn:Covarf} to make sure that the variance approaches zero at the boundaries and is highest in the center of the domain. Therefore, for the observations GP there holds
\begin{equation}
\bm{y} \sim \mathcal{GP}(\bar{\bm{z}},\bm{A}^{-1}c_z(x,x')\bm{A}^{-T} + \bm{e})
\end{equation}
with $\bm{e}$ the measurement noise GP, see \eqref{eqn:GeneratingNoise}.
Figure \ref{fig:FEMGP1D} shows that the mean and therefore also the variance band is, because of the harmonic mean, not as smooth as the prior. A model discrepancy error is present. 

Now, data is introduced and the posterior can be generated.



\FloatBarrier
\paragraph{Posterior for Observations with Model Error}
For the posterior, at first, one single sensor location is introduced at which one single measurement is taken. The point is picked arbitrarily and a measurement noise is introduced. Already it is visible in Figure \ref{fig:ModelError1Da} that the general shape of the FEM prior remains, but it is scaled to the data. Also, the variance becomes smaller than for the prior as there is now evidence on the true solution.
%
\begin{figure}[!ht]

\centering

	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/Model_Error/1o_1s/Result.pdf}
	\caption{One observation at one sensor location.\\
	The shape of FEM Prior remains but scaling and variance differ.}	
	\label{fig:ModelError1Da}
	\end{subfigure}%
	%\hfill
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/Model_Error/1o_4s/Result.pdf}
	\caption{Per observation point 1 individual observation was taken with a standard deviation of $2.5e-5$. }	
	\label{fig:ModelError1Db}
	\end{subfigure}
	
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/Model_Error/20o_4s/Result.pdf}
	\caption{Per observation point 20 individual \\
	observations were taken with a standard deviation of $2.5e-5$. }	
	\label{fig:ModelError1Dc}
	\end{subfigure}%
	%\hfill
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/Model_Error/100o_30s/Result.pdf}
	\caption{Per observation point 100 individual observations were taken with a standard deviation of $2.5e-5$. }	
	\label{fig:ModelError1Dd}
	\end{subfigure}

\caption[The posterior for the 1D example after introducing observations with a model error]{The posterior for different measurement scenarios. The more sensor locations are introduced and the more observations are taken, the smaller the variance gets and the closer the posterior mean comes to the mean of the ground truth. Each red dot rerpesents a single observation.}	
	
\label{fig:ModelError1D}
\end{figure}
%
Figure \ref{fig:NoModelError1D} shows the posterior for observations taken on a sample of the FEM prior. In this example multiple measurement locations were chosen and one single observation has been taken per point. The variance drops significantly and the solution is scaled to fit the data already for few observations.  As a direct comparison, in \ref{fig:ModelError1D} much more measurement noise is added which leads to a higher variance in the posterior. The variance could be decreased by taking more observations per sensor.

%If now the observations are taken from a sample which does not resemble the overall shape of the FEM prior as closely, model error is introduced, see Figures \ref{fig:ModelError1Db} - \ref{fig:ModelError1Dd}. Increasing the number of sensors $n_p$ and/or the number of observations $n_o$ leads to both a smaller variance in the posterior and to a mean which stongly deviates from the shape of the mean of the prior. The mean then follows more and more the mean of the ground truth $\bar{\bm{z}}$, as it is visible in Figures \ref{fig:ModelError1Dd} and \ref{fig:1DOneSidedb}.

Figure \ref{fig:ModelError1D} shows how the posterior is formed for a ground truth with model error, according to (\ref{eqn:zGP}). If only few observations are taken on few sensor locations, as in \ref{fig:ModelError1Da} and \ref{fig:ModelError1Db}, the FEM prior shape is retained and the variance doesn't reduce dramatically. As soon as more observations are taken, as in \ref{fig:ModelError1Dc}, the variance decreases much stronger. It is only until enough sensor locations are introduced on which many observations are taken, as in \ref{fig:ModelError1Dd}, that the mean of the ground truth is recognizable. The more data is introduced, the less weight is on the FEM prior and rather the underlying function of the data is follwed than the mean of the prior. Also, at that point, the spatial distribution of the sensors is finer than the feature size of the ground truth's mean, i.e. the roughness of the function: The mean follows a harmonic function and according to the Nyquist Shannon sampling theorem, a certain number of points is necessary per wavelength to resolve it \cite{Shannon1949,Marburga}. The mean then follows more and more the mean of the ground truth $\bar{\bm{z}}$, as it is visible in Figures \ref{fig:ModelError1Dd} and also \ref{fig:1DOneSidedb}. 
\begin{figure}[!ht]

	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/no_d_scaled_40proc.pdf}
	\caption{Observing data from a scaled sample of the\\
	 FEM prior leads to a very small variance.}
		\label{fig:NoModelError1D}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/d_scaled40proc.pdf}
\centering
\caption{Introducing significant measurement noise to the observations leads to a higher variance. It could be reduced by using more observations per sensor location.}
\label{fig:ModelError1D2}
	\end{subfigure}

\caption{Comparison of the influence of different amounts of model error on the posterior variance. Each dot represents a single observation.}
\label{fig:ModErrNoModErr1D}
\end{figure}
%

In Table \ref{tab:HyperPar1DModelError} the corresponding hyperparameters to Figure \ref{fig:ModelError1D} can be found. It is visible that the model error hyperparameter $\sigma_d$ becomes significantly higher after more than one sensor is introduced. The reason for this is that with only one sensor, any observation on the ground truth with model error could as well be an observation on some sample from the FEM prior, which, of course, wouldn't have any model error involved. As soon as more sensors are introduced, the method can't assign the posterior solely to the prior shape anymore what leads to an increasing model error.


\begin{table}[]
\centering
\caption{The estimated hyperparameters for the different observations with model error. The more observations are taken, the higher the estimated model error becomes.}
\label{tab:HyperPar1DModelError}
\begin{tabular}{@{}ccccc@{}}
\toprule
           & $n_s = 1, n_o = 1$ & $n_s = 4, n_o = 1$ & $n_s = 4, n_o = 20$ & $n_s = 30, n_o = 100$ \\ \midrule
$\rho$     & 0.876              & 0.742              & 0.807               & 0.739                 \\
$\sigma_d$ & 0.00036            & 0.00877            & 0.01021             & 0.00994               \\
$l_d$      & 0.251              & 0.045              & 0.217               & 0.743                 \\ \bottomrule
\end{tabular}
\end{table}
%










\FloatBarrier
\paragraph{Inferred True System Response}
As visible in Figure \ref{fig:Trueproc1D}, adding the model inadequacy GP $\bm{d}$ to the posterior solution GP according to \eqref{eqn:trueProcEstEq} leads to an estimation for the true underlying process \eqref{eqn:zGP}.
The variance did not converge to zero as it does for the posterior solution GP. Also, then variance is not lower on the boundaries compared to the rest of the domain. As \cite{girolami2021} explains, the reason for this is that $\sigma_d$ is considered as a constant throughout the domain. Using a kernel with $\sigma_d(x)$ could solve that issue. This is a good example for the influence of the chosen kernel function on the results and that they do serve as a way to incorporate prior knowledge.
\begin{figure}[!ht]
\includegraphics[width=0.5\textwidth]{../../Python/Results/1D/Model_Error/100o_30s/ResultZ.pdf}
\centering
\caption[Estimating the true process for the 1D example]{Considering the estimate for the true underlying process, the variance doesn't converge to zero if a model error is involved. Instead, it is taking a constant value in the magnitude of the true process. The BCs are not fulfilled for the variance because of the chosen Mat\'ern kernel which only features a constant variance term.}
\label{fig:Trueproc1D}
\end{figure}




\FloatBarrier
\paragraph{Spatially Bounded Sensor Locations}
\label{sec:spatBound}
Figure \ref{fig:1DOneSided} shows the formed posterior if data is observed only on one side of the domain. The question to be answered is if the posterior for rest of the domain will also yield correctly scaled results. At least for data without model error, this is the case. For even few observations the posterior is scaled according to the data throughout the domain while maintaining the overall shape of the FEM prior. It is visible that in regions of the domain where no observations have been taken the variance is higher. For observations with model error, on the other hand, the solution only scales correctly for rather small data sets. As more data is introduced, the mean of the ground truth is matched and the variance decreases in the region with data. In the rest of the domain, ther variance remains higher and the mean neither follows the prior nor the ground truth. %This happens because of the correlation requirements for the posterior GP: The approximated posterior is the simplest model to be 
%
\begin{figure}[!ht]

	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/HalfSide/Model_Error/5o_15s/Result.pdf}
	\caption{With model error. $n_p=15$, $n_o=5$}
		\label{fig:1DOneSideda}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/HalfSide/Model_Error/100o_15s/Result.pdf}
	\centering
	\caption{With model error. ${n_p=15}$, ${n_o=100}$ }
	\label{fig:1DOneSidedb}
	\end{subfigure}
	
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/HalfSide/No_Model_Error/5o_15s/Result.pdf}
	\caption{Without model error, only 0.75x\\
	 scaling. ${n_p=15}$, ${n_o=5}$}
		\label{fig:1DOneSidedc}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/HalfSide/No_Model_Error/100o_15s/Result.pdf}
	%\centering
	\caption{Without model error, only 0.75x scaling. ${n_p=15}$, ${n_o=100}$}
	\label{fig:1DOneSidedd}
	\end{subfigure}

\caption[Posterior of the 1D example for observations only on one side of the domain]{If data is measured only on one side of the domain, the overall shape of the FEM prior remains but the variance changes according to the presence of data in a region of the domain. If no model error is introduced but observations are only taken from a scaled FEM prior sample, the variance quickly reduced as soon as data is introduced. If there is a significant model error, more observations are needed to reduce variance and to match the mean of the data. Also, in the case of observations with model error, the posterior mean on the part without data does not follow the prior anymore. Each red dot represents a single observation.}
\label{fig:1DOneSided}
\end{figure}
%

Table \ref{tab:HyperPar1DSpatBound} shows the corresponding hyperparameters. The model error scaling hyperparameter $\sigma_d$ is correctly estimated higher in the first two examples as for the ones without introduced model error. 
\begin{table}[!ht]
\centering
\caption{The estimated hyperparameters for the different observations with spatially bounded sensor locations. }
\label{tab:HyperPar1DSpatBound}
\begin{tabular}{@{}ccccc@{}}
\toprule
           & (a) $n_s = 15, n_o = 5$ & (b) $n_s = 15, n_o = 100$ & (c) $n_s = 15, n_o = 5$ & (d) $n_s = 15, n_o = 100$ \\ \midrule
$\rho$     & 0.750             & 0.749              & 0.726               & 0.730                \\
$\sigma_d$ & 0.01196            & 0.00955            & 0.00039             & 0.00035               \\
$l_d$      & 0.0077              & 0.4569              & 0.0157               & 0.0075                 \\ \bottomrule
\end{tabular}
\end{table}


\FloatBarrier




\paragraph{Prior Data Conflict}
According to \cite{Evans2006}, a prior-data conflict is present if most of the probability mass of a prior lies in a region where no data is observed. The conflict can be resolved by observing more data.
Figure \ref{fig:1DConflict} shows the inferred solution in the case of the data being measured as far out of the FEM prior confidence bands. This new data has been generated by scaling the previously used data, see \eqref{eqn:zGP}, i.e. $\bm{\bar{y}}_{scaled} = 3 \bar{\bm{z}}$. A prior-data-conflict is visible in Figure \ref{fig:1DConflicta}: The solution isn't scaled correctly according to either data or FEM prior. The data lies way outside the possible FEM solutions. Observing more data eventually moves the curve further in direction of the data because the FEM prior looses importance as the data gains more weight, see Figure \ref{fig:1DConflictb}. However, this does, for this problem, only hold true for observations which include a model error. As visible in Figure \ref{fig:1DConflictc}, if no model error is introduced and the data is observed on a scaled FEM prior sample, the correct scaling can be found for even a single observation per sensor location. These findings are in agreement with how \cite{Evans2006} defines a prior-data conflict. %
%
\begin{figure}[!ht]
\centering
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/PriorDataConflict/Model_Error/30s_5o/Result.pdf}
	\caption{With model error. $n_p=30$, $n_o=5$}
		\label{fig:1DConflicta}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/PriorDataConflict/Model_Error/30s_100o/Result.pdf}
\centering
\caption{With model error. $n_p=30$, $n_o=100$ }
\label{fig:1DConflictb}
	\end{subfigure}
	
	\begin{subfigure}[t]{.5\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{../../Python/Results/1D/PriorDataConflict/No_Model_Error/Result.pdf}
	\caption{Without model error. $n_p=30$, $n_o=1$}
		\label{fig:1DConflictc}
	\end{subfigure}
	

\caption[Posterior of the 1D example for observations far outside the FEM prior variance band]{If data is observed far outside the variance bands of the FEM prior, a higher number of observations is needed to scale the posterior correctly and to match the data accurately. If the data are observed on a ground truth without any model error, this does not hold: Even with only 1 observation per sensor location the posterior scales correctly. Each red dot represents a single observation.}
\label{fig:1DConflict}
\end{figure}
%

Table \ref{tab:HyperParConflict1D} shows the corresponding hyperparameters. It is visible that in case (a) and (b) a very high value for $\sigma_d$ is estimated. This makes sense because the model on which observations are taken is far outside the FEM prior confidence bands and hence there is a huge error involved which gets bigger the more observations are taken and the posterior mean comes closer to the data mean.
	

	
\begin{table}[!ht]
\centering
\caption{The estimated hyperparameters for the different observations far outside the FEM prior confidence interval. }
\label{tab:HyperParConflict1D}
\begin{tabular}{cccc}
\hline
           & (a) $n_s = 30, n_o = 5$ & (b) $n_s = 30, n_o = 100$ & (c) $n_s = 30, n_o = 1$ \\ \hline
$\rho$     & 2.717                   & 2.718                     & 2.718                   \\
$\sigma_d$ & 0.134                   & 0.380                     & 0.006                   \\
$l_d$      & 0.252                   & 0.176                     & 0.027                   \\ \hline
\end{tabular}
\end{table}
	


\FloatBarrier
\paragraph{Handling of Ill-conditioned Matrices}
Table \ref{tab:Conditioning} gives insight on the condition numbers of $\bm{C}_u$ for different kernel functions and different numerical nuggets $\sigma_N$ added to $\bm{C}_u$. It is visible that the squared exponential kernel yields very high condition numbers, even if a nugget us used. Therefore, in the following 2D example only Mat\'ern kernel functions with $\nu=3/2$, see \eqref{eqn:MaternKernel}, are used which appears to be a good compromise between conditioning and smoothness.
\begin{table}[!ht]
\centering
\caption{Adding a small nugget to the matrix which is inverted leads to a much lower condition number, which is good. Also, not using a squared exponential kernel but any Mat\'ern kernel leads to much lower condition numbers.}
\label{tab:Conditioning}
\begin{tabular}{@{}ccccc@{}}
\toprule
                                  & \multicolumn{1}{l}{Sq. Exponential} & Mat\'ern, $\nu = 1/2$ & Mat\'ern, $\nu = 3/2$ & Mat\'ern, $\nu = 5/2$ \\ \midrule
$\sigma_N = 0$                    & $4.89\text{e}18$                        & $7.51\text{e}7$                       & $8.46\text{e}9$                       & $4.99\text{e}11$                      \\
$\sigma_N = 1\text{e-}10\sigma_f$ & $2.09\text{e}9$                         & $7.16\text{e}7$                       & $1.54\text{e}9$                       & $1.95\text{e}9$                       \\ \bottomrule
\end{tabular}
\end{table}
\FloatBarrier

Many aspects of the fusion of an FEM prior with data using statFEM have been explained. Now, the methods are applied to a 2D Helmholtz example.


\section{2D Vibroacoustics Example}
Based on the knowledge of the methods gained in the simple 1D example, the methods are now applied on a 2D vibroacoustics problem. At first, the FEM prior has to be constructed. This is done by introducing the Neumann boundary \eqref{eqn:NeumannBCDef} on the left side of the domain as a GP. Restating it, there holds for the Neumann BC
\begin{equation}
\pdv{p(x)}{n} = \rho \omega^2 \bm{U}(x) \cdot \vec{n} \quad \forall x \in \Gamma_N \;,
\end{equation}
with the GP in $\bm{U}(x)$.
Afterwards, synthetic observation data can be generated and the prior is conditioned on the data in order to form the posterior.

\paragraph{Generating Fake Data}
To generate a posterior, the prior needs to be conditioned on measured data. In this thesis no real measurements have been taken. Instead, fake measurement data is created. One advantage of using fake measurements at first is that the ground truth is very well known: Any number of measurements can be taken at any location in the domain, i.e. there is a whole GP available for the ground truth with which the posterior can be compared.
That ground truth can be created in different ways. Here, it is based on the FEM prior: The prior is modified using some predefined parameters which are then to be estimated by statFEM. Scaling the prior GP by a certain factor $\rho$ leads to a new GP without any model error. Superimposing the prior with a function or another GP leads to a new GP with some model error.

From the generated ground truth, observations have to be taken. Here, two methods have been applied to do so: The first one is to choose the sensor locations randomly in the domain. Another method is an "active learning" approach: The first sensor location is chosen randomly. After calculating the posterior GP, the next sensor location is placed at the point of the highest variance. This should lead to a quicker convergence to a low level of variance.


\paragraph{FEM setup}
Table \ref{tab:FEMSetup} shows the parameters which are used to setup the FEM mesh.  
\begin{table}[]
\centering
\caption{The parameters which are used to setup the 2D FEM mesh.}
\label{tab:FEMSetup}
\begin{tabular}{@{}ccccc@{}}
\toprule
$n_a$ & $n_b$ & basis function type & basis function degree & domain               \\ \midrule
24    & 24    & Lagrange            & 1                     & square, $1\times 1$m \\ \bottomrule
\end{tabular}
\end{table}
The material parameters are defined in Table \ref{tab:MatSetup}. The frequency was varied in one chapter but kept constant at $500\,\mathrm{Hz}$ in most parts of the thesis.
\begin{table}[]
\centering
\caption{The material and frequency constants in the PDE have been set to these values. The material inside the Helmholtz domain is considered to be air.}
\label{tab:MatSetup}
\begin{tabular}{@{}ccc@{}}
\toprule
$\rho_0${[}$\mathrm{kg/m^3}${]} & $c${[}m/s{]} & $f${[}Hz{]} \\ \midrule
1.2                   & 340          & 500         \\ \bottomrule
\end{tabular}
\end{table}
The numbers of elements in each direction $n_a$ and $n_b$ have been chosen to be $24\times 24$ because the used workstation was only able to compute the problem up to this amount of elements in an acceptable time. As a rule of thumb, 6 nodes per wave length $\lambda$ are usually used to be able to properly resolve the solution \cite{Marburga}. With an edge length of the domain of $1 \, \mathrm{m}$, 24 nodes can resolve 4 wave lengths of $0.25\, \mathrm{m}$ each. Therefore, for the maximum accurately resolved frequency there holds 
\begin{equation}
f = \frac{c}{\lambda} = \frac{340\,\mathrm{m/s}}{0.25 \, \mathrm{m}} = 1360 \, \mathrm{Hz} \;,
\end{equation}
hence all examples in this thesis are going to be solved for frequencies less than $1360\, \mathrm{Hz}$.
The final mesh can be seen in Figure \ref{fig:meshFEM}. For most examples, $500 \,\mathrm{Hz}$ is chosen as a frequency so there is no problem considering the resolution. Hence, the mesh is considered fine enough.
\begin{figure}[!ht]
\includegraphics[width=0.5\textwidth]{pics/mesh.pdf}
\centering
\caption[Mesh of the 2D example]{The chosen mesh consists of 24 elements per dimension. Linear basis functions are chosen which results in three nodes per element.}
\label{fig:meshFEM}
\end{figure}


\paragraph{Experimental Design}
The following numerical experiments are conducted and the results are discussed in the next chapters.

\begin{itemize}
  \item Formation of a FEM prior GP, approximation of variance and mean function
  \item Conditioning of the FEM prior on 2 sensor locations with 10 observations taken from an FEM prior sample. Expected outcome: Variance drop in the region around the sensor, mean function remains unchanged, model discrepancy term $\bm{d}$ will be very low and is hence insignificant.
  \item Increase of the number of sensor locations and observations: A further variance drop is expected.
  \item Same as before but measurements taken on a 75 \% scaling of the FEM prior sample. Expected outcome: Scaling factor of $\rho = 0.75$ should be found, model discrepancy term $\bm{d}$ stays insignificant.
  \item Alteration of the FEM prior sample in a way that a significant model inadequacy is expected. Expected outcome: Increased values for $\sigma_d$ and $l_d$ will be observed.
  \item Taking observations far beyond the FEM prior variance. An increased variance and a wrong scaling are expected.
  \item Observation only on a fraction of the domain. The overall shape should still follow the FEM prior. Variance drop in the region with measurements is expected but not in the other parts of the domain.
  \item A constant Neumann source term is chosen: The problem should reduce to an 1D equivalent.

\end{itemize}




At first, the FEM prior has to be generated.

\subsection{FEM prior}
\label{sec:2DFEMprior}
Figure \ref{fig:NeumannBC} shows the GP which is used to model the $\bar{U}$ variable in the Neumann source BC. The general Neumann boundary was explained and defined in Section \ref{sec:HelmholtzFormulations}, see \eqref{eqn:BCsHelmholtz}. For the Neumann term there holds
\begin{equation}
\int_{\Gamma_N} v (\rho \omega^2 \bm{U} \cdot \vec{n}) \;,
\end{equation}
with $\bm{U}$ the displacement. Here there holds $\bm{U} = \bm{U}(y)$. It is defined as a diplacement function which is modeled as a GP.
Figure \ref{fig:FEMGP} shows how the complete FEM prior is formed. On the left face of the domain, the source term GP is applied. The mean and confidence bands are visible in red. Exciting the domain with the source leads to an uncertain FEM solution in the 2D domain. At $y_c = 0.7$, an arbitrary value, the domain is cut and the mean and error bands for that position are displayed in green. Clearly, uncertainty in the source leads to uncertainty in the FEM prior. A MC simulation for the variance converges to the calculated variance.
The mean function which is evaluated at the test points to form the mean vector of the source GP reads as
\begin{equation}
\bar{U}(y) = 0.00001 \sin(\pi y) + 0.000005 \sin(2\pi y) \;,
\end{equation}
and the Mat\'ern kernel with $\nu = 5/2$, see \eqref{eqn:Materns3}, is evaluated with $l = \mathrm{ln}(0.8)$ and $\sigma = \mathrm{ln}(9)$ to form $\bm{C}_U$, so there holds
\begin{equation}
\bm{U} \sim \mathcal{GP}(\bar{\bm{U}},\bm{C}_U) \;.
\end{equation}

\begin{figure}[!ht]
\includegraphics[width=0.7\textwidth]{pics/NeumannBC.pdf}
\centering
\caption[Neumann source GP]{The Neumann BC is modeled as a GP with a harmonic mean and a Mat\'ern kernel. The mean, the $2 \sigma$ confidence band and some drawn samples are shown.}
\label{fig:NeumannBC}
\end{figure}
%
\begin{figure}[!ht]
\includegraphics[width=0.85\textwidth]{pics/SolutionCustom2D.pdf}
\centering
\caption[Overview of the source and prior GP]{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for $222\, \mathrm{Hz}$. The variance around the source leads to a variance in the pressure field which is the lowest where the mean absolute pressure is the lowest.}
\label{fig:FEMGP}
\end{figure}
%
Figure \ref{fig:varFieldPrior} shows the FEM prior variance for every point in the 2D solution space for $222\, \mathrm{Hz}$. It is visible that the variance in some regions is higher than in others. The regions of low variance correlate with the troughs of the mode shape.
%
\begin{figure}[!ht]
\includegraphics[width=0.7\textwidth]{pics/VarField.pdf}
\centering
\caption[Variance of the FEM prior at 500Hz]{The variance of the FEM prior for $500Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible: Regions with high absolute pressure peaks also have a high variance. This implies that the positions of the nodal points in the mode shape are approximated very accurately.}
\label{fig:varFieldPrior}
\end{figure}
%
%Figure \ref{fig:FEMGP320} shows again an overview for the FEM prior, but for $320 Hz$. 
%
%\begin{figure}[!ht]
%\includegraphics[width=0.7\textwidth]{../../Python/SolutionCustom320Hz.pdf}
%\centering
%\caption{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 320Hz.}
%\label{fig:FEMGP320}
%\end{figure}
%
%In Figure \ref{fig:varFieldPrior320} it is visible that the variance pattern changed to a %periodic one.
%\begin{figure}[!ht][!ht]
%\includegraphics[width=0.6\textwidth]{../../Python/VarField320Hz.pdf}
%\centering
%\caption{The variance of the FEM prior for $320Hz$ at all points in the 2D space. Regions %with high and low variance are clearly visible.}
%label{fig:varFieldPrior320}
%\end{figure}
%
\FloatBarrier

\subsection{Posterior, observations on prior sample}
For the posterior, data has to be generated or measured on which the FEM prior can be conditioned. For that, a ground truth has to be created: In the simplest case it is a sample from the FEM prior. The next step is scaling the sample. Then, to make the problem more complex, the prior sample is changed in a way that it does not resemble the prior GP anymore. This means that a model error is introduced.
\paragraph{Hypothesis}
As stated and described in \cite{rasmussen2006}, the variance is supposed to become lower with increasing the number of sensor locations and/or observations. It is expected that this behavior also holds for the Helmholtz problem. Also, it is expected that the mean of the posterior GP gets closer to the observations when more data are introduced. For both observations with and without artificially introduced model error, statFEM is expected to generate a suitable posterior which modifies the FEM prior to account for the data. The generated hyperparameters for the posterior GP are expected to reflect the scaling and/or model error.


\paragraph{Without Model Error}
As a first example, a sample of the FEM prior is used as a ground truth from which observations are made. It is basically the "true solution" which is measured. A random measurement error of $e \sim \mathcal{U}(\num{-2.5e-3}, \num{2.5e-3})$ is added to the artificial measurement values. At first only two sensor locations are chosen and ten measurements are taken. Then it is shown that introducing more sensor locations and measurements leads to an overall smaller variance. The sensor locations are chosen randomly. It is expected that the scaling hyperparameter $\rho$ is approximated as close to $1$ whereas $\sigma_d$ becomes very small because measuring from the FEM prior means measuring from a possible solution of the prior and therefore no model error is present. Figure \ref{fig:100proc_no_d} shows that this holds approximately true. %A $\sigma_d$  of $0.1010101010?!$ was determined.

In Figure \ref{fig:100proc_no_d} the variance field in the calculation domain is shown for different combinations of the number of sensor locations $n_p$ and the number of observations $n_o$. It is visible that the variance gets lower the more sensor locations are introduced and the more observations are made. For easier comparison, the $l_2$ norm for the whole domain is given in each plot.
\begin{figure}[!ht]
\includegraphics[width=.9\textwidth]{../../Python/Results/2D/100procent_no_d/VarField_Posterior.pdf}
\centering
\caption[Posterior of the 2D Helmholtz equation example for different numbers of sensors and observations]{The variance of the posterior for $500Hz$ at all points in the 2D space for different combinations of sensor and observation numbers. Regions with high and low variance are clearly visible. Where sensor locations are introduced, the variance gets smaller in patterns around the location all over the domain. Adding more sensor locations induces a thinner mesh of patterns. The more observations are taken per sensor, the smaller the overall variance becomes, which reflects in the specified l2 norm for it.}
\label{fig:100proc_no_d}
\end{figure}
Also, it is discernible that where a sensor location is introduced, the variance does not only get lower in the immediate surrounding of the sensor but rather in patterns which have their origin at the sensor location. The more sensors are introduced, the more complicated the pattern becomes until no pattern is visible anymore. Even with large $n_p$ and $n_o$, there is still the variance of the Neumann boundary visible in the left side of the domain.
Table \ref{tab:rho100p_nod} shows the approximated values for $\rho$ for each case. It is always close to $1$. The small difference is probably due to the introduced measurement noise. The values for $\sigma_d$ are always very small which corresponds to the fact that no model error was introduced.

\begin{table}[]
\centering\caption{For a $100\%$ scaling and no introduced model error, $\rho$ is close to $1$ for every case, which is expected for a direct FEM prior sample as ground truth. The model error parameters are close to zero which is expected because no model error was introduced.}
\label{tab:rho100p_nod}
\begin{tabular}{@{}cccc@{}}
\toprule
  & $n_o = 10$ & $n_o = 100$ & $n_o = 1000$ \\ \midrule
$n_p = 2$           & $\rho=1.013$, $\sigma_d=0.016$      & $\rho=1.022$, $\sigma_d=0.011$      & $\rho=1.001$, $\sigma_d=0.034$         \\
$n_p = 4$           & $\rho=0.989$, $\sigma_d=0.019$      & $\rho=0.977$, $\sigma_d=0.022$      & $\rho=0.974$, $\sigma_d=0.007$        \\
$n_p = 6$           & $\rho=0.963$, $\sigma_d=0.008$     & $\rho=0.974$, $\sigma_d=0.008$      & $\rho=0.960$, $\sigma_d=0.008$        \\ \bottomrule
\end{tabular}
\end{table}
%
\FloatBarrier


\subsection{Posterior, observations on scaled prior sample}
\label{sec:scaled2D}
Scaling the FEM prior sample by $75\%$ should lead to a scaling factor of $\rho = 0.75$. Figure \ref{fig:75proc_no_d_overview} shows that this is approximately the case.
For a $75\%$ scaling of an FEM prior realization as ground truth, the scaling parameter $\rho$ is correctly approximated to be close to $0.75$ in every case, see Table \ref{tab:rho75p_nod}. Again, the model error is very low due to the low $\sigma_d$, which is expected.
\begin{table}[]
\centering
\caption{For a $75\%$ scaling and no introduced model error, $\rho$ is close to $0.75$ for every case, which is expected for a $75 \%$ scales FEM prior sample as ground truth. The model error is close to zero.}
\label{tab:rho75p_nod}
\begin{tabular}{@{}cccc@{}}
\toprule
  & $n_o = 10$ & $n_o = 100$ & $n_o = 1000$ \\ \midrule
$n_p = 2$           & $\rho=0.741$, $\sigma_d=0.011$      & $\rho=0.746$, $\sigma_d=0.010$      & $\rho=0.735$, $\sigma_d=0.007$         \\
$n_p = 4$           & $\rho=0.750$, $\sigma_d=0.011$      & $\rho=0.753$, $\sigma_d=0.010$      & $\rho=0.739$, $\sigma_d=0.025$        \\
$n_p = 6$           & $\rho=0.764$, $\sigma_d=0.012$     & $\rho=0.762$, $\sigma_d=0.010$      & $\rho=0.764$, $\sigma_d=0.007$        \\ \bottomrule
\end{tabular}
\end{table}
%

\begin{figure}[!ht]
\includegraphics[width=0.8\textwidth]{../../Python/Results/2D/75procent_no_d/SolutionCustomPosterior.pdf}
\centering
\caption[Source, prior and posterior for the 2D example at 500Hz]{The variance of the posterior for $500\, \mathrm{Hz}$ at all points in the 2D space for different combinations of sensor and observation numbers. Sampled from a $75\%$ scaling. Regions with high and low variance are clearly visible.}
\label{fig:75proc_no_d_overview}
\end{figure}


The 2D field shows the mean posterior whereas the green curve describes a cut through the FEM prior field which gives insight on the variance of the prior. In black, a cut through the posterior mean is shown. The $75\%$ scaling is visible.
%
Figure \ref{fig:75proc_no_d} shows the variance fields for the posterior when taking measurements from the $75\%$ scaling. Still, the variance gets lower with higher $n_p$ and $n_o$.
%
\begin{figure}[!ht]
\includegraphics[width=.9\textwidth]{../../Python/Results/2D/75procent_no_d/VarField_Posterior.pdf}
\centering
\caption[Posterior of the 2D Helmholtz equation example for different numbers of sensors and observations for a 0.75x scaled prior as ground truth]{The variance of the posterior for $500\, \mathrm{Hz}$ at all points in the 2D space for different combinations of sensor and observation numbers. Sampled from a $75\%$ scaling. Regions with high and low variance are clearly visible. Although the observations were taken from the scaled sample, the variance quickly gets smaller with more sensors and observations.}
\label{fig:75proc_no_d}
\end{figure}
%
Figure \ref{fig:75proc_no_d3d} shows ten sampled points in space and the FEM prior on the left as well as the corresponding posterior on the right. It is visible that the posterior matches the data exactly.
%
\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{../../Python/Results/2D/75procent_no_d/3dMSE.pdf}
\centering
\caption[Comparison of prior and posterior in a 3D view for the 0.75x scaling]{See the \href{https://github.com/herluc/Masterarbeit/blob/master/MA_LucasHermann/Python/Results/2D/75procent_no_d/3DMSE.gif}{Animation} (https://bit.ly/3m4D2b4) to see the differences better. With observations on a 75\% scaling of the FEM prior and no other introduced model error, the posterior is able to match the data with an MSE of $0.0$. }
\label{fig:75proc_no_d3d}
\end{figure}

\FloatBarrier

\subsection{Posterior, observations on altered prior sample}
\label{sec:PostModErr}
In a second example, the ground truth to measure from was again the FEM prior but with certain changes. In a certain region of the domain the values of the FEM prior sample were changed according to a 2D Gaussian, resulting in a clearly visible deviation. That deviation should lead to a larger model inadequacy term whereas the scaling factor should stay roughly the same. Figure \ref{fig:ObsPat} shows how observations with model error are created. At first, a rescaling pattern is created. It comprises values of around one and is then multiplied with a sample from the FEM prior. This new, altered, sample is now used to make observations on. These observations differ from the standard prior sample by the amount specified in the rescaling pattern. Note that if the prior would only have been rescaled by a constant factor throughout the domain, no model error would have been introduced: statFEM would yield a fitting scaling parameter $\rho$ and model inadequacy hyperparameters of close to zero. Here, it is forced to work with all three hyperparameters since a simple constant rescaling would not be enough to explain the data. The non-constant rescaling follows
\begin{equation}
\bm{K}_{res} \odot \bm{K}_{samp} = \bm{K}_{obs} \;,
\end{equation}
where $K_{res}$ is the matrix which contains the values of the rescaling pattern and $K_{samp}$ is a sample drawn from the FEM prior GP. An element-wise multiplication yields $K_{obs}$, the resulting matrix to draw samples from.
\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{pics/ObsPattern.pdf}
\centering
\caption[Illustration of the ground truth with model error]{Multiplying a prior sample with a rescaling pattern leads to a new ground truth with model error. The pattern scales the sample strongly in the middle of the domain and less strongly on the corners.}
\label{fig:ObsPat}
\end{figure}

The pattern is described by
\begin{equation}
z(x,y) = 1+\frac{0.5}{\mathrm{exp}(5((x-0.5)^2 + (y-0.5)^2))} \;.
\end{equation}

As visible in Figure \ref{fig:ObservationPriorPost}, the observations follow the general shape of the prior but deviate slightly in terms of magnitude. It is now expected that statFEM is able to form a posterior which resembles the observed data more closely while maintaining the general shape of the FEM prior.
\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{pics/3dTest.pdf}
\centering
\caption[Comparison of prior and posterior in a 3D view for the observations with model error]{On the left: A sample from the FEM prior and the observations. It is visible that the points, first of all those in the middle, are not coincident with the surface formed by the prior. The mean squared error (MSE) is given and rather high. On the right: The posterior now fits to the observation data which is also reflected in a lower MSE.}
\label{fig:ObservationPriorPost}
\end{figure}
As it is hardly visible in the plots, the difference of the magnitude of the observations compared to the prior and the posterior are cumulated in the mean squared error (MSE). As expected, the MSE for the FEM prior is comparatively high. statFEM manages to find the best hyperparameters to find a posterior which matches the data as close as possible. 
This is also reflected in the generated hyperparameters: Since the rescaling pattern only comprises values greater than one, the overall prior is rescaled with $\rho > 1$. But, to account for the non-constant rescaling, also the model inadequacy hyperparameters $\sigma_d$ and $l_d$ are now much larger than in the example without model error, as it is visible in Table \ref{tab:100procWithError}.
\begin{table}[!ht]
\centering
\caption{Hyperparameters for the case of observed data with a model error. The solution is scaled by around 20\% and the hyperparameters describing the model inadequacy are not close to zero anymore.}
\label{tab:100procWithError}
\begin{tabular}{@{}lccc@{}}
\toprule
                            & $n_o = 10$                      & $n_o = 100$                     & $n_o = 500$                     \\ \midrule
\multirow{2}{*}{$n_p = 15$} & $\rho=1.206$, $\sigma_d=3.230$, & $\rho=1.206$, $\sigma_d=2.213$, & $\rho=1.181$, $\sigma_d=3.648$, \\
                            & \multicolumn{1}{l}{$l_d=0.471$} & \multicolumn{1}{l}{$l_d=0.471$} & \multicolumn{1}{l}{$l_d=0.556$} \\
\multirow{2}{*}{$n_p = 30$} & $\rho=1.125$, $\sigma_d=3.333$  & $\rho=1.220$, $\sigma_d=3.019$  & $\rho=1.239$, $\sigma_d=2.848$  \\
                            & \multicolumn{1}{l}{$l_d=0.147$} & \multicolumn{1}{l}{$l_d=0.154$} & \multicolumn{1}{l}{$l_d=0.105$} \\
\multirow{2}{*}{$n_p = 45$} & $\rho=1.284$, $\sigma_d=7.071$  & $\rho=1.195$, $\sigma_d=7.633$  & $\rho=1.181$, $\sigma_d=4.157$  \\
                            & \multicolumn{1}{l}{$l_d=0.343$} & \multicolumn{1}{l}{$l_d=0.429$} & \multicolumn{1}{l}{$l_d=0.301$} \\ \bottomrule
\end{tabular}
\end{table}
%
It is visible in Figure \ref{fig:100procent_d} that the variance doesn't become smaller for 10 observations when the number of sensor locations is changed from 4 to 6. Because of the small number of sensors and observations, the model error couldn't be resolved adequately which lead in turn to the higher variance. This has already been shown for the 1D example in Figure \ref{fig:ModErrNoModErr1D} and is therefore expected. The effect can also be explained in another way: As already discussed, see Section \ref{sec:BayesInf}, Bayesian regression always favors a less complex model over a more complex one. If now only very few sensors are used, it would be possible to find a posterior which adequately describes the data by only scaling the prior using the scaling hyperparameter $\rho$. The other hyperparameters could stay unchanged, i.e. zero, resulting in a simple model. If then more sensors are introduced, it is not possible to describe the data by only scaling the prior, so the model inadequacy part $\bm{d}$ becomes important. The posterior variance quickly converges in this case by observing more data per sensor.
\begin{figure}[!ht]
\includegraphics[width=.9\textwidth]{../../Python/Results/2D/100procent_d/VarField_Posterior.pdf}
\centering
\caption{Introducing model error leads to a higher variance for a small number of sensor locations.}
\label{fig:100procent_d}
\end{figure}
Figure \ref{fig:100procent_d_more} shows that this is the case. Introducing more sensor locations leads to the typical behavior of decreasing variance with more sensors and observations.
\begin{figure}[!ht]
\includegraphics[width=.9\textwidth]{../../Python/Results/2D/100procent_d/VarField_PosteriorMoreSensors.pdf}
\centering
\caption{Introducing more sensor locations, it is now visible that the variance goes down with more observations and more sensors, just as expected.}
\label{fig:100procent_d_more}
\end{figure}

\FloatBarrier


\paragraph{Inferred True System Response}
Exemplarically, the numerical experiment with introduced model error from this section with $n_p=30$ and and $n_o=100$ is considered again to show how the true system response can be estimated.
As visible in Figure \ref{fig:InferedTrueResponse2D}, adding the model inadequacy GP $\bm{d}$ to the posterior solution according to \eqref{eqn:trueProcEstEq} leads to an estimation of the true process behind the observed data. An increased variance as compared to only the posterior solution is discernible which is expected after the study of the comparatively high hyperparameters for $\bm{d}$ in Table \ref{tab:100procWithError}. The mean is the same as for the posterior solution.

\begin{figure}[!ht]
\includegraphics[width=0.75\textwidth]{../../Python/Results/2D/100procent_d/SolutionCustomPosterior.pdf}
\centering
\caption[The estimated true response for the 2D Helmholtz example with model error]{By adding the model inadequacy GP $\bm{d}$ to the posterior solution GP, an estimate for the true underlying process can be made.}
\label{fig:InferedTrueResponse2D}
\end{figure}
\FloatBarrier

\subsection{Prior-Data Conflict}
As already shown for the 1D example, as soon as data is observed far outside the confidence region of the prior, i.e. there is a prior-data conflict, the inference doesn't yield accurate results anymore. Figure \ref{fig:3dPriorDataConf} shows that especially the observations in the middle of the domain are not matched and that the MSE remains very high.
%
\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{../../Python/Results/2D/prior_data_conflict/3dMSE.pdf}
\centering
\caption[Comparison of prior and posterior in a 3D view for the observations with a prior-data conflict]{See the \href{https://github.com/herluc/Masterarbeit/blob/master/MA_LucasHermann/Python/Results/2D/prior_data_conflict/3DMSE_conflict.gif}{Animation} (https://bit.ly/3m7pwDK) to see the differences better. If data is observed far outside the confidence intervals of the prior, the procedure is not able to match the posterior perfectly to the data. More sensor locations and more observations are necessary to improve the result.}
\label{fig:3dPriorDataConf}
\end{figure}
%
Figure \ref{fig:PriorDataConfPOST} shows the FEM prior compared to the posterior. Obviously, a high scaling factor was found, but also the variance didn't get as low as for other examples in which there was no prior-data conflict. This is in agreement with the results of the 1D example and \cite{Evans2006} and therefore expected.
%
\begin{figure}[!ht]
\includegraphics[width=.8\textwidth]{../../Python/Results/2D/prior_data_conflict/SolutionCustomPosterior.pdf}
\centering
\caption{It is visible that a prior-data conflict is present. The posterior is scaled much larger than the prior and, even though observations have been taken, exhibits a large variance in the magnitude of the prior's variance.}
\label{fig:PriorDataConfPOST}
\end{figure}
The approximated hyperparameters can be found in Table \ref{tab:PriorDataConf}.
%
\begin{table}[!ht]
\centering
\caption{For the prior-data conflict, a comparatively high scaling factor $\rho$ is found. The standard deviation hyperparameter for the model inadequacy term $d$ is also higher than in other examples.}
\label{tab:PriorDataConf}
\begin{tabular}{@{}llcc@{}}
\toprule
                                                                & $\rho$ & $\sigma_d$ & $l_d$ \\ \midrule
\begin{tabular}[c]{@{}l@{}}$n_p = 20$,\\ $n_o = 1$\end{tabular} & 1.405  & 17.505     & 0.167 \\ \bottomrule
\end{tabular}
\end{table}
\FloatBarrier

\subsection{Spatially Bounded Sensor Locations}
Figure \ref{fig:VarianceHalfSided2D} exemplarically illustrates the variance of the FEM prior and the variance of the inferred posterior for observations taken only in a part of the domain. As already shown for the 1D example in Section \ref{sec:spatBound}, the variance is decreased throughout the domain and becomes especially low in the region with observations. The data have been observed on a $0.75\times$ scaled FEM prior as it was already used in Section \ref{sec:scaled2D}.
\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{../../Python/Results/2D/HalfSide/3/HalfSide.pdf}
\centering
\caption{Prior and posterior variance compared for 500Hz and sensor locations only in the upper half of the domain. The variance becomes lower in the whole domain but especially in regions where sensors are placed. $n_p=20$, $n_o=10$}
\label{fig:VarianceHalfSided2D}
\end{figure}

In Figure \ref{fig:VarianceHalfSided2D3t3} it is visible that, although for few sensors and observations the variance is still high in non-observed parts of the domain, the variance can be lowered by introducing more sensors and observations.

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{../../Python/Results/2D/HalfSide/4/VarField_Posterior.pdf}
\centering
\caption{Posterior variance for $500 \,\mathrm{Hz}$ and sensor locations only in the upper half of the domain. It is visible that the variance can still be lowered throughout the domain by increasing the number of sensors and observations.}
\label{fig:VarianceHalfSided2D3t3}
\end{figure}

The estimated hyperparameters for the plots in Figure \ref{fig:VarianceHalfSided2D3t3} can be found in Table \ref{tab:75procHalf}. It is visible that the scaling of $\rho=0.75$ is matched for every combination of sensors and observations even though only observations were taken only in a fraction of the domain. Also, the the model error, recognizable by the low values for $\sigma_d$, stays low, which is correct.

\begin{table}[!ht]
\centering
\caption{Hyperparameters for the case of observed data on a scaled FEM prior, only measured in a fraction of the domain. }
\label{tab:75procHalf}
\begin{tabular}{@{}lccc@{}}
\toprule
                            & $n_o = 10$                      & $n_o = 100$                     & $n_o = 1000$                     \\ \midrule
\multirow{2}{*}{$n_p = 10$} & $\rho=0.758$, $\sigma_d=0.151$, & $\rho=0.754$, $\sigma_d=0.222$, & $\rho=0.745$, $\sigma_d=0.177$, \\
                            & \multicolumn{1}{l}{$l_d=0.377$} & \multicolumn{1}{l}{$l_d=0.849$} & \multicolumn{1}{l}{$l_d=0.875$} \\
\multirow{2}{*}{$n_p = 20$} & $\rho=0.723$, $\sigma_d=0.143$  & $\rho=0.734$, $\sigma_d=0.188$  & $\rho=0.767$, $\sigma_d=0.151$  \\
                            & \multicolumn{1}{l}{$l_d=0.715$} & \multicolumn{1}{l}{$l_d=0.641$} & \multicolumn{1}{l}{$l_d=0.646$} \\
\multirow{2}{*}{$n_p = 30$} & $\rho=0.728$, $\sigma_d=0.235$  & $\rho=0.748$, $\sigma_d=0.139$  & $\rho=0.708$, $\sigma_d=0.156$  \\
                            & \multicolumn{1}{l}{$l_d=0.866$} & \multicolumn{1}{l}{$l_d=0.643$} & \multicolumn{1}{l}{$l_d=0.812$} \\ \bottomrule
\end{tabular}
\end{table}


\FloatBarrier

\subsection{Variations in the Neumann BC}
If instead of a sinusoidal source a flat one is chosen, the solution changes significantly, see Figures \ref{fig:varFieldPriorFlat1} and \ref{fig:varFieldPriorFlat005}, both for $500\,\mathrm{Hz}$. The extent of the flat Neumann BC in y direction is defined in the parameter $bc_l$. For Figure \ref{fig:varFieldPriorFlat1} the whole left side of the domain was used as a Neumann BC, for Figure \ref{fig:varFieldPriorFlat005} only a fraction with the length of $bc_l = 0.05$ was used which can be considered a point source.
The difference is very strong: Using a constant Neumann BC over the whole side reduces the problem to a 1D one. Using a point source leads to the natural response of an unconstrained cavity. Between these two extrema there is a smooth transition, as visible in the  \href{https://github.com/herluc/Masterarbeit/blob/master/MA_LucasHermann/Python/Results/2D/VaryingSource/SourceVar500Hz_Constant.gif}{animation} (https://bit.ly/3AIlSGa). Another \href{https://github.com/herluc/Masterarbeit/blob/master/MA_LucasHermann/Python/Results/2D/VaryingSource/SourceVar220Hz_Constant.gif}{animation} (https://bit.ly/3CKJVoF) shows the same plot for $220\,\mathrm{Hz}$. It is also visible that the $l_2$ norm for both mean and variance is smaller for a response to an excitation on a smaller fraction of the left side. This is reasonable because using a smaller "piston" to move air radiates less energy into the domain.

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{../../Python/Results/2D/VaryingSource/y_1/MeanVar.pdf}
\centering
\caption{Mean and variance for a constant boundary function over the whole left side of the domain. The problem reduces to a 1D problem.}
\label{fig:varFieldPriorFlat1}
\end{figure}
%

\begin{figure}[!ht]
\includegraphics[width=1\textwidth]{../../Python/Results/2D/VaryingSource/y_005/MeanVar.pdf}
\centering
\caption{If the constant boundary function is only applied on a very small fraction of the left side, it acts like a point source. The natural response of the cavity is visible. }
\label{fig:varFieldPriorFlat005}
\end{figure}

The above figures demonstrate that the choice of the boundary function has a strong influence on the mentioned behavior of the solution. Besides choosing a GP for the boundary function it would make sense for many real world scenarios to also make the fraction of the boundary on which the Neumann BC is applied a random variable itself.


%Certain mode shapes can be excited more strongly by using a corresponding boundary function.
\FloatBarrier

Several different scenarios for the combination of measured data and an FEM prior have been discussed in the last chapters. In the next and last chapter, the gained insight will be summarized.


\chapter{Discussion and Conclusion}
After a review of the used method and its theoretical foundation, i.e. GP regression, a statFEM approach for vibroacoustics was developed and numerical experiments were carried out to prove that the implemented methods which were programmed for this thesis works. 

At first, the simple academic example of solving the Poisson equation in a 1D domain was considered. The same parameters as in \cite{girolami2021} were chosen and its results could be replicated: The source term was modeled as a GP and through a first Bayesian inference step, the FEM prior GP could be formed. A second Bayesian inference step then lead to the posterior GP: The prior was conditioned on artificial measurement data. As in the reference, observing more data on more sensor locations leads to a more accurate representation of the data as well as to a smaller variance. Additionally, the limits of the method were examined by choosing measurement data which has either been observed only on a part of the domain or far outside the prior confidence bands. For the former, the method was able to form a plausible posterior with small variance in the region with data and larger variance in the other parts of the domain. The latter demonstrated a prior-data conflict. An accurate representation of the data would be very costly from a numerical point of view.

The same experiments have been conducted for a more complex, 2D Helmholtz equation example. The numerical setup is a first approximation of an interior radiation problem. In such a problem, there is some kind of two-dimensional structure, like a plate, which emits sound into a three-dimensional cavity. Without loss of generality, that problem was reduced to a 2D problem in this thesis which basically resembles a cross sectional view of the cavity and the plate. Hence, there is a 1D sound emitter on the boundary of a 2D domain in which the sound is radiated and reflected. The 1D emitter was completely modeled as a GP with a Mat\'ern kernel function.  Beam- and/or plate-theory wasn't used here.  As in the 1D example, an FEM prior was formed using the source GP which was, contrary to the 1D example, defined in a boundary condition. It was possible to condition the prior on artificial measurement data. The correct scaling factors were approximated and artificial model error could be resolved. Again, a prior-data conflict could be provoked, which was expected. 

Due to the promising results, statFEM appears to be a useful method to incorporate sound pressure measurements into a given FEM analysis of a vibroacoustic problem. However, further research should be conducted: A more realistic source GP has to be found. Dirichlet kernel functions appear promising \cite{Jeppsson} because they are derived using a Fourier expansion which makes them inherently based on harmonic functions, by which the mode shapes of vibrating plates are usually modeled. Research should be done ito find a kernel which is able to explain the frequency dependency of the Helmholtz equation directly. Also, it could be possible to model a complete fluid/structure coupling problem in which the material or geometrical parameters of, for instance, a radiating plate can be modeled as a GP. Another very important step towards a more realistic model is to introduce impedance boundary conditions. Those, or respectively their parameters, should also be modeled as GPs. Also, the third step of statFEM which is the model selection part should be applied to the vibroacoustics example.

statFEM for vibroacoustics could be useful for current research: Lately, universities conduct research on new kinds of aircrafts \cite{Blech2020}. This involves big and complex FEM models which are often reduced to the bare minimum of information. That model-to-reality mismatch could be reduced by taking a rather small amount of measurements in a real environment to condition the FEM model. 


\appendix
\renewcommand{\thesection}{\Alph{section}.\arabic{section}}
\setcounter{section}{0}
\begin{appendices}

\chapter{Additional Plots}

\section*{Eigenfrequencies}
\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.9\textwidth]{pics/Freq170}
\caption{The mean and variance for the first eigenfrequency of the 2D field.}
\label{fig:Eigen170}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.9\textwidth]{pics/Freq342}
\caption{The mean and variance for the third eigenfrequency of the 2D field.}
\label{fig:Eigen342}
\end{center}
\end{figure}



\chapter{Code}
\section*{FEM solving with FEniCS}
\label{sec:FEMsolveCode}
This is a simple FEM solver function \cite{langtangen2016} for the Helmholtz equation implemented in Python using FEniCS.
\begin{minted}
[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
bgcolor=backcolour,
fontsize=\footnotesize,
linenos
]
{python}
def doFEM(self):
	"""
	basic FEM solver for the Helmholtz equation. Gives the mean solution for the prior
	"""
	# Define variational problem
	u = TrialFunction(V) # test and trial function in the same function space
	v = TestFunction(V)
	f0 = 0.00001 # some boundary function, here constant
	boundary_markers = MeshFunction("size_t", self.mesh, 
		self.mesh.topology().dim()-1, 0)

	tol = 1e-14
	#definition of the left and right boundaries:
	class BoundaryX_L(SubDomain):
		self.tol = 1E-14
		def inside(self, x, on_boundary):
			return on_boundary and near(x[0], 0, tol)
	class BoundaryX_R(SubDomain):
		def inside(self, x, on_boundary):
			return on_boundary and near(x[0], 1, tol)# and (x[1] < 0.3)
			
	#a Dirichlet boundary:
	def boundaryDiri(x):
		return x[0] > (1.0 - DOLFIN_EPS) and x[1] < 0.0 + DOLFIN_EPS

	self.bcDir = DirichletBC(self.V, Constant(0.0), boundaryDiri)
	
	#setting boundaries and markers to work with it:
	bxL = BoundaryX_L()
	bxR = BoundaryX_R()
	bxL.mark(boundary_markers,0) # boundary bxL is marked as "1"
	bxR.mark(boundary_markers,1)
	SourceBoundary = 1
	sb = SourceBoundary
	ds = Measure('ds', domain=self.mesh, subdomain_data=boundary_markers) 
	
	#variational problem, assembly and solving:
	a = inner(nabla_grad(u), nabla_grad(v))*dx - self.k**2 * \
	 inner(u,v)*dx 
	L = (v*g*f0)*ds(sb)
	A = assemble(a)
	b = assemble(L)
	u = Function(self.V)
	U = self.u.vector()
	
	#the solution will be in U:
	solve(A,U,b)

\end{minted}


\end{appendices}

\printbibliography


%\begin{thebibliography}{4}
%
%\bibitem{campolina}
%5Campolina, B.: Vibroacoustic modelling of aircraft double-walls with structural links using %tatistical
%Energy Analysis (SEA). Acoustics [physics.class-ph]. Université de Sherbrooke; Université %Pierre
%et Marie Curie - Paris VI (2012)

%\bibitem{Langer2019}
%Langer, S.C., Blech, C.: Cabin noise prediction using wave‐resolving aircraft models. Proc. %Appl. Math. Mech., Vol. 12 (1) (2019): e201900388. doi:10.1002/pamm.201900388

%\bibitem {Yaghoubi:2017} Yaghoubi, V., Marelli, S., Sudret, B., Abrahamsson, T.: Sparse %polynomial chaos expansions of frequency response functions using stochastic frequency %transformation, Probabilistic Engineering Mechanics, volume 48, pages 39-58 (2017)
%\end{thebibliography}

\includepdf[pages=-]{Zsmfsg.pdf}
\includepdf[pages=-]{TaskSheet.pdf}
\includepdf[pages=-]{tasksheetGerman.pdf}


\end{document}
