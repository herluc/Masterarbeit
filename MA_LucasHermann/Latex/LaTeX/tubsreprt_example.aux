\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\abx@aux@refcontext{none/global//global/global}
\catcode `"\active 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{5}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\abx@aux@cite{atalla2015}
\abx@aux@segm{0}{0}{atalla2015}
\abx@aux@cite{larson2013}
\abx@aux@segm{0}{0}{larson2013}
\abx@aux@cite{moser2005}
\abx@aux@segm{0}{0}{moser2005}
\abx@aux@segm{0}{0}{moser2005}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}FEM Solution of the Helmholtz Equation}{6}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Wave Equation and Helmholtz Equation}{6}{section.6}}
\abx@aux@backref{1}{atalla2015}{0}{6}{6}
\abx@aux@backref{2}{larson2013}{0}{6}{6}
\abx@aux@backref{3}{moser2005}{0}{6}{6}
\abx@aux@backref{4}{moser2005}{0}{6}{6}
\newlabel{eqn:waveEqDeriv1}{{\relax 2.2}{6}{Wave Equation and Helmholtz Equation}{equation.8}{}}
\newlabel{eqn:Tragheitsges}{{\relax 2.3}{6}{Wave Equation and Helmholtz Equation}{equation.9}{}}
\abx@aux@segm{0}{0}{larson2013}
\abx@aux@cite{westermann2015}
\abx@aux@segm{0}{0}{westermann2015}
\abx@aux@cite{arens2015}
\abx@aux@segm{0}{0}{arens2015}
\abx@aux@segm{0}{0}{atalla2015}
\abx@aux@backref{5}{larson2013}{0}{7}{7}
\newlabel{eqn:Constit}{{\relax 2.5}{7}{Wave Equation and Helmholtz Equation}{equation.11}{}}
\newlabel{eqn:WaveEq}{{\relax 2.7}{7}{Wave Equation and Helmholtz Equation}{equation.13}{}}
\abx@aux@backref{6}{westermann2015}{0}{7}{7}
\abx@aux@backref{7}{arens2015}{0}{7}{7}
\newlabel{eqn:Helmholtz}{{\relax 2.12}{7}{Wave Equation and Helmholtz Equation}{equation.18}{}}
\abx@aux@cite{langtangen2016}
\abx@aux@segm{0}{0}{langtangen2016}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Boundary Conditions}{8}{section*.20}}
\abx@aux@backref{8}{atalla2015}{0}{8}{8}
\abx@aux@backref{9}{langtangen2016}{0}{8}{8}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.1}{\ignorespaces Two dimensional representation of various boundary conditions. In this example, the left side is a Neumann boundary, the upper and lower boundaries are totally reflecting and the right side consists of an impedance BC and a Dirichlet BC.\relax }}{9}{figure.caption.24}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:BCEx}{{\relax 2.1}{9}{Two dimensional representation of various boundary conditions. In this example, the left side is a Neumann boundary, the upper and lower boundaries are totally reflecting and the right side consists of an impedance BC and a Dirichlet BC.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.2}{\ignorespaces Boundary Conditions of the problem for this thesis. A Neumann boundary is applied on the side of the domain. It is going to be modeled as a GP. The rest of the boundary is considered as reflecting, i.e. $\Gamma _N = 0$.\relax }}{9}{figure.caption.25}}
\newlabel{fig:BCTh}{{\relax 2.2}{9}{Boundary Conditions of the problem for this thesis. A Neumann boundary is applied on the side of the domain. It is going to be modeled as a GP. The rest of the boundary is considered as reflecting, i.e. $\Gamma _N = 0$.\relax }{figure.caption.25}{}}
\abx@aux@cite{langtangen2019}
\abx@aux@segm{0}{0}{langtangen2019}
\abx@aux@segm{0}{0}{langtangen2019}
\abx@aux@segm{0}{0}{atalla2015}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}The Classical Finite Element Method}{10}{section.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Integral and Weak Formulation}{10}{subsection.27}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline The Weigthed Residuals Method}{10}{section*.28}}
\abx@aux@backref{10}{langtangen2019}{0}{10}{10}
\newlabel{eqn:IntForm}{{\relax 2.18}{10}{The Weigthed Residuals Method}{equation.30}{}}
\abx@aux@segm{0}{0}{atalla2015}
\abx@aux@backref{11}{langtangen2019}{0}{11}{11}
\abx@aux@backref{12}{atalla2015}{0}{11}{11}
\newlabel{eqn:DivTheo}{{\relax 2.19}{11}{The Weigthed Residuals Method}{equation.31}{}}
\newlabel{eqn:WeakHelmholtz}{{\relax 2.21}{11}{The Weigthed Residuals Method}{equation.33}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Discretization}{11}{subsection.35}}
\abx@aux@backref{13}{atalla2015}{0}{11}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.3}{\ignorespaces A discretized 1D domain with nodes in green and linear ansatz functions shown for the first four nodes. The ansatz functions are 1 on their respective nodes and are 0 on all other nodes.\relax }}{12}{figure.caption.37}}
\newlabel{fig:1DDom}{{\relax 2.3}{12}{A discretized 1D domain with nodes in green and linear ansatz functions shown for the first four nodes. The ansatz functions are 1 on their respective nodes and are 0 on all other nodes.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.4}{\ignorespaces A discretized 2D domain with nodes in green. Also here, the ansatz functions are defined so that they are 1 at their respective nodes and zero at all other nodes.\relax }}{12}{figure.caption.38}}
\newlabel{fig:2DDom}{{\relax 2.4}{12}{A discretized 2D domain with nodes in green. Also here, the ansatz functions are defined so that they are 1 at their respective nodes and zero at all other nodes.\relax }{figure.caption.38}{}}
\abx@aux@segm{0}{0}{langtangen2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Approximation and Building Element Matrices}{13}{subsection.39}}
\newlabel{chap:ApproxFem}{{2.2.3}{13}{Approximation and Building Element Matrices}{subsection.39}{}}
\newlabel{eqn:WeighingFunction}{{\relax 2.24}{13}{Approximation and Building Element Matrices}{equation.40}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Lagrange Polynomials}{13}{section*.41}}
\abx@aux@backref{14}{langtangen2019}{0}{13}{13}
\newlabel{eqn:FEMAnsatz}{{\relax 2.28}{13}{Lagrange Polynomials}{equation.45}{}}
\newlabel{eqn:DiscretizedFEM}{{\relax 2.29}{14}{Lagrange Polynomials}{equation.46}{}}
\newlabel{eqn:LinSys}{{\relax 2.31}{14}{Lagrange Polynomials}{equation.48}{}}
\newlabel{eqn:Assembly}{{\relax 2.36}{14}{Lagrange Polynomials}{equation.53}{}}
\abx@aux@cite{Druinsky2012}
\abx@aux@segm{0}{0}{Druinsky2012}
\abx@aux@cite{Higham2002}
\abx@aux@segm{0}{0}{Higham2002}
\abx@aux@segm{0}{0}{arens2015}
\abx@aux@cite{Davis2006}
\abx@aux@segm{0}{0}{Davis2006}
\abx@aux@cite{Elman2014}
\abx@aux@segm{0}{0}{Elman2014}
\abx@aux@cite{MathWorks}
\abx@aux@segm{0}{0}{MathWorks}
\abx@aux@cite{Belsley1980}
\abx@aux@segm{0}{0}{Belsley1980}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Solving and Convergence}{15}{subsection.55}}
\abx@aux@backref{15}{Druinsky2012}{0}{15}{15}
\abx@aux@backref{16}{Higham2002}{0}{15}{15}
\abx@aux@backref{17}{arens2015}{0}{15}{15}
\abx@aux@backref{18}{Davis2006}{0}{15}{15}
\abx@aux@backref{19}{Elman2014}{0}{15}{15}
\abx@aux@backref{20}{MathWorks}{0}{15}{15}
\abx@aux@backref{21}{Belsley1980}{0}{15}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 2.5}{\ignorespaces The frequency response plot shows the eigenfrequencies of the fluid in the domain. Exciting the fluid with an eigenfrequency leads to a comparatively strong response.\relax }}{16}{figure.caption.56}}
\newlabel{fig:frf}{{\relax 2.5}{16}{The frequency response plot shows the eigenfrequencies of the fluid in the domain. Exciting the fluid with an eigenfrequency leads to a comparatively strong response.\relax }{figure.caption.56}{}}
\abx@aux@segm{0}{0}{arens2015}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Gaussian Processes and the Statistical Finite Element Method}{17}{chapter.57}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Basic Probability Theory}{17}{section.58}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Random Variables and Distributions}{17}{subsection.59}}
\newlabel{sec:MultiGauss}{{3.1.1}{17}{Random Variables and Distributions}{subsection.59}{}}
\abx@aux@backref{22}{arens2015}{0}{17}{17}
\newlabel{eqn:Covariance}{{\relax 3.3}{17}{Random Variables and Distributions}{equation.62}{}}
\newlabel{eqn:MultivariateGaussian}{{\relax 3.5}{17}{Random Variables and Distributions}{equation.64}{}}
\abx@aux@cite{Virtanen2020}
\abx@aux@segm{0}{0}{Virtanen2020}
\abx@aux@segm{0}{0}{Virtanen2020}
\abx@aux@cite{rasmussen2006}
\abx@aux@segm{0}{0}{rasmussen2006}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.1}{\ignorespaces A multivariate Gaussian distribution with two dimensions. A strong correlation between the two random variables is visible: No correlation would mean a perfect circle in the lower 2D plot. The more correlated the two dimensions are, the more elliptical the plotted PDF becomes. \cite {Virtanen2020} \relax }}{18}{figure.caption.66}}
\abx@aux@backref{24}{Virtanen2020}{0}{18}{18}
\newlabel{fig:MultiGauss}{{\relax 3.1}{18}{A multivariate Gaussian distribution with two dimensions. A strong correlation between the two random variables is visible: No correlation would mean a perfect circle in the lower 2D plot. The more correlated the two dimensions are, the more elliptical the plotted PDF becomes. \cite {Virtanen2020} \relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Bayesian Inference}{18}{subsection.67}}
\abx@aux@backref{25}{rasmussen2006}{0}{18}{18}
\abx@aux@cite{murphy2012}
\abx@aux@segm{0}{0}{murphy2012}
\abx@aux@cite{gortler2019}
\abx@aux@segm{0}{0}{gortler2019}
\abx@aux@segm{0}{0}{gortler2019}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{murphy2012}
\abx@aux@segm{0}{0}{murphy2012}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Gaussian Process Regression}{19}{section.69}}
\abx@aux@backref{26}{murphy2012}{0}{19}{19}
\abx@aux@backref{27}{gortler2019}{0}{19}{19}
\abx@aux@backref{28}{gortler2019}{0}{19}{19}
\abx@aux@backref{29}{rasmussen2006}{0}{19}{19}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Linear Regression}{19}{subsection.70}}
\abx@aux@backref{30}{murphy2012}{0}{19}{19}
\abx@aux@backref{31}{murphy2012}{0}{19}{19}
\newlabel{eqn:linReg}{{\relax 3.8}{19}{Linear Regression}{equation.71}{}}
\abx@aux@segm{0}{0}{murphy2012}
\abx@aux@segm{0}{0}{gortler2019}
\abx@aux@segm{0}{0}{gortler2019}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{murphy2012}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@cite{damianou2021}
\abx@aux@segm{0}{0}{damianou2021}
\abx@aux@backref{32}{murphy2012}{0}{20}{20}
\abx@aux@backref{33}{gortler2019}{0}{20}{20}
\abx@aux@backref{34}{gortler2019}{0}{20}{20}
\abx@aux@backref{35}{rasmussen2006}{0}{20}{20}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Gaussian Processes}{20}{subsection.76}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.2}{\ignorespaces On the left: Regression for a 2nd degree polynomial. Both linear regression and GP approximate it fine. On the right: The same GP as on the left is able to model the ground truth accurately, the polynomial regression does not. The GP behaves more flexible in this case.\relax }}{20}{figure.caption.77}}
\newlabel{fig:GPvsReg}{{\relax 3.2}{20}{On the left: Regression for a 2nd degree polynomial. Both linear regression and GP approximate it fine. On the right: The same GP as on the left is able to model the ground truth accurately, the polynomial regression does not. The GP behaves more flexible in this case.\relax }{figure.caption.77}{}}
\abx@aux@backref{36}{murphy2012}{0}{21}{21}
\abx@aux@backref{37}{rasmussen2006}{0}{21}{21}
\abx@aux@backref{38}{damianou2021}{0}{21}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.3}{\ignorespaces A bivariate Gaussian can be interpreted as a GP. The more corellated the random variables are, the closer together the sampled values are. The plots on the top show the samples drawn from the well-known 2D PDF in another way: Sampling from a multivariate Gaussian yields a vector of correlated samples which can be interpreted as a subset of an infinitely dimensional vector, i.e. a function.\relax }}{21}{figure.caption.78}}
\newlabel{fig:bivarGP}{{\relax 3.3}{21}{A bivariate Gaussian can be interpreted as a GP. The more corellated the random variables are, the closer together the sampled values are. The plots on the top show the samples drawn from the well-known 2D PDF in another way: Sampling from a multivariate Gaussian yields a vector of correlated samples which can be interpreted as a subset of an infinitely dimensional vector, i.e. a function.\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.4}{\ignorespaces The more random variables are used to construct the multivariate Gaussian, the more the result looks like a smooth function. Since each variable is a random variable, there is also a variance available for every dimension, visible in grey.\relax }}{22}{figure.caption.79}}
\newlabel{fig:multiGP}{{\relax 3.4}{22}{The more random variables are used to construct the multivariate Gaussian, the more the result looks like a smooth function. Since each variable is a random variable, there is also a variance available for every dimension, visible in grey.\relax }{figure.caption.79}{}}
\newlabel{sec:Marg}{{3.2.2}{22}{Marginalization and Conditioning}{section*.80}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Marginalization and Conditioning}{22}{section*.80}}
\newlabel{eqn:margi}{{\relax 3.15}{22}{Marginalization and Conditioning}{equation.83}{}}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{gortler2019}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.5}{\ignorespaces Cutting, i.e. conditioning, a bivariate Gaussian leads to a new Gaussian with only one dimension. This example shows the result for a cut at the dashed line. The new Gaussian has a new mean and variance.\relax }}{23}{figure.caption.85}}
\newlabel{fig:GaussCut3d}{{\relax 3.5}{23}{Cutting, i.e. conditioning, a bivariate Gaussian leads to a new Gaussian with only one dimension. This example shows the result for a cut at the dashed line. The new Gaussian has a new mean and variance.\relax }{figure.caption.85}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Kernel Functions}{23}{subsection.86}}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{murphy2012}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@cite{Duvenaud}
\abx@aux@segm{0}{0}{Duvenaud}
\abx@aux@segm{0}{0}{gortler2019}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@backref{39}{rasmussen2006}{0}{24}{24}
\abx@aux@backref{40}{gortler2019}{0}{24}{24}
\abx@aux@backref{41}{rasmussen2006}{0}{24}{24}
\abx@aux@backref{42}{rasmussen2006}{0}{24}{24}
\abx@aux@backref{43}{murphy2012}{0}{24}{24}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Stationarity}{24}{section*.88}}
\abx@aux@backref{44}{rasmussen2006}{0}{24}{24}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Positive Definiteness}{24}{section*.89}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline The Characteristic Length Scale}{24}{section*.90}}
\abx@aux@backref{45}{rasmussen2006}{0}{24}{24}
\abx@aux@backref{46}{Duvenaud}{0}{24}{24}
\abx@aux@backref{47}{gortler2019}{0}{24}{24}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Linear Kernel Functions}{24}{section*.91}}
\abx@aux@segm{0}{0}{murphy2012}
\abx@aux@segm{0}{0}{Duvenaud}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@cite{stein1999}
\abx@aux@segm{0}{0}{stein1999}
\abx@aux@cite{matern2013}
\abx@aux@segm{0}{0}{matern2013}
\abx@aux@segm{0}{0}{stein1999}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.6}{\ignorespaces Samples drawn from a GP with a linear covariance function. There is a region with relatively low variance through which all samples pass. The poosition can be mived by varying a hyperparameter of the kernel.\relax }}{25}{figure.caption.93}}
\newlabel{fig:Linear}{{\relax 3.6}{25}{Samples drawn from a GP with a linear covariance function. There is a region with relatively low variance through which all samples pass. The poosition can be mived by varying a hyperparameter of the kernel.\relax }{figure.caption.93}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Squared Exponential Kernel Functions}{25}{section*.94}}
\abx@aux@backref{48}{rasmussen2006}{0}{25}{25}
\newlabel{lst:sqEx}{{3.2.3}{25}{Squared Exponential Kernel Functions}{lstnumber.100}{}}
\abx@aux@backref{49}{murphy2012}{0}{25}{25}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Periodic Kernel Functions}{25}{section*.102}}
\abx@aux@backref{50}{Duvenaud}{0}{25}{25}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Dirichlet Kernel Functions}{25}{section*.105}}
\abx@aux@cite{abramowitz2013}
\abx@aux@segm{0}{0}{abramowitz2013}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{rasmussen2006}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.7}{\ignorespaces The source term GP of $f(x)$ sampled for a Squared Exponential kernel with $l = 0.25$ and $\sigma = 0.3$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. The samples are very smooth, which is a charcteristic of this particular kernel.\relax }}{26}{figure.caption.101}}
\newlabel{fig:Matern1_2}{{\relax 3.7}{26}{The source term GP of $f(x)$ sampled for a Squared Exponential kernel with $l = 0.25$ and $\sigma = 0.3$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. The samples are very smooth, which is a charcteristic of this particular kernel.\relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.8}{\ignorespaces Three samples from a periodic kernel. The periodicity is clearly discernible. The variance is constant.\relax }}{26}{figure.caption.104}}
\newlabel{fig:Periodic}{{\relax 3.8}{26}{Three samples from a periodic kernel. The periodicity is clearly discernible. The variance is constant.\relax }{figure.caption.104}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Matern Kernel Functions}{26}{section*.106}}
\abx@aux@backref{51}{rasmussen2006}{0}{26}{26}
\abx@aux@backref{52}{stein1999}{0}{26}{26}
\abx@aux@backref{53}{matern2013}{0}{26}{26}
\abx@aux@backref{54}{stein1999}{0}{26}{26}
\newlabel{eqn:MaternKernel}{{\relax 3.21}{26}{Matern Kernel Functions}{equation.107}{}}
\abx@aux@backref{55}{abramowitz2013}{0}{26}{26}
\abx@aux@backref{56}{rasmussen2006}{0}{26}{26}
\abx@aux@segm{0}{0}{Duvenaud}
\abx@aux@backref{57}{rasmussen2006}{0}{27}{27}
\newlabel{eqn:Materns1}{{\relax 3.23}{27}{Matern Kernel Functions}{equation.109}{}}
\newlabel{eqn:Materns3}{{\relax 3.24}{27}{Matern Kernel Functions}{equation.110}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.9}{\ignorespaces The source term GP of $f(x)$ sampled for a Matern kernel with $\nu =1/2$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. For this $\nu $ the samples behave very rough.\relax }}{27}{figure.caption.111}}
\newlabel{fig:Matern1_2}{{\relax 3.9}{27}{The source term GP of $f(x)$ sampled for a Matern kernel with $\nu =1/2$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. For this $\nu $ the samples behave very rough.\relax }{figure.caption.111}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Combination of Different Kernel Functions}{27}{section*.115}}
\abx@aux@backref{58}{Duvenaud}{0}{27}{27}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.4}Bayesian Inference}{27}{subsection.117}}
\abx@aux@segm{0}{0}{rasmussen2006}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.10}{\ignorespaces The source term GP of $f(x)$ sampled for a Matern kernel with $\nu =3/2$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. For this $\nu $ the samples behave less rough.\relax }}{28}{figure.caption.112}}
\newlabel{fig:Matern3_2}{{\relax 3.10}{28}{The source term GP of $f(x)$ sampled for a Matern kernel with $\nu =3/2$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. For this $\nu $ the samples behave less rough.\relax }{figure.caption.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.11}{\ignorespaces The source term GP of $f(x)$ sampled for a Matern kernel with $\nu =5/2$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. For this $\nu $ the samples behave almost as smooth as in the Squared Exponential kernel.\relax }}{28}{figure.caption.113}}
\newlabel{fig:Matern5_2}{{\relax 3.11}{28}{The source term GP of $f(x)$ sampled for a Matern kernel with $\nu =5/2$. $\bar {f}(x)$ in solid black, $2\sigma $ confidence bands in green. For this $\nu $ the samples behave almost as smooth as in the Squared Exponential kernel.\relax }{figure.caption.113}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Prior}{28}{section*.118}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.12}{\ignorespaces The covariance matrices for the different kernel functions. 100 linearly spaced test points between $-5$ and $5$ were used as inputs. The Squared Exponential and Matern kernels yield similar results. For the periodic kernel, the periodicity is also visible in the covariance matrix. For the covariance matrix derived from the linear kernel, the region of lower variance is visible.\relax }}{29}{figure.caption.114}}
\newlabel{fig:CovMats}{{\relax 3.12}{29}{The covariance matrices for the different kernel functions. 100 linearly spaced test points between $-5$ and $5$ were used as inputs. The Squared Exponential and Matern kernels yield similar results. For the periodic kernel, the periodicity is also visible in the covariance matrix. For the covariance matrix derived from the linear kernel, the region of lower variance is visible.\relax }{figure.caption.114}{}}
\abx@aux@backref{59}{rasmussen2006}{0}{29}{29}
\abx@aux@segm{0}{0}{damianou2021}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Occam's Razor}{30}{section*.120}}
\newlabel{sec:GPcond}{{3.2.4}{30}{Posterior after observation}{section*.121}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Posterior after observation}{30}{section*.121}}
\newlabel{eqn:noisefreeJoint}{{\relax 3.27}{30}{Posterior after observation}{equation.122}{}}
\newlabel{eqn:noiseJoint}{{\relax 3.28}{30}{Posterior after observation}{equation.123}{}}
\newlabel{eqn:ConditioningDistr}{{\relax 3.29}{30}{Posterior after observation}{equation.124}{}}
\abx@aux@backref{60}{damianou2021}{0}{30}{30}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.13}{\ignorespaces The prior GP on the left and the posterior GP after conditioning on two training points on the right. The variance approaches zero at the noise-less training points. The further away from the training points, the closer the variance is back at the prior variance.\relax }}{31}{figure.caption.125}}
\newlabel{fig:GPExpl}{{\relax 3.13}{31}{The prior GP on the left and the posterior GP after conditioning on two training points on the right. The variance approaches zero at the noise-less training points. The further away from the training points, the closer the variance is back at the prior variance.\relax }{figure.caption.125}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.14}{\ignorespaces The covariance matrix before and after conditioning. The lighter areas show higher (co)variance. The part of the domain on the right, where no observations were made, is visible as entries with high variance in the top right.\relax }}{31}{figure.caption.126}}
\newlabel{fig:CovPOst}{{\relax 3.14}{31}{The covariance matrix before and after conditioning. The lighter areas show higher (co)variance. The part of the domain on the right, where no observations were made, is visible as entries with high variance in the top right.\relax }{figure.caption.126}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}The Statistical Finite Element Method}{31}{section.127}}
\abx@aux@cite{girolami2021}
\abx@aux@segm{0}{0}{girolami2021}
\abx@aux@cite{kennedy2001}
\abx@aux@segm{0}{0}{kennedy2001}
\abx@aux@segm{0}{0}{girolami2021}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline The Statistical Generating Model}{32}{section*.128}}
\abx@aux@backref{61}{girolami2021}{0}{32}{32}
\abx@aux@backref{62}{kennedy2001}{0}{32}{32}
\newlabel{eqn:statGen}{{\relax 3.30}{32}{The Statistical Generating Model}{equation.129}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Basic statFEM procedure}{32}{section*.132}}
\abx@aux@backref{63}{girolami2021}{0}{32}{32}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Posterior of the FEM Forward Model}{32}{subsection.133}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Assembling the FEM System Matrix}{32}{section*.134}}
\abx@aux@segm{0}{0}{arens2015}
\abx@aux@cite{schon2011}
\abx@aux@segm{0}{0}{schon2011}
\abx@aux@cite{bardsley2018}
\abx@aux@segm{0}{0}{bardsley2018}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Generate and Sample From Source Term GP}{34}{section*.144}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Generate and sample from the diffusion coefficient GP}{34}{section*.145}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Computing the FEM Prior}{34}{section*.147}}
\abx@aux@backref{64}{arens2015}{0}{34}{34}
\abx@aux@backref{65}{schon2011}{0}{35}{35}
\abx@aux@backref{66}{bardsley2018}{0}{35}{35}
\newlabel{eqn:u_GP_disc}{{\relax 3.39}{35}{Computing the FEM Prior}{equation.153}{}}
\newlabel{lst:get_u_mean}{{3.3.1}{35}{Computing the FEM Prior}{lstnumber.163}{}}
\newlabel{lst:get_C_u}{{3.3.1}{35}{Computing the FEM Prior}{lstnumber.177}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Find Prior Mean and Variance}{36}{section*.178}}
\newlabel{eqn:varCu}{{\relax 3.42}{36}{Find Prior Mean and Variance}{equation.179}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline The Projection Matrix $P$}{36}{section*.180}}
\pgfsyspdfmark {pgfid1}{20557161}{22722105}
\pgfsyspdfmark {pgfid2}{19176541}{18825015}
\pgfsyspdfmark {pgfid3}{22219048}{17815762}
\pgfsyspdfmark {pgfid4}{23233217}{16806509}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.15}{\ignorespaces The matrix $P$ contains the values of the FEM ansatz functions belonging to the nodes on which observations are available. Since linear Lagrange elements yield $1$ at their respective node number and $0$ on all other nodes, $P$ consists of only $1$s and $0$s.\relax }}{36}{figure.caption.181}}
\newlabel{fig:PIllust}{{\relax 3.15}{36}{The matrix $P$ contains the values of the FEM ansatz functions belonging to the nodes on which observations are available. Since linear Lagrange elements yield $1$ at their respective node number and $0$ on all other nodes, $P$ consists of only $1$s and $0$s.\relax }{figure.caption.181}{}}
\pgfsyspdfmark {pgfid5}{20977648}{11321889}
\abx@aux@segm{0}{0}{girolami2021}
\newlabel{lst:getP}{{3.3.1}{37}{The Projection Matrix $P$}{lstnumber.200}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Calculate $C_e$}{37}{section*.201}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Calculate $C_d$}{37}{section*.202}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Inference of the Posterior Density}{37}{subsection.203}}
\abx@aux@segm{0}{0}{girolami2021}
\abx@aux@cite{riedel1992}
\abx@aux@segm{0}{0}{riedel1992}
\abx@aux@segm{0}{0}{girolami2021}
\abx@aux@backref{67}{girolami2021}{0}{38}{38}
\newlabel{eqn:bayesPrior}{{\relax 3.45}{38}{Inference of the Posterior Density}{equation.205}{}}
\abx@aux@backref{68}{girolami2021}{0}{38}{38}
\newlabel{eqn:p_u_y_unnorm}{{\relax 3.50}{38}{Inference of the Posterior Density}{equation.210}{}}
\newlabel{eqn:statFEMConditioned}{{\relax 3.52}{38}{Inference of the Posterior Density}{equation.212}{}}
\abx@aux@backref{69}{riedel1992}{0}{38}{38}
\abx@aux@backref{70}{girolami2021}{0}{38}{38}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Probabilistic FEM Prior}{39}{subsection.217}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.16}{\ignorespaces Convergence of the error norm for the FEM prior mean with a slope of 2 on a log-log scale.\relax }}{39}{figure.caption.226}}
\newlabel{fig:u_mean_conv}{{\relax 3.16}{39}{Convergence of the error norm for the FEM prior mean with a slope of 2 on a log-log scale.\relax }{figure.caption.226}{}}
\abx@aux@segm{0}{0}{girolami2021}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@segm{0}{0}{kennedy2001}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@cite{Eberly2000}
\abx@aux@segm{0}{0}{Eberly2000}
\abx@aux@cite{Bayarri}
\abx@aux@segm{0}{0}{Bayarri}
\abx@aux@segm{0}{0}{kennedy2001}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.4}Estimation of Hyperparameters}{40}{subsection.227}}
\newlabel{eqn:bayesHyperp}{{\relax 3.57}{40}{Estimation of Hyperparameters}{equation.228}{}}
\abx@aux@backref{71}{girolami2021}{0}{40}{40}
\abx@aux@backref{72}{rasmussen2006}{0}{40}{40}
\abx@aux@backref{73}{kennedy2001}{0}{40}{40}
\abx@aux@backref{74}{rasmussen2006}{0}{40}{40}
\newlabel{eqn:neglog}{{\relax 3.62}{40}{Estimation of Hyperparameters}{equation.233}{}}
\abx@aux@cite{chambers1998}
\abx@aux@segm{0}{0}{chambers1998}
\abx@aux@cite{tomaskovic-moore}
\abx@aux@segm{0}{0}{tomaskovic-moore}
\abx@aux@cite{Svensson2015}
\abx@aux@segm{0}{0}{Svensson2015}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Non-identifiability}{41}{section*.234}}
\abx@aux@backref{75}{kennedy2001}{0}{41}{41}
\newlabel{sec:Cholesky}{{3.3.4}{41}{Cholesky Decompositon}{section*.235}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Cholesky Decompositon}{41}{section*.235}}
\abx@aux@backref{76}{chambers1998}{0}{41}{41}
\abx@aux@backref{77}{tomaskovic-moore}{0}{41}{41}
\newlabel{eqn:CholDet}{{\relax 3.66}{41}{Cholesky Decompositon}{equation.243}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Minimization of the Negative log-Likelihood}{41}{section*.244}}
\abx@aux@backref{78}{Svensson2015}{0}{41}{41}
\abx@aux@segm{0}{0}{girolami2021}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 3.17}{\ignorespaces See the \href  {https://github.com/herluc/herluc.github.io/blob/main/MLL.gif}{Animation} to get a better understanding of where the minimum lies. Sampling throughout a carefully predefined parameter space and solving for the negative log-likelihood yields a field of solutions. The set of hyperparameters is chosen which yields the smallest negative log-likelihood. The scaling factor $\rho $ is also part of the random sampling but not shown here. The found minimum is shown as a red dot.\relax }}{42}{figure.caption.246}}
\newlabel{fig:RandSampHyper}{{\relax 3.17}{42}{See the \href {https://github.com/herluc/herluc.github.io/blob/main/MLL.gif}{Animation} to get a better understanding of where the minimum lies. Sampling throughout a carefully predefined parameter space and solving for the negative log-likelihood yields a field of solutions. The set of hyperparameters is chosen which yields the smallest negative log-likelihood. The scaling factor $\rho $ is also part of the random sampling but not shown here. The found minimum is shown as a red dot.\relax }{figure.caption.246}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Random Sampling in the Parameter Space}{42}{section*.245}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline MCMC}{42}{section*.247}}
\abx@aux@backref{79}{girolami2021}{0}{43}{43}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline L-BGFS}{43}{section*.249}}
\abx@aux@segm{0}{0}{girolami2021}
\abx@aux@segm{0}{0}{langtangen2019}
\abx@aux@segm{0}{0}{girolami2021}
\abx@aux@segm{0}{0}{murphy2012}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}A statFEM Approach for Vibroacoustics}{44}{chapter.250}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\abx@aux@backref{80}{girolami2021}{0}{44}{44}
\abx@aux@backref{81}{langtangen2019}{0}{44}{44}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Simple 1D example}{44}{section.251}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Choice of PDE}{44}{section*.252}}
\newlabel{eqn:Poisson}{{\relax 4.1}{44}{Choice of PDE}{equation.253}{}}
\abx@aux@backref{82}{girolami2021}{0}{44}{44}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Construction of the GP}{44}{section*.255}}
\newlabel{eqn:sqEx_f}{{\relax 4.3}{44}{Construction of the GP}{equation.256}{}}
\abx@aux@backref{83}{murphy2012}{0}{44}{44}
\abx@aux@segm{0}{0}{rasmussen2006}
\abx@aux@backref{84}{rasmussen2006}{0}{45}{45}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}FEM Prior}{45}{subsection.257}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.1}{\ignorespaces FEM Prior for the 1D example. The mean and confidence band as well as samples drawn from the GP are shown. As the boundary conditions imply, both mean and variance are zero on the boundary of the domain.\relax }}{45}{figure.caption.258}}
\newlabel{fig:FEMGP1D}{{\relax 4.1}{45}{FEM Prior for the 1D example. The mean and confidence band as well as samples drawn from the GP are shown. As the boundary conditions imply, both mean and variance are zero on the boundary of the domain.\relax }{figure.caption.258}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Posterior}{46}{subsection.259}}
\newlabel{fig:OneSensor}{{\relax 4.2a}{46}{One observation at one sensor location. Shape of FEM Prior remains but scaling and variance differ.\relax }{figure.caption.260}{}}
\newlabel{sub@fig:OneSensor}{{a}{46}{One observation at one sensor location. Shape of FEM Prior remains but scaling and variance differ.\relax }{figure.caption.260}{}}
\newlabel{fig:4p10}{{\relax 4.2b}{46}{Per observation point 1 individual observation was taken with a standard deviation of $2.5e-3$. \relax }{figure.caption.260}{}}
\newlabel{sub@fig:4p10}{{b}{46}{Per observation point 1 individual observation was taken with a standard deviation of $2.5e-3$. \relax }{figure.caption.260}{}}
\newlabel{fig:DataCloseToCI2}{{\relax 4.2c}{46}{Per observation point 20 individual observations were taken with a standard deviation of $2.5e-3$. \relax }{figure.caption.260}{}}
\newlabel{sub@fig:DataCloseToCI2}{{c}{46}{Per observation point 20 individual observations were taken with a standard deviation of $2.5e-3$. \relax }{figure.caption.260}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.2}{\ignorespaces The posterior for different measurement scenarios. The more sensor locations are introduced and the more observations are taken, the smaller the variance gets.\relax }}{46}{figure.caption.260}}
\newlabel{fig:OneSensor}{{\relax 4.2}{46}{The posterior for different measurement scenarios. The more sensor locations are introduced and the more observations are taken, the smaller the variance gets.\relax }{figure.caption.260}{}}
\newlabel{fig:NoModelError1D}{{\relax 4.3a}{47}{Observing data from a scaled sample of the FEM prior leads to a very small variance.\relax }{figure.caption.261}{}}
\newlabel{sub@fig:NoModelError1D}{{a}{47}{Observing data from a scaled sample of the FEM prior leads to a very small variance.\relax }{figure.caption.261}{}}
\newlabel{fig:ModelError1D}{{\relax 4.3b}{47}{Introducing significant model error to the observations leads to a higher variance. It could be reduced by using more observations per sensor location.\relax }{figure.caption.261}{}}
\newlabel{sub@fig:ModelError1D}{{b}{47}{Introducing significant model error to the observations leads to a higher variance. It could be reduced by using more observations per sensor location.\relax }{figure.caption.261}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.3}{\ignorespaces Comparison of the influence of different amounts of model error on the posterior variance.\relax }}{47}{figure.caption.261}}
\newlabel{fig:ModErrNoModErr1D}{{\relax 4.3}{47}{Comparison of the influence of different amounts of model error on the posterior variance.\relax }{figure.caption.261}{}}
\newlabel{fig:DOneSided}{{\relax 4.4a}{47}{If data is measured only on one side of the domain the overall shape of the FEM prior still remains but the variance changes according to the presence of data in a region of the domain.\relax }{figure.caption.262}{}}
\newlabel{sub@fig:DOneSided}{{a}{47}{If data is measured only on one side of the domain the overall shape of the FEM prior still remains but the variance changes according to the presence of data in a region of the domain.\relax }{figure.caption.262}{}}
\newlabel{fig:DataPriorConflict}{{\relax 4.4b}{47}{Per observation point 50 individual observations were used with a standard deviation of $2.5e-3$. 30 Elements.\relax }{figure.caption.262}{}}
\newlabel{sub@fig:DataPriorConflict}{{b}{47}{Per observation point 50 individual observations were used with a standard deviation of $2.5e-3$. 30 Elements.\relax }{figure.caption.262}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.4}{\ignorespaces Taking observations slightly out of the prior confidence bands works, but the further away the measurements are taken, the more weight is on thr prior and the data is not matched.\relax }}{47}{figure.caption.262}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}2D Vibroacoustics Example}{47}{section.263}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Generating Fake Data}{48}{section*.264}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Experimental Design}{48}{section*.265}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}FEM prior}{48}{subsection.266}}
\abx@aux@segm{0}{0}{rasmussen2006}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.5}{\ignorespaces The Neumann BC is modeled as a GP with a harmonic mean and a Matern kernel. The mean, the $2 \sigma $ confidence band and some drawn samples are shown.\relax }}{49}{figure.caption.269}}
\newlabel{fig:NeumannBC}{{\relax 4.5}{49}{The Neumann BC is modeled as a GP with a harmonic mean and a Matern kernel. The mean, the $2 \sigma $ confidence band and some drawn samples are shown.\relax }{figure.caption.269}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Posterior, observations on prior sample}{49}{subsection.272}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Hypothesis}{49}{section*.273}}
\abx@aux@backref{85}{rasmussen2006}{0}{49}{49}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.6}{\ignorespaces The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 222Hz. The variance around the source leads to a variance in the pressure field which is the lowest where the mean absolute pressure is the lowest.\relax }}{50}{figure.caption.270}}
\newlabel{fig:FEMGP}{{\relax 4.6}{50}{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 222Hz. The variance around the source leads to a variance in the pressure field which is the lowest where the mean absolute pressure is the lowest.\relax }{figure.caption.270}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Without Model Error}{50}{section*.274}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.7}{\ignorespaces The variance of the FEM prior for $500Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible: Regions with high absolute pressure peaks also have a high variance. This implies that the positions of the nodal points in the mode shape are approximated very accurately.\relax }}{51}{figure.caption.271}}
\newlabel{fig:varFieldPrior}{{\relax 4.7}{51}{The variance of the FEM prior for $500Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible: Regions with high absolute pressure peaks also have a high variance. This implies that the positions of the nodal points in the mode shape are approximated very accurately.\relax }{figure.caption.271}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 4.1}{\ignorespaces For a $100\%$ scaling and no introduced model error, $\rho $ is close to $1$ for every case, which is expected for a direct FEM prior sample as ground truth. The model error parameters are close to zero which is expected, because no model error was introduced.\relax }}{51}{table.276}}
\newlabel{tab:rho100p_nod}{{\relax 4.1}{51}{For a $100\%$ scaling and no introduced model error, $\rho $ is close to $1$ for every case, which is expected for a direct FEM prior sample as ground truth. The model error parameters are close to zero which is expected, because no model error was introduced.\relax }{table.276}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Posterior, observations on scaled prior sample}{51}{subsection.277}}
\newlabel{fig:75proc_no_d_overview}{{4.2.3}{51}{Posterior, observations on scaled prior sample}{figure.caption.279}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.8}{\ignorespaces The variance of the posterior for $500Hz$ at all points in the 2D space for different combinations of sensor and observation numbers. Regions with high and low variance are clearly visible. Where sensor locations are introduced, the variance gets smaller in patterns around the location all over the domain. Adding more sensor locations induces a thinner mesh of patterns. The more observations are taken per sensor, the smaller the overall variance becomes, which reflects in the specified l2 norm for it.\relax }}{52}{figure.caption.275}}
\newlabel{fig:100proc_no_d}{{\relax 4.8}{52}{The variance of the posterior for $500Hz$ at all points in the 2D space for different combinations of sensor and observation numbers. Regions with high and low variance are clearly visible. Where sensor locations are introduced, the variance gets smaller in patterns around the location all over the domain. Adding more sensor locations induces a thinner mesh of patterns. The more observations are taken per sensor, the smaller the overall variance becomes, which reflects in the specified l2 norm for it.\relax }{figure.caption.275}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Posterior, observations on altered prior sample}{52}{subsection.282}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 4.2}{\ignorespaces For a $75\%$ scaling and no introduced model error, $\rho $ is close to $0.75$ for every case, which is expected for a $75 \%$ scales FEM prior sample as ground truth. The model error is close to zero.\relax }}{53}{table.278}}
\newlabel{tab:rho75p_nod}{{\relax 4.2}{53}{For a $75\%$ scaling and no introduced model error, $\rho $ is close to $0.75$ for every case, which is expected for a $75 \%$ scales FEM prior sample as ground truth. The model error is close to zero.\relax }{table.278}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.9}{\ignorespaces The variance of the posterior for $500Hz$ at all points in the 2D space for different combinations of sensor and observation numbers. Sampled from a $75\%$ scaling. Regions with high and low variance are clearly visible.\relax }}{53}{figure.caption.279}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.10}{\ignorespaces The variance of the posterior for $500Hz$ at all points in the 2D space for different combinations of sensor and observation numbers. Sampled from a $75\%$ scaling. Regions with high and low variance are clearly visible. Although the observations were taken from the scaled sample, the variance quickly gets smaller with more sensors and observations.\relax }}{54}{figure.caption.280}}
\newlabel{fig:75proc_no_d}{{\relax 4.10}{54}{The variance of the posterior for $500Hz$ at all points in the 2D space for different combinations of sensor and observation numbers. Sampled from a $75\%$ scaling. Regions with high and low variance are clearly visible. Although the observations were taken from the scaled sample, the variance quickly gets smaller with more sensors and observations.\relax }{figure.caption.280}{}}
\@writefile{lot}{\contentsline {table}{\numberline {\relax 4.3}{\ignorespaces Hyperparameters for the case of observed data with a model error. The solution is scaled by around 20\% and the hyperparameters describing the model inadequacy are not close to zero anymore.\relax }}{54}{table.287}}
\newlabel{tab:100procWithError}{{\relax 4.3}{54}{Hyperparameters for the case of observed data with a model error. The solution is scaled by around 20\% and the hyperparameters describing the model inadequacy are not close to zero anymore.\relax }{table.287}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.11}{\ignorespaces See the \href  {https://github.com/herluc/herluc.github.io/blob/main/3DMSE.gif}{Animation} to see the differences better. With observations on a 75\% scaling of the FEM prior and no other introduced model error, the posterior is able to match the data with an MSE of $0.0$. \relax }}{55}{figure.caption.281}}
\newlabel{fig:75proc_no_d3d}{{\relax 4.11}{55}{See the \href {https://github.com/herluc/herluc.github.io/blob/main/3DMSE.gif}{Animation} to see the differences better. With observations on a 75\% scaling of the FEM prior and no other introduced model error, the posterior is able to match the data with an MSE of $0.0$. \relax }{figure.caption.281}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.12}{\ignorespaces Multiplying a prior sample with a rescaling pattern leads to a new ground truth with model error. The pattern strongly scales the sample in the middle of the domain and less strongly on the corners.\relax }}{55}{figure.caption.284}}
\newlabel{fig:ObsPat}{{\relax 4.12}{55}{Multiplying a prior sample with a rescaling pattern leads to a new ground truth with model error. The pattern strongly scales the sample in the middle of the domain and less strongly on the corners.\relax }{figure.caption.284}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.13}{\ignorespaces On the left: A sample from the FEM prior and the observations. It is visible that the points, first if al those in the middle, are not coincident with the surface formed by the prior. The mean squared error (MSE) is given and rather high. On the right: The posterior now fits to the observation data which is also reflected in a lower MSE.\relax }}{56}{figure.caption.286}}
\newlabel{fig:ObservationPriorPost}{{\relax 4.13}{56}{On the left: A sample from the FEM prior and the observations. It is visible that the points, first if al those in the middle, are not coincident with the surface formed by the prior. The mean squared error (MSE) is given and rather high. On the right: The posterior now fits to the observation data which is also reflected in a lower MSE.\relax }{figure.caption.286}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.14}{\ignorespaces Introducing model error leads to a higher variance for a small number of sensor locations.\relax }}{56}{figure.caption.288}}
\newlabel{fig:100procent_d}{{\relax 4.14}{56}{Introducing model error leads to a higher variance for a small number of sensor locations.\relax }{figure.caption.288}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.15}{\ignorespaces Introducing more sensor locations, it is now visible that the variance goes down with mmore observations and more sensors, just as expected.\relax }}{57}{figure.caption.289}}
\newlabel{fig:100procent_d_more}{{\relax 4.15}{57}{Introducing more sensor locations, it is now visible that the variance goes down with mmore observations and more sensors, just as expected.\relax }{figure.caption.289}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Prior-Data Conflict}{57}{subsection.290}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}Variations in the Neumann BC}{57}{subsection.292}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.16}{\ignorespaces See the \href  {https://github.com/herluc/herluc.github.io/blob/main/3DMSE.gif}{Animation} to see the differences better. With observations on a 75\% scaling of the FEM prior and no other introduced model error, the posterior is able to match the data with an MSE of $0.0$. \relax }}{58}{figure.caption.291}}
\newlabel{fig:3dPriorDataConf}{{\relax 4.16}{58}{See the \href {https://github.com/herluc/herluc.github.io/blob/main/3DMSE.gif}{Animation} to see the differences better. With observations on a 75\% scaling of the FEM prior and no other introduced model error, the posterior is able to match the data with an MSE of $0.0$. \relax }{figure.caption.291}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.17}{\ignorespaces The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 320Hz.\relax }}{58}{figure.caption.293}}
\newlabel{fig:FEMGPFlat320}{{\relax 4.17}{58}{The source GP, the mean prior and, at the cut $y=y_c$, the prior FEM GP for 320Hz.\relax }{figure.caption.293}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {\relax 4.18}{\ignorespaces The variance of the FEM prior for $320Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible.\relax }}{59}{figure.caption.294}}
\newlabel{fig:varFieldPriorFlat320}{{\relax 4.18}{59}{The variance of the FEM prior for $320Hz$ at all points in the 2D space. Regions with high and low variance are clearly visible.\relax }{figure.caption.294}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion and Discussion}{60}{chapter.295}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\@writefile{lol}{\addvspace {10\p@ }}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{atalla2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{larson2013}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{moser2005}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{westermann2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{arens2015}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{langtangen2016}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{langtangen2019}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Druinsky2012}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Higham2002}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Davis2006}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Elman2014}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{MathWorks}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Belsley1980}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Virtanen2020}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{rasmussen2006}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{murphy2012}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gortler2019}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{damianou2021}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Duvenaud}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{stein1999}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{matern2013}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{abramowitz2013}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{girolami2021}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kennedy2001}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{schon2011}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{bardsley2018}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{riedel1992}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{chambers1998}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{tomaskovic-moore}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{Svensson2015}{none/global//global/global}
\gdef\minted@oldcachelist{,
  default-pyg-prefix.pygstyle,
  default.pygstyle,
  E8F0F0B0D3211E13EE6BBA47A6A76A084919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  4F5E25E415FD3903B5540297FD4D99E94919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  E98B375D4B33A5A7E28F370F7231F83D4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  D7DA64A03C3D6E00B7B501E6F755EA0A4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  B9A80D549FC2AA1F35FBB6875C55A7AF4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  D6E2688FA448FCAE7560F1A2327F427B4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  3E69A0F873FF73D028F4E944017BB9E34919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  572FC33881192738CFE863B89BF87FFD4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  A00BD5E9986F5A94ECAB00F4C690CD6D4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  ECF558AC2E711FD116EF39EF3D9617714919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  8F71197D32440058870392EB615C42784919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  0708711679AF536F33FAA14CCAFD932D4919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  966314A1EC01560B3B6EF7ECFF8646514919F94676096CAB56C0D9EE67FD8EB5.pygtex,
  11DC5E93A93CE1F6690578A10164E3FC4919F94676096CAB56C0D9EE67FD8EB5.pygtex}
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{8.99994pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{16.49994pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{23.99994pt}
